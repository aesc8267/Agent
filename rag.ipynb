{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study 用\n",
    "\n",
    "# import datasets\n",
    "\n",
    "# knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "# docs=[doc for doc in knowledge_base]\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "knowledge_base=[]\n",
    "read_files = glob.glob(\"./数据分析教程/*.txt\")\n",
    "for f in read_files:\n",
    "    with open(f,\"r\") as fp:\n",
    "        knowledge_base.append({'text':fp.read(),'source':f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:01<00:00, 25.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]}) for doc in knowledge_base\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Split docs and keep only unique ones\n",
    "print(\"Splitting documents...\")\n",
    "docs_processed = []\n",
    "unique_texts = {}\n",
    "for doc in tqdm(source_docs):\n",
    "    new_docs = text_splitter.split_documents([doc])\n",
    "    for new_doc in new_docs:\n",
    "        if new_doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aesc\\AppData\\Local\\Temp\\ipykernel_14140\\1302164431.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=docs_processed,\n",
    "    embedding=embedding_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用本地vectordb\n",
    "from langchain.vectorstores import FAISS\n",
    "# vectordb=FAISS.load_local(\"./vectordb\",embeddings=HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\"),allow_dangerous_deserialization=True)\n",
    "vectordb.save_local(\"vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, vectordb: VectorStore, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.vectordb.similarity_search(\n",
    "            query,\n",
    "            k=7,\n",
    "        )\n",
    "\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [f\"===== Document {str(i)} =====\\n\" + doc.page_content for i, doc in enumerate(docs)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import HfApiModel, ToolCallingAgent,OpenAIServerModel\n",
    "model=OpenAIServerModel(model_id='qwen-plus',api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",api_key=\"sk-615616fb539749dda57c80cc0928669d\")\n",
    "retriever_tool = RetrieverTool(vectordb)\n",
    "agent = ToolCallingAgent(tools=[retriever_tool], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How can I push a model to the Hub?</span>                                                                              <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ OpenAIServerModel - qwen-plus ─────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow can I push a model to the Hub?\u001b[0m                                                                              \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m OpenAIServerModel - qwen-plus \u001b[0m\u001b[38;2;212;183;2m────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'push model to Hub'}                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'retriever' with arguments: {'query': 'push model to Hub'}                                        │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: Retrieved documents:\n",
       "===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> =====\n",
       "--push_to_hub\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> =====\n",
       "```py\n",
       "&gt;&gt;&gt; <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">trainer.push_to_hub</span><span style=\"font-weight: bold\">()</span>\n",
       "```\n",
       "<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">pt</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&lt;tf&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Share a model to the Hub with |`PushToHubCallback`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">. In the |`PushToHubCallback`</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\"> function, add:</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- An output directory for your model.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- A tokenizer.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">- The `hub_model_id`, which is your Hub username and model name.</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">```py</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;&gt; from transformers import PushToHubCallback</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"font-weight: bold\">&gt;</span> push_to_hub_callback = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PushToHubCallback</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span>     <span style=\"color: #808000; text-decoration-color: #808000\">output_dir</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"./your_model_save_path\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">tokenizer</span>=<span style=\"color: #800080; text-decoration-color: #800080\">tokenizer</span>, <span style=\"color: #808000; text-decoration-color: #808000\">hub_model_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"your-username/my-awesome-model\"</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">...</span> <span style=\"font-weight: bold\">)</span>\n",
       "```===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> =====\n",
       "# Step <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. Push everything to the Hub\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">api.upload_folder</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">repo_id</span>=<span style=\"color: #800080; text-decoration-color: #800080\">repo_id</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">folder_path</span>=<span style=\"color: #800080; text-decoration-color: #800080\">repo_local_path</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">path_in_repo</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\".\"</span>,\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Your model is pushed to the Hub. You can view your model here: \"</span>, repo_url<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "### .\n",
       "\n",
       "By using `push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the \n",
       "Hub**.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> =====\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">processor.push_to_hub</span><span style=\"font-weight: bold\">(</span>hub_model_id<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">trainer.push_to_hub</span><span style=\"font-weight: bold\">(</span>**kwargs<span style=\"font-weight: bold\">)</span>\n",
       "```\n",
       "\n",
       "# <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Inference\n",
       "\n",
       "Now comes the exciting part, using our fine-tuned model! In this section, we'll show how you can load your model \n",
       "from the hub and use it for inference.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> =====\n",
       "Finally, if you want, you can push your model up to the hub. Here, we'll push it up if you specified \n",
       "`<span style=\"color: #808000; text-decoration-color: #808000\">push_to_hub</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>` in the training configuration. Note that in order to push to hub, you'll have to have git-lfs \n",
       "installed and be logged into your Hugging Face account <span style=\"font-weight: bold\">(</span>which can be done via `huggingface-cli login`<span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "```python\n",
       "kwargs = <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"finetuned_from\"</span>: model.config._name_or_path,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tasks\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"image-classification\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"dataset\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'beans'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"tags\"</span>: |<span style=\"color: #008000; text-decoration-color: #008000\">'image-classification'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"font-weight: bold\">}</span>===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> =====\n",
       "The `<span style=\"color: #808000; text-decoration-color: #808000\">push_to_hub</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>` argument will allow us to push the model to the Hub after training; you'll find the \n",
       "repository under your user profile in the location defined by `output_dir`. Note that you can specify the name of \n",
       "the repository you want to push to with the `hub_model_id` argument <span style=\"font-weight: bold\">(</span>in particular, you will have to use this \n",
       "argument to push to an organization<span style=\"font-weight: bold\">)</span>. For instance, when we pushed the model to the |`huggingface-course` \n",
       "organization<span style=\"font-weight: bold\">](</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/huggingface-course),</span> we added \n",
       "`<span style=\"color: #808000; text-decoration-color: #808000\">hub_model_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"huggingface-course/mt5-finetuned-amazon-en-es\"</span>` to `Seq2SeqTrainingArguments`.===== Document <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> =====\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">if</span><span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "At a lower level, accessing the Model Hub can be done directly on models, tokenizers, and configuration objects via\n",
       "their `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">push_to_hub</span><span style=\"font-weight: bold\">()</span>` method. This method takes care of both the repository creation and pushing the model and \n",
       "tokenizer files directly to the repository. No manual handling is required, unlike with the API we'll see below.\n",
       "\n",
       "To get an idea of how it works, let's first initialize a model and a tokenizer:\n",
       "\n",
       "<span style=\"font-weight: bold\">{</span>#if fw === <span style=\"color: #008000; text-decoration-color: #008000\">'pt'</span><span style=\"font-weight: bold\">}</span>\n",
       "```py\n",
       "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
       "\n",
       "checkpoint = <span style=\"color: #008000; text-decoration-color: #008000\">\"camembert-base\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: Retrieved documents:\n",
       "===== Document \u001b[1;36m0\u001b[0m =====\n",
       "--push_to_hub\n",
       "```===== Document \u001b[1;36m1\u001b[0m =====\n",
       "```py\n",
       ">>> \u001b[1;35mtrainer.push_to_hub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "```\n",
       "\u001b[1m<\u001b[0m\u001b[35m/\u001b[0m\u001b[95mpt\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39m<tf>\u001b[0m\n",
       "\u001b[39mShare a model to the Hub with |`PushToHubCallback`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m. In the |`PushToHubCallback`\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m function, add:\u001b[0m\n",
       "\n",
       "\u001b[39m- An output directory for your model.\u001b[0m\n",
       "\u001b[39m- A tokenizer.\u001b[0m\n",
       "\u001b[39m- The `hub_model_id`, which is your Hub username and model name.\u001b[0m\n",
       "\n",
       "\u001b[39m```py\u001b[0m\n",
       "\u001b[39m>>> from transformers import PushToHubCallback\u001b[0m\n",
       "\n",
       "\u001b[39m>>\u001b[0m\u001b[1m>\u001b[0m push_to_hub_callback = \u001b[1;35mPushToHubCallback\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[33m...\u001b[0m     \u001b[33moutput_dir\u001b[0m=\u001b[32m\"./your_model_save_path\"\u001b[0m, \u001b[33mtokenizer\u001b[0m=\u001b[35mtokenizer\u001b[0m, \u001b[33mhub_model_id\u001b[0m=\u001b[32m\"your\u001b[0m\u001b[32m-username/my-awesome-model\"\u001b[0m\n",
       "\u001b[33m...\u001b[0m \u001b[1m)\u001b[0m\n",
       "```===== Document \u001b[1;36m2\u001b[0m =====\n",
       "# Step \u001b[1;36m7\u001b[0m. Push everything to the Hub\n",
       "    \u001b[1;35mapi.upload_folder\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrepo_id\u001b[0m=\u001b[35mrepo_id\u001b[0m,\n",
       "        \u001b[33mfolder_path\u001b[0m=\u001b[35mrepo_local_path\u001b[0m,\n",
       "        \u001b[33mpath_in_repo\u001b[0m=\u001b[32m\".\"\u001b[0m,\n",
       "    \u001b[1m)\u001b[0m\n",
       "\n",
       "    \u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"Your model is pushed to the Hub. You can view your model here: \"\u001b[0m, repo_url\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "### .\n",
       "\n",
       "By using `push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the \n",
       "Hub**.===== Document \u001b[1;36m3\u001b[0m =====\n",
       "\u001b[1;35mprocessor.push_to_hub\u001b[0m\u001b[1m(\u001b[0mhub_model_id\u001b[1m)\u001b[0m\n",
       "\u001b[1;35mtrainer.push_to_hub\u001b[0m\u001b[1m(\u001b[0m**kwargs\u001b[1m)\u001b[0m\n",
       "```\n",
       "\n",
       "# \u001b[1;36m4\u001b[0m. Inference\n",
       "\n",
       "Now comes the exciting part, using our fine-tuned model! In this section, we'll show how you can load your model \n",
       "from the hub and use it for inference.===== Document \u001b[1;36m4\u001b[0m =====\n",
       "Finally, if you want, you can push your model up to the hub. Here, we'll push it up if you specified \n",
       "`\u001b[33mpush_to_hub\u001b[0m=\u001b[3;92mTrue\u001b[0m` in the training configuration. Note that in order to push to hub, you'll have to have git-lfs \n",
       "installed and be logged into your Hugging Face account \u001b[1m(\u001b[0mwhich can be done via `huggingface-cli login`\u001b[1m)\u001b[0m.\n",
       "\n",
       "```python\n",
       "kwargs = \u001b[1m{\u001b[0m\n",
       "    \u001b[32m\"finetuned_from\"\u001b[0m: model.config._name_or_path,\n",
       "    \u001b[32m\"tasks\"\u001b[0m: \u001b[32m\"image-classification\"\u001b[0m,\n",
       "    \u001b[32m\"dataset\"\u001b[0m: \u001b[32m'beans'\u001b[0m,\n",
       "    \u001b[32m\"tags\"\u001b[0m: |\u001b[32m'image-classification'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[1m}\u001b[0m===== Document \u001b[1;36m5\u001b[0m =====\n",
       "The `\u001b[33mpush_to_hub\u001b[0m=\u001b[3;92mTrue\u001b[0m` argument will allow us to push the model to the Hub after training; you'll find the \n",
       "repository under your user profile in the location defined by `output_dir`. Note that you can specify the name of \n",
       "the repository you want to push to with the `hub_model_id` argument \u001b[1m(\u001b[0min particular, you will have to use this \n",
       "argument to push to an organization\u001b[1m)\u001b[0m. For instance, when we pushed the model to the |`huggingface-course` \n",
       "organization\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\u001b[4;94mhttps://huggingface.co/huggingface-course\u001b[0m\u001b[4;94m)\u001b[0m\u001b[4;94m,\u001b[0m we added \n",
       "`\u001b[33mhub_model_id\u001b[0m=\u001b[32m\"huggingface\u001b[0m\u001b[32m-course/mt5-finetuned-amazon-en-es\"\u001b[0m` to `Seq2SeqTrainingArguments`.===== Document \u001b[1;36m6\u001b[0m =====\n",
       "\u001b[1m{\u001b[0m\u001b[35m/\u001b[0m\u001b[95mif\u001b[0m\u001b[1m}\u001b[0m\n",
       "\n",
       "At a lower level, accessing the Model Hub can be done directly on models, tokenizers, and configuration objects via\n",
       "their `\u001b[1;35mpush_to_hub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m` method. This method takes care of both the repository creation and pushing the model and \n",
       "tokenizer files directly to the repository. No manual handling is required, unlike with the API we'll see below.\n",
       "\n",
       "To get an idea of how it works, let's first initialize a model and a tokenizer:\n",
       "\n",
       "\u001b[1m{\u001b[0m#if fw === \u001b[32m'pt'\u001b[0m\u001b[1m}\u001b[0m\n",
       "```py\n",
       "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
       "\n",
       "checkpoint = \u001b[32m\"camembert-base\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 1.65 seconds| Input tokens: 1,199 | Output tokens: 20]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 1.65 seconds| Input tokens: 1,199 | Output tokens: 20]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To push a model to the Hub, you can use the            │\n",
       "│ `push_to_hub` method from the trainer object or the `PushToHubCallback` function. Alternatively, you can use    │\n",
       "│ the `api.upload_folder` method or set `push_to_hub=True` in the training configuration. Lastly, you can access  │\n",
       "│ the Model Hub directly on models, tokenizers, and configuration objects via their `push_to_hub()` method.'}     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'To push a model to the Hub, you can use the            │\n",
       "│ `push_to_hub` method from the trainer object or the `PushToHubCallback` function. Alternatively, you can use    │\n",
       "│ the `api.upload_folder` method or set `push_to_hub=True` in the training configuration. Lastly, you can access  │\n",
       "│ the Model Hub directly on models, tokenizers, and configuration objects via their `push_to_hub()` method.'}     │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: To push a model to the Hub, you can use the `push_to_hub` method from the trainer object or the </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">`PushToHubCallback` function. Alternatively, you can use the `api.upload_folder` method or set `push_to_hub=True` </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">in the training configuration. Lastly, you can access the Model Hub directly on models, tokenizers, and </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">configuration objects via their `push_to_hub()` method.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: To push a model to the Hub, you can use the `push_to_hub` method from the trainer object or the \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m`PushToHubCallback` function. Alternatively, you can use the `api.upload_folder` method or set `push_to_hub=True` \u001b[0m\n",
       "\u001b[1;38;2;212;183;2min the training configuration. Lastly, you can access the Model Hub directly on models, tokenizers, and \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mconfiguration objects via their `push_to_hub()` method.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 27.25 seconds| Input tokens: 3,205 | Output tokens: 465]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 27.25 seconds| Input tokens: 3,205 | Output tokens: 465]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "To push a model to the Hub, you can use the `push_to_hub` method from the trainer object or the `PushToHubCallback` function. Alternatively, you can use the `api.upload_folder` method or set `push_to_hub=True` in the training configuration. Lastly, you can access the Model Hub directly on models, tokenizers, and configuration objects via their `push_to_hub()` method.\n"
     ]
    }
   ],
   "source": [
    "agent_output = agent.run(\"How can I push a model to the Hub?\")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(agent_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = text_splitter.split_documents([source_docs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

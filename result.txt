进行数据分析，首先我们要知道python会用到的库：Pandas库、Matplotlib库。



数据分析的基本过程分为：提出问题、理解数据、数据清洗、构建模型、数据可视化。

（1）提出问题：明确分析的目的；

（2）理解数据：主要从数据的采集、导入、查看数据的描述统计信息等方面对数据有基本了解；

（3）数据清洗：主要有6个步骤：选择子集、列名重命名、缺失数据处理、数据类型转换、数据排序、异常值处理；

（4）构建模型：

（5）数据可视化：

接下来我们以朝阳医院2016年的销售数据作为样本，进行简单的数据分析。



一、提出问题：

拿到这组数据，首先我们要明确分析的目的，我们是想知道以下数据：客户的月均消费次数；客户的月均消费金额；客单价；客户的消费趋势。



二、理解数据，对数据有大体认知

首先导入会用到的库。

import pandas as pd
import matplotlib.pyplot as plt
然后载入数据。首次读取excel文件，需要安装xlrd库。

在pandas中，常用的载入函数是read_csv、read_excel和read_table，table可以读取txt。若是服务器相关的部署，则还会用到read_sql，直接访问数据库，但它必须配合mysql相关包。

sales=pd.read_excel('C:\\Users\\test data\\朝阳医院2016年销售数据.xlsx',dtype='object')
接下来查看数据概览。以下代码列出了数据的各个字段名，共6577行，7列，6577 non-null 说明各字段没有空值,’社保账号‘有5个空值,object说明类型均为object。

print(sales.info())   

若需要查看文件有多少行多少列，可用以下代码：

print(sales.shape)

查看文件的部分数据，用head函数显示头部数据，默认5行，可自由设置参数。如果是尾部数据，可用tail函数。

print(sales.head())



三、数据清洗

（1）选择子集：

若数据量较大，且不是每行/列信息都有用，可针对性的选择子集进行分析。可用ix函数， 它通过DataFrame的索引和轴标签选择行或列的子集，代码形式为：

subsales=sales.ix[:,:]
因为数据本身不多，此处不再取子集。

（2）列名重命名：

将'购药时间'改为’销售时间‘，利用inplace=True可直接在原数据框内改动

sales.rename(columns={'购药时间':'销售时间'},inplace=True)
再次显示文件信息，查看是否列名是否已经修改。

print(sales.head())

（3）缺失数据处理：删除销售数据和社保卡号为空的行。

有两种办法：①删除缺失数据（dropna方法）；② 填充缺失数据（filina方法）。

此处使用dropna直接删除缺失数据。

dropna默认丢弃任何含有缺失值的行，即dropna()等价于dropna(axis=0,how='any')；

若要丢弃列，则axis=1;how 参数可选的值为 any 或者 all，若要丢弃全为NA的行/列，则how='all';

dropna还有一个参数-thresh，该参数的类型为整数，它的作用是，比如
thresh=3，会在一行中至少有 3 个非 NA
值时将其保留。dropna(axis=1,thresh=1),保留至少有1个非NA值的列。还可以通过dropna的参数subset移除指定列为空的数据。

此处代码为：

sales=sales.dropna(subset=['销售时间','社保卡号'],how='any')
（4）数据类型处理：

因导入时均以‘object’类型导入，为便于计算，需要做类型转换。

sales['销售数量']=sales['销售数量'].astype('float')
sales['应收金额']=sales['应收金额'].astype('float')
sales['实收金额']=sales['实收金额'].astype('float')
print('转换后的数据类型为：\n',sales.dtypes)

销售时间列格式现在为‘2016-01-01 星期五’，需先把日期和星期切分，然后转换为时间类型。

此处介绍2种办法：

①

sales['销售时间'], sales['销售星期'] = sales['销售时间'].str.split(' ', 1).str
切分好之后，把销售时间变为时间类型。以下代码中，erros='coerce'如果原始数据不符合日期的格式，转换后的值会变为NAT。format是原始数据的日期格式。

sales.ix[:,'销售时间']=pd.to_datetime(sales.ix[:,'销售时间'],format='%Y-%m-%d',errors='coerce')
print(sales.dtypes)

②

def split_time(t):
t=t.split(' ',1)[0]
return t
sales['销售时间-格式化']=sales['销售时间'].apply(split_time)
sales['销售时间-格式化']=pd.to_datetime(sales['销售时间-格式化'],format='%Y-%m-%d',errors='coerce')
print(sales.dtypes)
此处采用的方法一。

在转换过程中，可能会出现不符合日期格式的数值会转换为空值：这时删除缺失数据

sales=sales.dropna(subset=['销售时间','社保卡号'],how='any')
（5）排序

时间一列现在是无序排列的，需要排序一下，排序之后索引会被打乱，所以也需要重置一下索引。

如下为按照销售日期进行升序排列，代码中，对数据按照时间排序，by:按那几列排序；ascending=True 表示升序排列；ascending=False表示降序排列。

sales=sales.sort_values(by='销售时间',ascending=True) 
print('排序后的数据集为：\n',sales.head())

排序之后再重置一下index。其中代码中drop=True会舍弃原有的index，drop默认等于’Fales‘，保留原索引，增加一列新的索引。

sales=sales.reset_index(drop=True) 
print(sales.head())

6、异常值处理

排序后，查看每一列的描述统计信息

print(sales.describe())

销售数量不可能小于0。所以需去除小于0的值。

sales=sales.ix[sales.销售数量>0,:] 
或者这样写：

sales=sales[(sales['销售数量']>0)&(sales['应收金额']>0)&(sales['实收金额']>0)]
然后打印结果看下是否还有异常值：

print(sales.describe())

完成上述步骤，基本的数据清洗工作完成，接下来对数据进行分析。



四、构建模型

现在针对最开始提出的问题，运用不同的分析方法进行解决。

（1）月均消费次数：等于总消费次数 / 月份数

在此约定，总消费次数：同一天内，同一个人发生的所有消费算作一次消费。
根据列名（销售时间，社区卡号），如果这两个列值同时相同，只保留一条，将重复的数据删除。
总消费次数如下，即subsales有多少行信息：

subsales=sales.drop_duplicates(subset=['销售时间','社保卡号'])
total=subsales.shape[0]  
print(total)

计算月份数

month=(subsales['销售时间'].max()-subsales['销售时间'].min()).days//30
计算月均消费次数

t1=total/month
print('月均消费次数为：','%.2f'%t1)

（2） 月均消费金额：等于总消费金额/月份数。在计算总金额的时候不能去重，需要都计算上金额。

total1=sales['实收金额'].sum()
t2=total1/month
print('月均消费金额为：','%.2f'%t2)

（3） 客单价：等于总消费金额/总消费次数

total1为总消费金额；total为总消费次数

t3=total1/total
print('客单价为：','%.2f'%t3)

（4）消费趋势

要了解消费趋势，我们先根据时间的不同，了解下消费金额的大体变化。

需要对去重后的数据按照天进行重新采样，首先要把索引变成时间：

sales.index = pd.DatetimeIndex(sales['销售时间'])


以下为按日采样：

salesdays=sales.resample('D').count()     
#count（）会对取样的数据按行进行计数
salesdays.plot(x=salesdays.index,y='实收金额')
plt.xlabel('Time')
plt.ylabel('Money')
plt.title('sales volume-day')
plt.show()

这样我们对每天的消费数据有一个大体的掌握，然后按月采样看下每个月的总销售额是否有很大的差异。



以下为按月采样：

salesm = sales.resample('M').sum()   
#sum()会为取样的数值数据按月进行求和，打印出来如下：
print(salesm.head())

salesm.plot(x = salesm.index, y = '实收金额')
plt.xlabel('Time')
plt.ylabel('month')
plt.title('sales voume(month)')
plt.show()

了解下月均的客流量

salesm = sales.resample('M').count()
salesm.plot(x = salesm.index, y = '实收金额')
plt.xlabel('Time')
plt.ylabel('people')
plt.title('passager flow(month)')
plt.show() 

这时候可以看出，医院的销售总额和客流量基本成正比。但六月数据较异常，可能该月份人均客单价提升所致。具体问题还需在具体分析。以上仅提供分析思路参考。一、设计要求
项目名称
年度人口结构数据分析与可视化
目标
开发一个数据处理与分析系统，旨在读取年度人口结构数据，对数据进行清洗和预处理，计算关键指标的均值，并生成可视化图表展示人口结构变化趋势。

• 数据清洗与预处理
• 检查数据中的缺失值并打印缺失值情况。
• 检查数据类型并打印数据类型。
• 将非数值数据转换为NaN，并去除小于或等于0的异常值。
• 再次检查清洗后的数据缺失值情况，并用前向填充方法填充缺失值。
• 数据分组与聚合
• 将“指标”列设置为索引，并转置数据框。
• 计算每年总人口、城镇人口和乡村人口的均值。
• 数据可视化
• 生成并展示以下趋势图
• 年末总人口趋势图
• 城镇人口与乡村人口趋势图
• 总人口均值趋势图
• 城镇人口均值与乡村人口均值趋势图

二、设计思路
这段代码主要是为了处理和分析年度人口结构数据，并生成几个趋势图表来展示人口数据的变化。以下是代码的详细设计思路和步骤分析：
1. 读取数据
df = pd.read_csv(file_path, encoding='gbk')
首先从指定的CSV文件中读取数据，文件编码为’gbk’。
2. 数据清洗和预处理
print("缺失值情况：")
print("\n数据类型：")
print("略。。。。。")
检查数据中的缺失值和数据类型，确保数据的完整性和类型正确性。
数据预处理 - 去除异常值
df_cleaned = df.copy()
for col in numeric_columns:
    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')  # 将非数值数据转换为NaN
    
将所有的数值列转换为数值类型，处理过程中将非数值数据转换为NaN，并且去除小于或等于0的异常值。
再次检查缺失值并填充
print("\n清洗后的缺失值情况：")
print(df_cleaned.isnull().sum())
# 略....
# 略....
再次检查清洗后的数据缺失值情况，并用前向填充方法填充缺失值。
3. 数据分组与聚合
df_grouped = df_cleaned.set_index('指标').T
df_grouped['总人口均值'] = df_grouped[['男性人口(万人)', '女性人口(万人)']].mean(axis=1)
# 略....
# 略....
# 略....
将指标列设置为索引，并转置数据框。计算每年总人口、城镇人口和乡村人口的均值。
4. 数据可视化
分别生成总人口趋势图、城镇与乡村人口趋势图、总人口均值趋势图、城镇人口均值与乡村人口均值趋势图：
# 总人口趋势
plt.figure(figsize=(10, 6))
# 略.....
# 略.....
plt.show()
# 城镇人口与乡村人口趋势
plt.figure(figsize=(10, 6))
# 略.....
# 略.....
plt.show()
# 总人口均值
plt.figure(figsize=(10, 6))
# 略.....
# 略.....
plt.show()
# 城镇人口均值与乡村人口均值
plt.figure(figsize=(10, 6))
# 略.....
# 略.....
plt.xlabel('年份')
plt.ylabel('人口(万人)')
plt.legend()
plt.grid(True)
plt.show()
以上代码依次生成总人口、城镇与乡村人口、总人口均值以及城镇与乡村人口均值的趋势图，并设置图表的标题、坐标轴标签和网格显示，确保图表清晰易读。
该代码的设计思路是通过读取、清洗和预处理人口结构数据，然后进行数据分组与聚合，最终通过可视化展示数据的变化趋势。这种方法有助于直观地理解人口变化的总体趋势和结构变化，为进一步分析和决策提供依据。
三、可视化分析
年末总人口趋势图
年末总人口趋势图展示了各年份末总人口的变化情况。这一图表能够直观反映出人口增长或减少的总体趋势，帮助我们理解某一时期内人口的增长速度及其变化规律。通过观察图中各点的分布和连接线的走向，可以判断出人口是否呈现稳定增长、波动或是其他变化趋势。这对于政府部门制定人口政策和资源分配计划具有重要参考价值。
Python 数据分析实战：使用 Pandas 进行数据清洗与可视化
数据科学是一个快速发展的领域，Python 成为了该领域中最受欢迎的编程语言之一。其中一个重要的原因是 Python 拥有丰富的库支持，如 NumPy、Pandas、Matplotlib 等。本文将详细介绍如何使用 Pandas 库来进行数据清洗、处理以及可视化。

1. 安装必要的库
首先，确保你的环境中已安装了必要的库。如果还没有安装，可以通过 pip 或 conda 来安装它们：

pip install pandas matplotlib seaborn
2. 导入库并准备数据
接下来，我们需要导入 Pandas 库，并加载一个数据集来演示数据处理过程。这里我们使用一个虚构的数据集来模拟真实场景。

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 加载数据集
url = "https://example.com/dataset.csv"
df = pd.read_csv(url)

# 查看前几条记录
print(df.head())
3. 数据清洗
数据清洗是数据分析的重要步骤之一。常见的数据清洗任务包括处理缺失值、去除重复记录、转换数据类型等。

# 处理缺失值
print(df.isnull().sum())  # 查看每列的缺失值数量
df.dropna(inplace=True)   # 删除含有缺失值的行

# 去除重复记录
df.drop_duplicates(inplace=True)

# 转换数据类型
df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')
4. 数据分析
一旦数据被清洗干净，我们就可以开始对其进行分析了。这里我们通过描述性统计来了解数据的基本情况。

# 描述性统计
print(df.describe())

# 分组分析
grouped_data = df.groupby('category').mean()
print(grouped_data)
5. 数据可视化
数据可视化是呈现分析结果的重要手段之一。使用 Matplotlib 和 Seaborn 库可以方便地绘制图表。

# 设置绘图风格
sns.set(style="whitegrid")

# 绘制柱状图
plt.figure(figsize=(10, 6))
sns.barplot(x='category', y='value', data=df)
plt.title("Category Value Distribution")
plt.show()

# 绘制散点图
plt.figure(figsize=(10, 6))
sns.scatterplot(x='date', y='value', hue='category', data=df)
plt.title("Value Over Time by Category")
plt.show()
6. 数据导出
分析完成后，我们可能还需要将处理后的数据导出，以便后续使用。

# 导出数据到 CSV 文件
df.to_csv("cleaned_data.csv", index=False)
7. 总结
通过上述步骤，我们展示了如何使用 Pandas 库来处理数据，包括数据的加载、清洗、分析以及可视化。Python 强大的库支持使得数据分析变得更加简单高效。无论是学术研究还是商业应用，掌握这些技能都将使你在数据科学领域中更具竞争力。

代码完整示例
下面是将上述所有代码片段整合在一起的完整示例，你可以复制并在本地环境中运行。

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 加载数据集
url = "https://example.com/dataset.csv"
df = pd.read_csv(url)

# 查看前几条记录
print(df.head())

# 数据清洗
print(df.isnull().sum())  # 查看每列的缺失值数量
df.dropna(inplace=True)   # 删除含有缺失值的行

# 去除重复记录
df.drop_duplicates(inplace=True)

# 转换数据类型
df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')

# 数据分析
print(df.describe())

# 分组分析
grouped_data = df.groupby('category').mean()
print(grouped_data)

# 数据可视化
sns.set(style="whitegrid")

plt.figure(figsize=(10, 6))
sns.barplot(x='category', y='value', data=df)
plt.title("Category Value Distribution")
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='date', y='value', hue='category', data=df)
plt.title("Value Over Time by Category")
plt.show()

# 数据导出
df.to_csv("cleaned_data.csv", index=False)Python基础的数据分析
随着数据在现代社会中的作用变得越来越重要，数据分析成为了一个非常关键的技能。Python作为一种强大且易用的编程语言，广泛用于数据科学和数据分析领域。它拥有丰富的库和工具来处理、分析和可视化数据。本文将为你介绍Python基础的数据分析方法，并提供详细的代码示例，帮助你快速入门数据分析领域。


1. 数据分析的基本步骤
数据分析通常包含以下几个步骤：

获取数据：从文件、数据库或API中获取数据。
清理数据：处理缺失值、异常值，并转换数据类型。
探索数据：通过统计和可视化手段了解数据的分布和特点。
建模与分析：使用机器学习或统计模型对数据进行分析。
结果可视化：将分析结果用图表或报告呈现出来。

2. Python中的核心数据分析库
Python拥有一些常用的库，几乎涵盖了数据分析的各个方面：

NumPy：用于高效的数组和矩阵操作。
Pandas：用于数据处理和操作，尤其适合表格数据。
Matplotlib 和 Seaborn：用于数据的可视化。
Scikit-learn：用于机器学习和建模。

3. 数据分析示例项目
接下来，通过一个示例项目来展示如何使用这些库进行数据分析。我们将使用Pandas来读取和处理数据，并用Matplotlib和Seaborn来进行数据可视化。

3.1 导入所需库
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
1
2
3
4
首先，使用 pip install 命令确保已经安装了这些库：

pip install numpy pandas matplotlib seaborn
1
3.2 加载数据
在实际应用中，数据通常保存在CSV文件、数据库或API中。我们可以使用Pandas的 read_csv 函数来加载CSV数据：

# 加载示例数据集
data = pd.read_csv('your_data.csv')

# 查看数据的前几行
print(data.head())
1
2
3
4
5
3.3 数据清理
数据清理是数据分析的重要步骤，因为现实中的数据可能包含缺失值、重复值或异常值。我们可以使用Pandas提供的函数来处理这些问题。

处理缺失值：可以使用 dropna() 删除缺失值，也可以用 fillna() 填充缺失值。
# 查看缺失值
print(data.isnull().sum())

# 删除含有缺失值的行
data_cleaned = data.dropna()

# 或者，用中位数填充缺失值
data['column_name'].fillna(data['column_name'].median(), inplace=True)
1
2
3
4
5
6
7
8
去除重复值：可以使用 drop_duplicates() 去除重复行。
data_cleaned = data_cleaned.drop_duplicates()
1
3.4 数据探索
在数据清理完成后，可以开始探索数据。首先查看数据的统计信息：

# 查看基本统计信息
print(data_cleaned.describe())
1
2
然后，我们可以使用可视化工具来更好地理解数据。

直方图：用于查看某个数值型数据的分布。
# 使用Matplotlib绘制直方图
plt.hist(data_cleaned['column_name'], bins=30, edgecolor='black')
plt.title('Column Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()
1
2
3
4
5
6
散点图：查看两个变量之间的关系。
# 使用Seaborn绘制散点图
sns.scatterplot(x='column1', y='column2', data=data_cleaned)
plt.title('Scatter Plot of Column1 vs Column2')
plt.show()
1
2
3
4
热力图：查看变量之间的相关性。
# 计算相关性矩阵
corr_matrix = data_cleaned.corr()

# 使用Seaborn绘制热力图
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()
1
2
3
4
5
6
7
3.5 数据建模
数据建模是数据分析中的一个重要部分，可以使用机器学习算法对数据进行建模。Python的 scikit-learn 提供了大量的机器学习模型，例如线性回归、决策树、随机森林等。

下面是一个简单的线性回归模型示例：

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 分割数据为训练集和测试集
X = data_cleaned[['feature1', 'feature2']]
y = data_cleaned['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建线性回归模型并训练
model = LinearRegression()
model.fit(X_train, y_train)

# 预测并评估模型
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
3.6 数据可视化
建模结束后，通常会通过可视化来展示结果。下面展示如何使用Matplotlib绘制模型预测值与实际值的对比图：

# 绘制预测值与实际值的对比图
plt.scatter(y_test, y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)
plt.title('Actual vs Predicted')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.show()
1
2
3
4
5
6
7


4. Python 数据分析常用库详细介绍
在进行 Python 数据分析时，几个核心库是必须掌握的，它们涵盖了数据的加载、处理、可视化和建模等多个步骤。下面详细介绍几个最常用的库及其功能：

4.1 NumPy
NumPy 是 Python 中用于处理数值计算的基础库，提供了支持高效数组和矩阵运算的多维数组对象。它在处理大规模数据时尤其高效，是许多其他库的基础。

主要功能包括：

多维数组对象 ndarray
数学函数库（如矩阵运算、随机数生成等）
与 Python 的列表、元组等数据结构的高效集成
使用示例：

import numpy as np

# 创建一个一维数组
arr = np.array([1, 2, 3, 4, 5])

# 创建一个二维数组
matrix = np.array([[1, 2, 3], [4, 5, 6]])

# 进行矩阵乘法运算
result = np.dot(matrix, matrix.T)

print(result)
1
2
3
4
5
6
7
8
9
10
11
12
4.2 Pandas
Pandas 是用于数据操作和分析的库，特别适合处理表格数据。它的 DataFrame 对象相当于一个 Excel 表格，能轻松进行数据的筛选、清洗、合并等操作。

Pandas 功能包括：

强大的数据结构（Series 和 DataFrame）
读取和写入多种文件格式（如 CSV、Excel、SQL 数据库等）
数据清洗、处理、筛选、转换等
使用示例：

import pandas as pd

# 创建一个DataFrame
data = {'Name': ['Alice', 'Bob', 'Charlie'],
        'Age': [25, 30, 35],
        'City': ['New York', 'Paris', 'London']}
df = pd.DataFrame(data)

# 读取CSV文件
df_from_csv = pd.read_csv('example.csv')

# 筛选年龄大于30的数据
filtered_df = df[df['Age'] > 30]

print(filtered_df)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
4.3 Matplotlib 和 Seaborn
Matplotlib 是 Python 中最流行的绘图库之一，适用于创建各种静态、动态和交互式图表。Seaborn 是基于 Matplotlib 的高级库，能够更简单地创建复杂的统计图表。

主要功能包括：

绘制简单的图表（如折线图、柱状图、饼图等）
处理多维数据的可视化
配合 Pandas 进行数据的快速可视化
使用示例：

import matplotlib.pyplot as plt
import seaborn as sns

# 简单的折线图
x = [1, 2, 3, 4, 5]
y = [2, 3, 5, 7, 11]
plt.plot(x, y)
plt.title('Simple Line Plot')
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.show()

# 使用Seaborn创建箱线图
sns.boxplot(x='City', y='Age', data=df)
plt.show()
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
4.4 Scikit-learn
Scikit-learn 是 Python 中最常用的机器学习库，包含大量经典的机器学习算法。它支持监督学习和无监督学习，并提供了数据预处理、模型选择和评估的功能。

主要功能包括：

线性回归、逻辑回归、决策树、随机森林、支持向量机等
数据预处理（标准化、归一化等）
模型的训练、预测和评估
使用示例：

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 加载数据
X = df[['Age']]
y = [0, 1, 0]  # 简单的二分类标签

# 将数据分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 创建并训练模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 进行预测并评估
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19


5. 实践中的完整数据分析流程
现在我们将这些工具结合起来，通过一个实际的例子展示完整的数据分析流程。假设我们有一个关于房价的数据集，想要分析房价与面积之间的关系，并构建一个简单的预测模型。

5.1 步骤 1：加载数据
首先，使用 Pandas 读取数据，并快速浏览数据的前几行：

import pandas as pd

# 读取CSV文件
data = pd.read_csv('house_prices.csv')

# 查看数据的前几行
print(data.head())
1
2
3
4
5
6
7
5.2 步骤 2：数据清理
我们需要确保数据没有缺失值，并去除不必要的列：

# 检查缺失值
print(data.isnull().sum())

# 删除含有缺失值的行
data_cleaned = data.dropna()

# 只保留我们感兴趣的列：房价和面积
data_cleaned = data_cleaned[['Price', 'Area']]
1
2
3
4
5
6
7
8
5.3 步骤 3：数据可视化
使用 Seaborn 绘制散点图，查看房价与面积之间的关系：

import seaborn as sns
import matplotlib.pyplot as plt

# 绘制散点图
sns.scatterplot(x='Area', y='Price', data=data_cleaned)
plt.title('Area vs Price')
plt.show()
1
2
3
4
5
6
7
5.4 步骤 4：数据建模
接下来，我们使用线性回归模型预测房价。首先将数据分为训练集和测试集，然后训练模型并进行预测。

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 分割数据为训练集和测试集
X = data_cleaned[['Area']]
y = data_cleaned['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 创建并训练线性回归模型
model = LinearRegression()
model.fit(X_train, y_train)

# 预测房价
y_pred = model.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
5.5 步骤 5：结果可视化
最后，绘制模型的预测结果与实际值的对比图：

# 绘制实际值与预测值的对比
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', label='Predicted')
plt.xlabel('Area')
plt.ylabel('Price')
plt.legend()
plt.show()
1
2
3
4
5
6
7


6. 数据分析实践中的挑战与解决方案
在进行数据分析的过程中，除了使用合适的工具和技术，还会遇到一些实际的挑战。我们将在本节中讨论常见的挑战，并给出相应的解决方案。

6.1 数据质量问题
问题：数据分析过程中，数据的质量往往决定了分析结果的准确性。然而，实际的数据集经常存在缺失值、重复值、错误的格式等问题，影响后续分析。

解决方案：

处理缺失值：对于缺失值，可以选择删除含有缺失值的行，或者使用均值、中位数、众数等进行填充。
重复值检测与处理：使用 Pandas 的 drop_duplicates() 方法删除重复数据。
数据格式转换：确保数据类型正确，例如，将字符串类型的数字转换为数值型。
示例代码：

# 检查缺失值
print(data.isnull().sum())

# 填充缺失值
data_filled = data.fillna(data.mean())

# 删除重复值
data_cleaned = data_filled.drop_duplicates()

# 数据类型转换
data_cleaned['Age'] = pd.to_numeric(data_cleaned['Age'], errors='coerce')
1
2
3
4
5
6
7
8
9
10
11
6.2 特征选择与工程
问题：在构建机器学习模型时，并不是所有的特征（变量）都有用，冗余或无关的特征会影响模型的表现。

解决方案：

相关性分析：可以通过计算特征之间的相关系数来选择重要特征。
特征工程：对数据进行转换，例如将分类变量转换为数值变量，或者对特征进行归一化处理。
示例代码：

import seaborn as sns

# 计算特征之间的相关系数
corr_matrix = data_cleaned.corr()

# 可视化相关系数矩阵
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()
1
2
3
4
5
6
7
8
6.3 模型过拟合与欠拟合
问题：

过拟合：模型在训练集上表现很好，但在测试集上表现较差，这是因为模型过于复杂，拟合了训练集中的噪音。
欠拟合：模型在训练集和测试集上都表现不好，说明模型过于简单，无法捕捉数据中的规律。
解决方案：

正则化：通过 L1 和 L2 正则化可以限制模型的复杂度，减少过拟合。
交叉验证：通过交叉验证选择最优的模型参数，避免过拟合或欠拟合。
数据增强：通过扩展训练数据集，增加数据的多样性，帮助模型学到更多的模式。
示例代码：

from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# 使用岭回归进行正则化
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# 交叉验证
scores = cross_val_score(ridge, X_train, y_train, cv=5)
print(f"Cross-validation scores: {scores}")
1
2
3
4
5
6
7
8
9
10
6.4 数据可视化的有效性
问题：虽然绘制图表是数据分析的重要步骤，但如果图表不清晰或不具解释性，可能会误导读者。

解决方案：

简洁明了的图表：确保图表能够传达关键信息，去除不必要的元素（如多余的线条或颜色）。
注释和标题：为图表添加有意义的标题和注释，使得观众能够轻松理解图表传达的信息。
选择合适的图表类型：根据数据的特点选择合适的图表类型，例如用柱状图展示分类变量的分布，用散点图展示两个连续变量的关系。
示例代码：

import matplotlib.pyplot as plt

# 添加标题和注释的散点图
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', label='Predicted')
plt.xlabel('Area')
plt.ylabel('Price')
plt.title('Actual vs Predicted Prices')
plt.legend()
plt.show()
1
2
3
4
5
6
7
8
9
10


7. Python 数据分析工具的生态系统扩展
除了前面介绍的核心工具，Python 数据分析的生态系统还有许多其他强大的库，可以根据具体的需求选择使用：

7.1 Statsmodels
Statsmodels 是一个专注于统计模型的库，适用于更复杂的统计分析，提供了详细的统计检验和结果解释。它比 Scikit-learn 更适合传统的统计建模。

示例代码：

import statsmodels.api as sm

# 构建线性回归模型
X = sm.add_constant(X_train)  # 添加常数项
model = sm.OLS(y_train, X)
result = model.fit()

# 输出模型结果
print(result.summary())
1
2
3
4
5
6
7
8
9
7.2 TensorFlow 和 PyTorch
当数据分析涉及深度学习时，TensorFlow 和 PyTorch 是两个主要的框架。它们不仅用于构建和训练深度学习模型，还支持复杂的自动微分和 GPU 加速计算。

示例代码（使用 TensorFlow 构建简单神经网络）：

import tensorflow as tf

# 构建一个简单的神经网络模型
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])

# 编译模型
model.compile(optimizer='adam', loss='mse')

# 训练模型
model.fit(X_train, y_train, epochs=10)

# 预测
y_pred = model.predict(X_test)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17


8. Python 数据分析项目案例分享
为进一步理解如何将这些工具和技术应用于实际项目，我们将简单介绍一个 Python 数据分析项目的完整案例。

项目：预测公司员工离职率
目标：通过现有的公司员工数据，预测哪些员工有可能离职，并分析影响员工离职的主要因素。

步骤：

数据准备：获取公司员工的个人信息、工作表现和离职记录。
数据清洗：处理缺失值、错误数据、重复数据等问题。
探索性数据分析：使用可视化工具探索员工离职与工作时长、薪资等特征的关系。
特征工程：将分类变量转换为数值变量，归一化特征，减少数据的冗余。
模型构建与评估：使用决策树、随机森林等模型进行预测，并评估模型的准确性。
结果可视化与报告：展示模型的预测结果，并撰写报告分析预测中发现的规律和模式。

9. 总结
本文详细介绍了 Python 数据分析的基础知识、常用工具和实际应用。从 NumPy 和 Pandas 的基础操作，到 Scikit-learn 和 Statsmodels 的模型构建，再到使用 Matplotlib 和 Seaborn 进行可视化，每个步骤都涵盖了数据分析项目的各个方面。通过掌握这些工具和技巧，你可以轻松上手 Python 数据分析，并能在实际项目中进行灵活应用。
前言：
在当今数据驱动的时代，数据分析成为了解锁洞见和驱动业务决策的关键。Python，作为一种功能强大且易于学习的编程语言，已成为数据分析的首选工具之一。本文将引导您了解使用Python进行数据分析的一般流程，包括数据清洗、分析处理、可视化和得出结论。

数据分析的一般流程：
数据分析通常包括以下几个关键步骤：数据收集、数据清洗、数据分析、数据可视化和得出结论。每一步都是解锁数据潜力的重要环节。接下来，我围绕这个流程展开详细的介绍。

1，数据清洗
数据清洗是数据分析过程中的第一步，目的是将原始数据转化为可分析的格式。这包括处理缺失值、去除重复记录、纠正错误和不一致性。在Python中，`pandas`库是处理数据清洗的利器。例如，使用`DataFrame.drop_duplicates()`可以去除重复的记录。下面我们通过一个例子来展开说明。

这是一份超市的线下销售数据。我先打开excel简单看下数据格式。



我先用Python读取这份表格，这里我使用的是jupyter notebook工具。

使用pandas，读取excel表格数据，并用data.info()方法，查看整体数据情况，可以看到一共有51290条数据，24个字段。



用 duplicated().sum()方法，查看是否存在重复值。可以看到没有重复值



通过 data.isnull().sum()方法，查看是否存在数据缺失情况。可以看到Postal Code字段是存在缺失值的，该字段指的是邮编号码，而我接下来的分析不涉及该字段，所以做忽略处理。



因为原始数据中，该字段格式异常，接下来，我需要对日期字段，进行处理



这里，我是用to_datetime 这个方法来实现的，将日期进行格式转换。



经过处理后，发现日期数据是 “年-月-日 时分秒”，而我的分析中，只需要精确到天即可，于是我进行第二轮处理。



通过以上的操作，我完成了对数据的清洗过程。

2，分析处理和可视化
在完成数据清洗之后，接下来的步骤是分析处理和数据可视化，这两个环节是紧密相连且相辅相成的。

不仅能够对数据进行深入分析，还能以图形的方式直观展示分析结果，使数据的理解更加直接和高效。

我先对客户类别的单量进行分析，对细分客户情况进行分析。



接下来，我对国家、城市的订单分布情况，进行分析



接下来，对产品类型进行分析



对销售额，利润进行分析：查看是否存在季节性特征



通过RFM模型，对客户进行分层



3，数据分析的结论与洞见：引导业务决策
数据分析的终极目标是支持和优化经营决策。当我们完成了以上的数据分析过程后，需要从中分析出合理的决策措施。为了更好的向上汇报，有时候还需要制作一份精美的PPT。



指标体系的说明，是评估业务表现和决策成效的基础，需要清晰地定义每个指标的计算方式和意义。



接下来就可以根据分析的结果，提出对应合理的解决策略。这样可以帮助领导做出更加明智和数据驱动的决策。





我将市场分为了2大类：主要市场、潜在市场，并提出了改善方案。





对于季节性特征表现明显的地区，也需要给出对应的策略



客户分层后，相当于给每个客户打上了一个标签，将客户根据自己的规则体系，重新进行了分类，并给出对应的策略。

比如对新用户，应该以提高转化，引导下单为主，对价格敏感客户，可以提供一些促销活动等等。





结论
可以看到，Python在数据分析方面表现出的强大能力。利用Python，能极大地增强数据分析的效率和直观性。

前言：各种和数据分析相关python库的介绍（前言1~4摘抄自《利用python进行数据分析》）

1.Numpy：
　　Numpy是python科学计算的基础包，它提供以下功能（不限于此）：
　　　　(1)快速高效的多维数组对象ndarray 　　　　(2)用于对数组执行元素级计算以及直接对数组执行数学运算的函数
　　　　(3)用于读写硬盘上基于数组的数据集的工具
　　　　(4)线性代数运算、傅里叶变换，以及随机数生成
　　　　(5)用于将C、C++、Fortran代码集成到python的工具

2.pandas
　　pandas提供了使我们能够快速便捷地处理结构化数据的大量数据结构和函数。pandas兼具Numpy高性能的数组计算功能以及电子表格和关系型数据（如SQL）灵活的数据处理能力。它提供了复杂精细的索引功能，以便更为便捷地完成重塑、切片和切块、聚合以及选取数据子集等操作。
　　对于金融行业的用户，pandas提供了大量适用于金融数据的高性能时间序列功能和工具。
　　DataFrame是pandas的一个对象，它是一个面向列的二维表结构，且含有行标和列标。
　　ps.引用一段网上的话说明DataFrame的强大之处：
　　Excel 2007及其以后的版本的最大行数是1048576，最大列数是16384，超过这个规模的数据Excel就会弹出个框框“此文本包含多行文本，无法放置在一个工作表中”。Pandas处理上千万的数据是易如反掌的事情，同时随后我们也将看到它比SQL有更强的表达能力，可以做很多复杂的操作，要写的code也更少。 说了一大堆它的好处，要实际感触还得动手码代码。

3.matplotlib
　　matplotlib是最流行的用于绘制数据图表的python库。

4.Scipy
　　Scipy是一组专门解决科学计算中各种标准问题域的包的集合。
5.statsmodels： https://github.com/statsmodels/statsmodels
6.scikit-learn： http://scikit-learn.org/stable/

一.数据导入和导出
（一）读取csv文件
1.本地读取

import pandas as pd
df = pd.read_csv('E:\\tips.csv')  #根据自己数据文件保存的路径填写(p.s.  python填写路径时，要么使用/，要么使用\\)
#输出：
     total_bill   tip     sex smoker   day    time  size
0         16.99  1.01  Female     No   Sun  Dinner     2
1         10.34  1.66    Male     No   Sun  Dinner     3
2         21.01  3.50    Male     No   Sun  Dinner     3
3         23.68  3.31    Male     No   Sun  Dinner     2
4         24.59  3.61  Female     No   Sun  Dinner     4
5         25.29  4.71    Male     No   Sun  Dinner     4
..          ...   ...     ...    ...   ...     ...   ...
240       27.18  2.00  Female    Yes   Sat  Dinner     2
241       22.67  2.00    Male    Yes   Sat  Dinner     2
242       17.82  1.75    Male     No   Sat  Dinner     2
243       18.78  3.00  Female     No  Thur  Dinner     2
[244 rows x 7 columns]

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
2.网络读取

import pandas as pd
data_url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv" #填写url读取
df = pd.read_csv(data_url)
#输出同上，为了节省篇幅这儿就不粘贴了
1
2
3
4
3.read_csv详解

功能： Read CSV (comma-separated) file into DataFrame

read_csv(filepath_or_buffer, sep=',', dialect=None, compression='infer', doublequote=True, escapechar=None, quotechar='"', quoting=0, skipinitialspace=False, lineterminator=None, header='infer', index_col=None, names=None, prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None, true_values=None, false_values=None, delimiter=None, converters=None, dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False, na_filter=True, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None, decimal='.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None, memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None, verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False, infer_datetime_format=False, skip_blank_lines=True)
1
参数详解：
http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html

(二)读取Mysql数据
　　假设数据库安装在本地，用户名为myusername,密码为mypassword,要读取mydb数据库中的数据

import pandas as pd
import MySQLdb
mysql_cn= MySQLdb.connect(host='localhost', port=3306,user='myusername', passwd='mypassword', db='mydb')
df = pd.read_sql('select * from test;', con=mysql_cn)    
mysql_cn.close()
1
2
3
4
5
上面的代码读取了test表中所有的数据到df中，而df的数据结构为Dataframe。
ps.MySQL教程:http://www.runoob.com/mysql/mysql-tutorial.html
(三)读取excel文件
要读取excel文件还需要安装xlrd模块，pip install xlrd即可。

df = pd.read_excel('E:\\tips.xls')
1
(四)数据导出到csv文件

df.to_csv('E:\\demo.csv', encoding='utf-8', index=False) 
#index=False表示导出时去掉行名称，如果数据中含有中文，一般encoding指定为‘utf-8’
1
2
(五)读写SQL数据库

import pandas as pd
import sqlite3
con = sqlite3.connect('...')
sql = '...'
df=pd.read_sql(sql,con)

#help文件
help(sqlite3.connect)
#输出
Help on built-in function connect in module _sqlite3:

connect(...)
    connect(database[, timeout, isolation_level, detect_types, factory])
    
    Opens a connection to the SQLite database file *database*. You can use
    ":memory:" to open a database connection to a database that resides in
    RAM instead of on disk.
#############
help(pd.read_sql)
#输出
Help on function read_sql in module pandas.io.sql:

read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None)
    Read SQL query or database table into a DataFrame.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
二.提取和筛选需要的数据
（一）提取和查看相应数据 （用的是tips.csv的数据，数据来源：https://github.com/mwaskom/seaborn-data）

print df.head() #打印数据前五行
#输出
   total_bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4
1
2
3
4
5
6
7
8
print df.tail()  #打印数据后5行
#输出
     total_bill   tip     sex smoker   day    time  size
239       29.03  5.92    Male     No   Sat  Dinner     3
240       27.18  2.00  Female    Yes   Sat  Dinner     2
241       22.67  2.00    Male    Yes   Sat  Dinner     2
242       17.82  1.75    Male     No   Sat  Dinner     2
243       18.78  3.00  Female     No  Thur  Dinner     2

1
2
3
4
5
6
7
8
9
print df.tail()  #打印数据后5行
#输出
     total_bill   tip     sex smoker   day    time  size
239       29.03  5.92    Male     No   Sat  Dinner     3
240       27.18  2.00  Female    Yes   Sat  Dinner     2
241       22.67  2.00    Male    Yes   Sat  Dinner     2
242       17.82  1.75    Male     No   Sat  Dinner     2
243       18.78  3.00  Female     No  Thur  Dinner     2
1
2
3
4
5
6
7
8
print df.columns  #打印列名
#输出
Index([u'total_bill', u'tip', u'sex', u'smoker', u'day', u'time', u'size'], dtype='object')
1
2
3
print df.index  #打印行名
#输出
Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,
            ...
            234, 235, 236, 237, 238, 239, 240, 241, 242, 243],
           dtype='int64', length=244)
1
2
3
4
5
6
print df.ix[10:20, 0:3]  #打印10~20行前三列数据
#输出
    total_bill   tip     sex
10       10.27  1.71    Male
11       35.26  5.00  Female
12       15.42  1.57    Male
13       18.43  3.00    Male
14       14.83  3.02  Female
15       21.58  3.92    Male
16       10.33  1.67  Female
17       16.29  3.71    Male
18       16.97  3.50  Female
19       20.65  3.35    Male
20       17.92  4.08    Male
1
2
3
4
5
6
7
8
9
10
11
12
13
14
#提取不连续行和列的数据，这个例子提取的是第1,3,5行，第2,4列的数据
df.iloc[[1,3,5],[2,4]]
#输出
    sex  day
1  Male  Sun
3  Male  Sun
5  Male  Sun
1
2
3
4
5
6
7
#专门提取某一个数据，这个例子提取的是第三行，第二列数据（默认从0开始算哈）
df.iat[3,2]
#输出
'Male'
1
2
3
4
print df.drop(df.columns[1, 2], axis = 1) #舍弃数据前两列
print df.drop(df.columns[[1, 2]], axis = 0) #舍弃数据前两行
#为了节省篇幅结果就不贴出来了哈~
1
2
3
print df.shape #打印维度
#输出
(244, 7)
1
2
3
df.iloc[3] #选取第3行
#输出1
total_bill     23.68
tip             3.31
sex             Male
smoker            No
day              Sun
time          Dinner
size               2
Name: 3, dtype: object

df.iloc[2:4] #选取第2到第3行
#输出2
   total_bill   tip   sex smoker  day    time  size
2       21.01  3.50  Male     No  Sun  Dinner     3
3       23.68  3.31  Male     No  Sun  Dinner     2

df.iloc[0,1] #选取第0行1列的元素
#输出3
1.01

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
(二)筛选出需要的数据（用的是tips.csv的数据，数据来源：https://github.com/mwaskom/seaborn-data）

#example:假设我们要筛选出小费大于$8的数据
df[df.tip>8]
#输出
     total_bill  tip   sex smoker  day    time  size
170       50.81   10  Male    Yes  Sat  Dinner     3
212       48.33    9  Male     No  Sat  Dinner     4
1
2
3
4
5
6
#数据筛选同样可以用”或“和”且“作为筛选条件，比如
#1
df[(df.tip>7)|(df.total_bill>50)] #筛选出小费大于$7或总账单大于$50的数据
#输出
     total_bill    tip   sex smoker  day    time  size
23        39.42   7.58  Male     No  Sat  Dinner     4
170       50.81  10.00  Male    Yes  Sat  Dinner     3
212       48.33   9.00  Male     No  Sat  Dinner     4

#2
df[(df.tip>7)&(df.total_bill>50)]#筛选出小费大于$7且总账单大于$50的数据
#输出
     total_bill  tip   sex smoker  day    time  size
170       50.81   10  Male    Yes  Sat  Dinner     3
1
2
3
4
5
6
7
8
9
10
11
12
13
14
#接上
#假如加入了筛选条件后，我们只关心day和time
df[['day','time']][(df.tip>7)|(df.total_bill>50)]
#输出
     day    time
23   Sat  Dinner
170  Sat  Dinner
212  Sat  Dinner
1
2
3
4
5
6
7
8
三.统计描述
（用的是tips.csv的数据，数据来源：https://github.com/mwaskom/seaborn-data）


print df.describe() #描述性统计
#输出  各指标都比较简单就不解释了哈
       total\_bill         tip        size
count  244.000000  244.000000  244.000000
mean    19.785943    2.998279    2.569672
std      8.902412    1.383638    0.951100
min      3.070000    1.000000    1.000000
25%     13.347500    2.000000    2.000000
50%     17.795000    2.900000    2.000000
75%     24.127500    3.562500    3.000000
max     50.810000   10.000000    6.000000
1
2
3
4
5
6
7
8
9
10
11
12
四.数据处理
(一)数据转置（用的是tips.csv的数据，数据来源：https://github.com/mwaskom/seaborn-data）


print df.T
#output
               0       1       2       3       4       5       6       7    \
total_bill   16.99   10.34   21.01   23.68   24.59   25.29    8.77   26.88   
tip           1.01    1.66     3.5    3.31    3.61    4.71       2    3.12   
sex         Female    Male    Male    Male  Female    Male    Male    Male   
smoker          No      No      No      No      No      No      No      No   
day            Sun     Sun     Sun     Sun     Sun     Sun     Sun     Sun   
time        Dinner  Dinner  Dinner  Dinner  Dinner  Dinner  Dinner  Dinner   
size             2       3       3       2       4       4       2       4   

               8       9     ...       234     235     236     237     238  \
total_bill   15.04   14.78   ...     15.53   10.07    12.6   32.83   35.83   
tip           1.96    3.23   ...         3    1.25       1    1.17    4.67   
sex           Male    Male   ...      Male    Male    Male    Male  Female   
smoker          No      No   ...       Yes      No     Yes     Yes      No   
day            Sun     Sun   ...       Sat     Sat     Sat     Sat     Sat   
time        Dinner  Dinner   ...    Dinner  Dinner  Dinner  Dinner  Dinner   
size             2       2   ...         2       2       2       2       3   

               239     240     241     242     243  
total_bill   29.03   27.18   22.67   17.82   18.78  
tip           5.92       2       2    1.75       3  
sex           Male  Female    Male    Male  Female  
smoker          No     Yes     Yes      No      No  
day            Sat     Sat     Sat     Sat    Thur  
time        Dinner  Dinner  Dinner  Dinner  Dinner  
size             3       2       2       2       2  

[7 rows x 244 columns]

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
(二)数据排序（用的是tips.csv的数据，数据来源：https://github.com/mwaskom/seaborn-data）

df.sort_values(by='tip')  #按tip列升序排序
#输出（为了不占篇幅我简化了一部分）
     total_bill    tip     sex smoker   day    time  size
67         3.07   1.00  Female    Yes   Sat  Dinner     1
236       12.60   1.00    Male    Yes   Sat  Dinner     2
92         5.75   1.00  Female    Yes   Fri  Dinner     2
111        7.25   1.00  Female     No   Sat  Dinner     1
0         16.99   1.01  Female     No   Sun  Dinner     2
..          ...    ...     ...    ...   ...     ...   ...
214       28.17   6.50  Female    Yes   Sat  Dinner     3
141       34.30   6.70    Male     No  Thur   Lunch     6
59        48.27   6.73    Male     No   Sat  Dinner     4
23        39.42   7.58    Male     No   Sat  Dinner     4
212       48.33   9.00    Male     No   Sat  Dinner     4
170       50.81  10.00    Male    Yes   Sat  Dinner     3

[244 rows x 7 columns]

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
(三)缺失值处理

1.填充缺失值(数据来自《利用python进行数据分析》第二章 usagov_bitly_data2012-03-16-1331923249.txt，需要的同学可以找我要)

import json  #python有许多内置或第三方模块可以将JSON字符串转换成python字典对象
import pandas as pd
import numpy as np
from pandas import DataFrame
path = 'F:\PycharmProjects\pydata-book-master\ch02\usagov_bitly_data2012-03-16-1331923249.txt' #根据自己的路径填写
records = [json.loads(line) for line in open(path)]
frame = DataFrame(records)
frame['tz']

#输出（为了节省篇幅我删除了部分输出结果）
0          America/New_York
1            America/Denver
2          America/New_York
3         America/Sao_Paulo
4          America/New_York
5          America/New_York
6             Europe/Warsaw
7                          
8                          
9                          
10      America/Los_Angeles
11         America/New_York
12         America/New_York
13                      NaN
               ...         
Name: tz, dtype: object

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
从以上输出值可以看出数据存在未知或缺失值，接着咱们来处理缺失值。

print frame['tz'].fillna(1111111111111)  #以数字代替缺失值
#输出结果（为了节省篇幅我删除了部分输出结果）
0          America/New_York
1            America/Denver
2          America/New_York
3         America/Sao_Paulo
4          America/New_York
5          America/New_York
6             Europe/Warsaw
7                          
8                          
9                          
10      America/Los_Angeles
11         America/New_York
12         America/New_York
13            1111111111111
Name: tz, dtype: object


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
print frame['tz'].fillna('YuJie2333333333333') #用字符串代替缺失值
#输出（为了节省篇幅我删除了部分输出结果）
0          America/New_York
1            America/Denver
2          America/New_York
3         America/Sao_Paulo
4          America/New_York
5          America/New_York
6             Europe/Warsaw
7                          
8                          
9                          
10      America/Los_Angeles
11         America/New_York
12         America/New_York
13       YuJie2333333333333
Name: tz, dtype: object


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
还有：

print frame['tz'].fillna(method='pad') #用前一个数据代替缺失值
print frame['tz'].fillna(method='bfill') #用后一个数据代替缺失值
1
2
2.删除缺失值 （数据同上）

print frame['tz'].dropna(axis=0) #删除缺失行
print frame['tz'].dropna(axis=1) #删除缺失列
1
2
3.插值法填补缺失值

由于没有数据，这儿插播一个小知识点：创建一个随机的数据框

import pandas as pd
import numpy as np
#创建一个6*4的数据框，randn函数用于创建随机数
czf_data = pd.DataFrame(np.random.randn(6,4),columns=list('ABCD')) 
czf_data
#输出
          A         B         C         D
0  0.355690  1.165004  0.810392 -0.818982
1  0.496757 -0.490954 -0.407960 -0.493502
2 -0.202123 -0.842278 -0.948464  0.223771
3  0.969445  1.357910 -0.479598 -1.199428
4  0.125290  0.943056 -0.082404 -0.363640
5 -1.762905 -1.471447  0.351570 -1.546152
1
2
3
4
5
6
7
8
9
10
11
12
13
好啦，数据就出来了。接着我们用空值替换数值，创造出一个含有空值的DataFrame。

#把第二列数据设置为缺失值
czf_data.ix[2,:]=np.nan
czf_data
#输出
          A         B         C         D
0  0.355690  1.165004  0.810392 -0.818982
1  0.496757 -0.490954 -0.407960 -0.493502
2       NaN       NaN       NaN       NaN
3  0.969445  1.357910 -0.479598 -1.199428
4  0.125290  0.943056 -0.082404 -0.363640
5 -1.762905 -1.471447  0.351570 -1.546152
1
2
3
4
5
6
7
8
9
10
11
#接着就可以利用插值法填补空缺值了~
print czf_data.interpolate()
#输出
          A         B         C         D
0  0.355690  1.165004  0.810392 -0.818982
1  0.496757 -0.490954 -0.407960 -0.493502
2  0.733101  0.433478 -0.443779 -0.846465
3  0.969445  1.357910 -0.479598 -1.199428
4  0.125290  0.943056 -0.082404 -0.363640
5 -1.762905 -1.471447  0.351570 -1.546152
1
2
3
4
5
6
7
8
9
10
(四)数据分组（用的是tips.csv的数据，数据来源：https://github.com/mwaskom/seaborn-data）

group = df.groupby('day')  #按day这一列进行分组
#1
print group.first()#打印每一组的第一行数据
#输出
      total_bill   tip     sex smoker    time  size
day                                                
Fri        28.97  3.00    Male    Yes  Dinner     2
Sat        20.65  3.35    Male     No  Dinner     3
Sun        16.99  1.01  Female     No  Dinner     2
Thur       27.20  4.00    Male     No   Lunch     4
#2
print group.last()#打印每一组的最后一行数据
#输出
      total_bill   tip     sex smoker    time  size
day                                                
Fri        10.09  2.00  Female    Yes   Lunch     2
Sat        17.82  1.75    Male     No  Dinner     2
Sun        15.69  1.50    Male    Yes  Dinner     2
Thur       18.78  3.00  Female     No  Dinner     2

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
(五)值替换

import pandas as pd
import numpy as np
#首先创造一个Series（没有数据情况下的福音233）
Series = pd.Series([0,1,2,3,4,5])
#输出
Series
0    0
1    1
2    2
3    3
4    4
5    5
dtype: int64
1
2
3
4
5
6
7
8
9
10
11
12
13
#数值替换，例如将0换成10000000000000
print Series.replace(0,10000000000000)
#输出
0    10000000000000
1                 1
2                 2
3                 3
4                 4
5                 5
dtype: int64
1
2
3
4
5
6
7
8
9
10
#列和列的替换同理
print Series.replace([0,1,2,3,4,5]，[11111,222222,3333333,44444,55555,666666])
#输出
0      11111
1     222222
2    3333333
3      44444
4      55555
5     666666
dtype: int64
1
2
3
4
5
6
7
8
9
10
五.统计分析
(一)t检验

1.独立样本t检验

两独立样本t检验就是根据样本数据对两个样本来自的两独立总体的均值是否有显著差异进行推断；进行两独立样本t检验的条件是，两样本的总体相互独立且符合正态分布。

开始找不到合适的数据，我就在网上随便摘抄了个spss做独立样本t检验的实例数据作为例子大家暂时看着吧找到合适的例子再给大家举~

数据如下，我将数据保存为本地xlsx格式：

   group  data
0      1    34
1      1    37
2      1    28
3      1    36
4      1    30
5      2    43
6      2    45
7      2    47
8      2    49
9      2    39
1
2
3
4
5
6
7
8
9
10
11
import pandas as pd
from scipy.stats import ttest_ind
IS_t_test = pd.read_excel('E:\\IS_t_test.xlsx') 
Group1 = IS_t_test[IS_t_test['group']==1]['data']
Group2 = IS_t_test[IS_t_test['group']==2]['data']
print ttest_ind(Group1,Group2)

#输出
(-4.7515451390104353, 0.0014423819408438474) 
1
2
3
4
5
6
7
8
9
输出结果的第一个元素为t值，第二个元素为p-value

ttest_ind默认两组数据方差齐性的，如果想要设置默认方差不齐，可以设置equal_var=False

print ttest_ind(Group1,Group2,equal_var=True)
print ttest_ind(Group1,Group2,equal_var=False)
#输出
(-4.7515451390104353, 0.0014423819408438474)
(-4.7515451390104353, 0.0014425608643614844)
1
2
3
4
5
2.配对样本t检验

同样找不到数据，让我们暂且假设上边独立样本是配对样本吧，使用同样的数据。

import pandas as pd
from scipy.stats import ttest_rel
IS_t_test = pd.read_excel('E:\\IS_t_test.xlsx') 
Group1 = IS_t_test[IS_t_test['group']==1]['data']
Group2 = IS_t_test[IS_t_test['group']==2]['data']
print ttest_rel(Group1,Group2)

#输出
(-5.6873679190073361, 0.00471961872448184)
1
2
3
4
5
6
7
8
9
同样的，输出结果的第一个元素为t值，第二个元素为p-value。

(二)方差分析

1.单因素方差分析

这里依然沿用t检验的数据

import pandas as pd
from scipy import stats
IS_t_test = pd.read_excel('E:\\IS_t_test.xlsx') 
Group1 = IS_t_test[IS_t_test['group']==1]['data']
Group2 = IS_t_test[IS_t_test['group']==2]['data']
w,p = stats.levene(*args)
#levene方差齐性检验。levene(*args, **kwds)  Perform Levene test for equal variances.如果p<0.05，则方差不齐
print w,p
#进行方差分析
f,p = stats.f_oneway(*args)
print f,p

#输出
(0.019607843137254936, 0.89209916055865535)
22.5771812081 0.00144238194084
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
2.多因素方差分析

数据是我从网上找的多因素方差分析的一个例子，研究区组和营养素对体重的影响。我做成了excel文件，需要的同学可以问我要哈~做多因素方差分析需要加载statsmodels模块，如果电脑没有安装可以pip install一下。

#数据导入
import pandas as pd
MANOVA=pd.read_excel('E:\\MANOVA.xlsx')
MANOVA
#输出（为了节省篇幅删掉了中间部分的输出结果）
    id  nutrient  weight
0    1         1    50.1
1    2         1    47.8
2    3         1    53.1
3    4         1    63.5
4    5         1    71.2
5    6         1    41.4
.......................
21   6         3    38.5
22   7         3    51.2
23   8         3    46.2

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
#多因素方差分析
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
formula = 'weight~C(id)+C(nutrient)+C(id):C(nutrient)'
anova_results = anova_lm(ols(formula,MANOVA).fit())
print anova_results
#output
                   df        sum_sq     mean_sq   F  PR(>F)
C(id)               7  2.373613e+03  339.087619   0     NaN
C(nutrient)         2  1.456133e+02   72.806667   0     NaN
C(id):C(nutrient)  14  3.391667e+02   24.226190   0     NaN
Residual            0  8.077936e-27         inf NaN     NaN
1
2
3
4
5
6
7
8
9
10
11
12
也许数据选得不对，p-value全是空值23333，待我找个好点儿的数据再做一次多因素方差分析。

3.重复测量设计的方差分析（单因素） ********待完善

重复测量设计是对同一因变量进行重复测度，重复测量设计的方差分析可以是同一条件下进行的重复测度，也可以是不同条件下的重复测量。

代码和多因素方差分析一样，思路不一样而已~但我还找不到多因素方差分析合适的数据所以这儿就先不写了2333

4.混合设计的方差分析 ********待完善

#########统计学学得好的同学们，教教我吧。。

(三)卡方检验

卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。（from 百度百科2333）

1.单因素卡方检验

数据源于网络，男女化妆与不化妆人数的理论值与实际值。

import numpy as np
from scipy import stats
from scipy.stats import chisquare
observed = np.array([15,95])
 #观测值：110学生中化妆的女生95人，化妆的男生15人
expected = np.array([55,55])
#理论值：110学生中化妆的女生55人，化妆的男生55人
chisquare(observed,expected)
#output
(58.18181818181818, 2.389775628860044e-14)
1
2
3
4
5
6
7
8
9
10
2.多因素卡方检验*****正在研究中，学会了完善这一块~

(四)计数统计（用的数据为tips.csv）

#example：统计性别
count = df['sex'].value_counts()
#输出
print count
Male      157
Female     87
Name: sex, dtype: int64
1
2
3
4
5
6
7
(五)回归分析 *****待学习： 数据拟合，广义线性回归。。。。等等

六.可视化

我觉得吧，其实看着excel就可以实现的功能为何那么复杂，excel确实够通用够便捷，但是处理很大数据量的话也许吃不消吧。学学python绘图也不赖，而且讲真，有的成效真的挺好看的。

(一)Seaborn

我学数据分析可视化是从学习Seaborn入门的，Seaborn是基于matplotlib的Python可视化库，刚开始便接触matplotlib难免有些吃力，参数多且难理解，但是慢慢来总会学会的。还有关键的一点是，seaborn画出来的图好好看。。

#基础导入
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
1
2
3
4
5
6
#小费数据真的挺好的，这儿用tips作为example
tips = sns.load\_dataset('tips') #从网络环境导入数据tips  

1
2
3
1.lmplot函数

lmplot(x, y, data, hue=None, col=None, row=None, palette=None, col_wrap=None, size=5, aspect=1, markers=‘o’, sharex=True, sharey=True, hue_order=None, col_order=None, row_order=None, legend=True, legend_out=True, x_estimator=None, x_bins=None, x_ci=‘ci’, scatter=True, fit_reg=True, ci=95, n_boot=1000, units=None, order=1, logistic=False, lowess=False, robust=False, logx=False, x_partial=None, y_partial=None, truncate=False, x_jitter=None, y_jitter=None, scatter_kws=None, line_kws=None)

功能：Plot data and regression model fits across a FacetGrid.

下面就不同的例子，对lmplot的参数进行解释

例子1. 画出总账单和小费回归关系图

用到了lmplot(x, y, data,scatter_kws）

x,y,data一目了然这儿就不多解释了，scatter_kws和line_kws的官方解释如下：

{scatter,line}_kws : dictionarie

Additional keyword arguments to pass to plt.scatter and plt.plot.

scatter为点，line为线。其实就是用字典去限定点和线的各种属性，如例子所示，散点的颜色为灰石色，线条的颜色为印度红，成像效果就是这样点线颜色分离，展现效果很好。大家也可以换上自己想要的图片属性。

sns.lmplot("total_bill", "tip", tips,
           scatter_kws={"marker": ".", "color": "slategray"},
           line_kws={"linewidth": 1, "color": "indianred"}).savefig('picture2') 
1
2
3


另外：颜色还可以使用RGB代码，具体对照表可以参考这个网站，可以自己搭配颜色：

http://www.114la.com/other/rgb.htm

marker也可以有多种样式，具体如下：

. Point marker
, Pixel marker
o Circle marker
v Triangle down marker
^ Triangle up marker
< Triangle left marker
> Triangle right marker
1 Tripod down marker
2 Tripod up marker
3 Tripod left marker
4 Tripod right marker
s Square marker
p Pentagon marker
* Star marker
h Hexagon marker
H Rotated hexagon D Diamond marker
d Thin diamond marker
| Vertical line (vlinesymbol) marker
_ Horizontal line (hline symbol) marker

Plus marker
x Cross (x) marker
sns.lmplot("total_bill", "tip", tips,
           scatter_kws={"marker": ".","color":"#FF7F00"},
           line_kws={"linewidth": 1, "color": "#BF3EFF"}).savefig('s1')
  

1
2
3
4
5


例子2.用餐人数(size)和小费(tip)的关系图

官方解释：

x_estimator : callable that maps vector -> scalar, optional

Apply this function to each unique value of x and plot the resulting estimate. This is useful when x is a discrete variable. If x_ci is not None, this estimate will be bootstrapped and a confidence interval will be drawn.

大概解释就是：对拥有相同x水平的y值进行映射

plt.figure()
sns.lmplot('size', 'tip', tips, x_estimator= np.mean).savefig('picture3')
1
2


{x,y}_jitter : floats, optional

Add uniform random noise of this size to either the x or y variables. The noise is added to a copy of the data after fitting the regression, and only influences the look of the scatterplot. This can be helpful when plotting variables that take discrete values.

jitter是个很有意思的参数, 特别是处理靶数据的overlapping过于严重的情况时, 通过增加一定程度的噪声(noise)实现数据的区隔化, 这样原始数据是若干 点簇 变成一系列密集邻近的点群. 另外, 有的人会经常将 rug 与 jitter 结合使用. 这依人吧.对于横轴取离散水平的时候, 用x_jitter可以让数据点发生水平的扰动.但扰动的幅度不宜过大。

sns.lmplot('size', 'tip', tips, x_jitter=.15).savefig('picture4')
1


seaborn还可以做出xkcd风格的图片，还挺有意思的

with plt.xkcd():
    sns.color_palette('husl', 8)
    sns.set_context('paper')
    sns.lmplot(x='total_bill', y='tip', data=tips, ci=65).savefig('picture1')
1
2
3
4


with plt.xkcd():
    sns.lmplot('total_bill', 'tip', data=tips, hue='day')
    plt.xlabel('hue = day')
    plt.savefig('picture5')
1
2
3
4


with plt.xkcd():
    sns.lmplot('total_bill', 'tip', data=tips, hue='smoker')
    plt.xlabel('hue = smoker')
    plt.savefig('picture6')
1
2
3
4


sns.set_style('dark')
sns.set_context('talk')
sns.lmplot('size', 'total_bill', tips, order=2)
plt.title('# poly order = 2')
plt.savefig('picture7')
plt.figure()
sns.lmplot('size', 'total_bill', tips, order=3)
plt.title('# poly order = 3')
plt.savefig('picture8')

1
2
3
4
5
6
7
8
9
10
sns.jointplot("total_bill", "tip", tips).savefig('picture9')
1


(二)matplotlib ********待完善

七.其它~

(一)调用R

让Python直接调用R的函数，下载安装rpy2模块即可~

(二)ipython ********待完善

一、数据分析有关的python库简介

二、数据的导入和导出

三、数据筛选

四、数据描述

五、数据处理

六、统计分析

七、可视化

八、其它
一、前期准备
在开始进行数据分析之前，我们需要做一些准备工作。这包括了解数据分析的基本概念和流程，以及掌握一些必要的Python库。

1. 数据分析的基本概念和流程
数据分析通常包括数据收集、数据清洗、数据分析和数据可视化等几个步骤。

数据收集可以通过多种方式进行，例如从数据库中提取、从文件中读取或者通过网络爬虫获取。

数据清洗是非常关键的一步，它涉及到处理缺失值、异常值和重复数据等问题。

数据分析可以采用多种方法，例如描述性统计、相关性分析、回归分析等。

数据可视化则是将分析结果以图表的形式展示出来，以便更直观地理解数据。

2. Python数据分析库
Pandas：这是一个用于数据处理和分析的强大库。它提供了高效的数据结构，如Series和DataFrame，使得数据操作变得非常方便。

Matplotlib：用于数据可视化的库，可以绘制各种类型的图表，如折线图、柱状图、散点图等。

二、代码实践
在完成前期准备之后，我们就可以开始编写代码进行数据分析了。以下是一些常见的数据分析任务和对应的Python代码。

1. 数据读取和处理
我们可以使用Pandas的`read_csv()`函数来读取CSV格式的数据文件。例如： ``python import pandas as pd data = pd.read_csv('your_file.csv') ``

在读取数据之后，可能需要对数据进行清洗。例如，处理缺失值可以使用`dropna()`函数或者`fillna()`函数。 ``python data = data.dropna() # 删除包含缺失值的行 data = data.fillna(0) # 将缺失值填充为0 ``

2.数据分析
进行描述性统计分析可以使用`describe()`函数： ``python desc = data.describe() ``

相关性分析可以使用`corr()`函数： ``python correlation = data.corr() `` 3. 数据可视化

使用Matplotlib绘制柱状图： ``python import matplotlib.pyplot as plt plt.bar(data['category'], data['value']) plt.xlabel('Category') plt.ylabel('Value') plt.title('Bar Chart') plt.show() ``

三、总结
通过以上的准备工作和代码实践，我们可以看到Python在数据分析领域的强大功能。当然，这只是一个开始，数据分析还有很多深入的内容值得我们去探索，比如更高级的数据分析算法、机器学习在数据分析中的应用等。 希望这篇文章能够对正在学习Python数据分析的朋友们有所帮助。让我们一起在数据分析的道路上不断探索和进步什么是数据可视化？ 数据可视化是为了使得数据更高效地反应数据情况，便于让读者更高效阅读，通过数据可视化突出数据背后的规律，以此突出数据中的重要因素，如果使用Python做数据可视化，建议学好如下这四个Python数据分析包，分别是：

Pandas、Matplotlib、Seaborn、Pyecharts

学好以上四个数据分析包，做可视化足够用了，全文较长，建议耐心看完，学习后即可使用Python做数据可视化，具体的代码实操部分可以实际用代码进行演示，这样才能更好的掌握，下面一起来学习~



01. Pandas
官网https://www.pypandas.cn/

Pandas 是 Python的核心数据分析支持库，提供了快速、灵活、明确的数据结构，旨在简单、直观地处理关系型、标记型数据，广泛应用于数据分析领域，Pandas 适用于处理与 Excel 表类似的表格数据，以及有序和无序的时间序列数据等。

Pandas 的主要数据结构是 Series（一维数据）和 DataFrame（二维数据），这两种数据结构足以处理金融、统计、社会科学、工程等领域里的大多数典型用例，使用pandas进行数据分析流程包含数据整理与清洗、数据分析与建模、数据可视化与制表等阶段。

灵活的分组功能：group by数据分组；

直观地合并功能：merge数据连接；

灵活地重塑功能：reshape数据重塑；



pandas库不仅可以做一些数据清洗的工作，还可以使用pandas作图，并且做图时，使用一行代码就可以轻松作图，详细的作图方法可以看代码中的注释。

#导入pandas库  
import pandas as pd    
#生成一个Series  
s=pd.Series([1,3,3,4], index=list('ABCD'))    

#括号内不指定图表类型，则默认生成直线图  
s.plot()


#条形图
s.plot(kind='bar')


#水平条形图   
s.plot.barh()


#饼图   
s.plot.pie()


#直方图   
s.plot.hist()


#密度图   
import numpy as np 

s=pd.Series(np.random.randn(1000))  #生成一列随机数   
s.plot.kde()   
s.plot.density()

#散点图   
import numpy as np 
#生成一个DataFrame  
df=pd.DataFrame(np.random.randn(1000,2),
                 columns=['X1','Y'])
df.plot.scatter(x='X1',y='Y')


#六角箱图   
df.plot.hexbin(x='X1',y='Y',gridsize=8)


#箱型图
df=pd.DataFrame(np.random.rand(10,2),columns=['A','B'])
df.plot.box()

#面积图
df=pd.DataFrame(np.random.randint(10,size=(4,4)),
                 columns=list('ABCD'),
                 index=list('WXYZ'))    

df.plot.area()

02. Matplotlib
官网https://www.matplotlib.org.cn/

Matplotlib是一个Python 2D绘图库，它以多种硬拷贝格式和跨平台的交互式环境生成出版物质量的图形。Matplotlib可用于Python脚本，Python和IPython Shell、Jupyter笔记本，Web应用程序服务器和四个图形用户界面工具包。

Matplotlib 尝试使容易的事情变得更容易，使困难的事情变得可能，只需几行代码就可以生成图表、直方图、功率谱、条形图、误差图、散点图等。

为了简单绘图，该 pyplot 模块提供了类似于MATLAB的界面，尤其是与IPython结合使用时，对于高级用户，您可以通过面向对象的界面或MATLAB用户熟悉的一组功能来完全控制线型，字体属性，轴属性等。



下面介绍matplotlib的用法，使用matplotlib除了可以作图外，还可以对于图表的参数做一些调整，使得图表更加美观，关于使用matplotlib的建议，可以做一些常用的图表模板，更换代码的数据源就可以生成图表，而不用一点一点的去调整参数。

#导入模块  
import matplotlib.pyplot as plt  

#设置风格  
plt.style.use('seaborn-white')  

#中文显示问题，如果没有这段代码，图表不显示中文汉字   
plt.rcParams['font.sans-serif'] =['SimHei']

这里首先导入matplotlib库，并使用了seaborn-white的图表风格，可以使用plt.style.available 查看图表的风格，选择一个自己喜欢的图表风格，在图表中不能显示汉字，使用一段代码就可以显示了。


#构建一个DataFrame 
import pandas as pd  
import matplotlib.pyplot as plt
  
df=pd.DataFrame({'X':[1,3,5,7]})  
df['Y']=df['X']**3  
df


#设置图像的大小 
plt.figure(facecolor='white',figsize=(9,6),dpi=100)  
plt.plot(df['X'],df['Y'])
   
#设置图像的标题 
plt.title('折线图',fontsize=15,color='b') 
 
#设置图像的X、Y轴标题大小，颜色，与坐标轴的距离  
plt.xlabel('X轴',fontsize=10,color='r',labelpad=15)  
plt.ylabel('Y轴',fontsize=10,color='g',rotation=0,labelpad=15)   

#设置起始坐标点 
plt.xlim([1,8])  
plt.ylim([1,350]) 
#plt.xticks([1,2,3,4])只显示1,2,3,4  
#plt.yticks([50,150,250,300])只显示50,150,250,300 
  
#图像的网格线进行设置 
plt.grid(color='r', linestyle='-.')


这里首先设置图像的大小，跟我们画画一样，选择多大的纸张去作图，一样的道理，然后设置坐标轴，起始坐标，网格线等。



有时候，要在一张图表上绘制多条线。

#多个图的绘图方法  
import numpy as np  
import matplotlib.pyplot as plt  

x=np.array([1,3,5])  
y1=x  
y2=x * 10  
y3=x * 20  
y4=x * 30
可以在一个plt.plot命令后继续加另一个plt.plot命令，可以在一张图上做另一条线。

plt.figure(facecolor='white')  
plt.plot(x,y1,label='A')  
plt.plot(x,y2,label='B')  
plt.plot(x,y3,label='C')  
plt.plot(x,y4,label='D')   

plt.legend()#显示图例


使用plt.subplots命令也可以作出同样的图。

#使用面向对象绘图  
fig,ax=plt.subplots(facecolor='white')  
plt.plot(x,y1,label='A')  
plt.plot(x,y2,label='B')  
plt.plot(x,y3,label='C')  
plt.plot(x,y4,label='D')   

plt.legend()#显示图例


多表绘制

下面介绍在一张图表的不同位置绘制不同的线型，使用plt.subplot命令首先确定绘图的位置，比如plt.subplot(223)表示在2*2分布的图表中第三个位置，其余的绘图命令相似。

plt.figure(facecolor='white',figsize=(9,6)) 

plt.subplot(221)  
plt.plot(x,y1,label='A',color='r')  
plt.xticks(fontsize=15)  
plt.legend()#显示图例   

plt.subplot(222)  
plt.plot(x,y2,label='B',color='y')  
plt.xticks(fontsize=15)  
plt.legend()#显示图例   

plt.subplot(223)  
plt.plot(x,y3,label='C',color='b')  
plt.xticks(fontsize=15)  
plt.legend()#显示图例 
  
plt.subplot(224)  
plt.plot(x,y4,label='D',color='g')  
plt.xticks(fontsize=15)  
plt.legend()#显示图例   

plt.tight_layout()#密集显示


除了使用plt.subplot命令确定绘图区域外，还可以用axs\[ \]命令绘图，这种绘图方式是面向对象的绘图方式。


#面向对象绘制多图  
fig,axs=plt.subplots(2,2,facecolor='white',figsize=(9,6))  

axs[0,0].plot(x,y1,label='A',color='r')  
axs[0,1].plot(x,y2,label='B',color='y')  
axs[1,0].plot(x,y3,label='C',color='b')  
axs[1,1].plot(x,y4,label='D',color='g')


有时候绘制多张表时需共享一个坐标轴，可以使用sharex='all’命令。

#sharex='all'共享X轴  
fig,axs=plt.subplots(4,1,facecolor='white', figsize=(9,6), sharex='all')   
axs[0].plot(x,y1,label='A',color='r')  
axs[1].plot(x,y2,label='B',color='y')  
axs[2].plot(x,y3,label='C',color='b')  
axs[3].plot(x,y4,label='D',color='g')


设置全局变量

使用plt.rcParams命令对全局变量设置，包括字符显示、中文显示、背景颜色、标题大小、坐标轴字体大小，线型等。

#导入模块  
import matplotlib.pyplot as plt  

#设置风格  
plt.style.use('seaborn-white')  

#设置全局变量  
plt.rcParams['axes.unicode_minus'] = False #字符显示 
plt.rcParams['font.sans-serif'] =['SimHei'] #中文显示  
plt.rcParams['figure.facecolor'] = 'b' #设置图表背景颜色 
plt.rcParams['axes.facecolor'] = (0.8,0.9,0.8) #设置RGB颜色  
plt.rcParams['axes.titlesize'] = 20 #设置标题大小  
plt.rcParams['axes.labelsize'] = 20 #设置轴大小  
plt.rcParams['xtick.labelsize'] = 20 #设置X坐标大小  
plt.rcParams['ytick.labelsize'] = 20 #设置Y坐标大小  
plt.rcParams['lines.linestyle'] = '-.' #设置线型  

plt.plot(x,y1,label='A')  
plt.plot(x,y2,label='B')  
plt.plot(x,y3,label='C')  
plt.plot(x,y4,label='D')   
plt.title('折线图')  
plt.xlabel('X轴')  
plt.ylabel('Y轴')  
plt.legend()#显示图例


下图就是通过设置全局变量做的图，个人觉得并不美观，对于其他图表全局变量的设置，大家可以探索，做出更好看的图表。



03. Seaborn
官网http://seaborn.pydata.org/

Seaborn 是一个基于matplotlib的 Python 数据可视化库，它建立在matplotlib之上，并与Pandas数据结构紧密集成，用于绘制有吸引力和信息丰富的统计图形的高级界面。

Seaborn 可用于探索数据，它的绘图功能对包含整个数据集的数据框和数组进行操作，并在内部执行必要的语义映射和统计聚合以生成信息图，其面向数据集的声明式 API可以专注于绘图的不同元素的含义，而不是如何绘制它们的细节。

Matplotlib 拥有全面而强大的 API，几乎可以根据自己的喜好更改图形的任何属性，seaborn 的高级界面和 matplotlib 的深度可定制性相结合，使得Seaborn既可以快速探索数据，又可以创建可定制为出版质量最终产品的图形。



绘制多行图

将变量按照多行的形式进行绘制，使用sns.FacetGrid命令。

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})

rs = np.random.RandomState(1979)
x = rs.randn(500)
g = np.tile(list("ABCDEFGHIJ"), 50)
df = pd.DataFrame(dict(x=x, g=g))
m = df.g.map(ord)
df["x"] += m

pal = sns.cubehelix_palette(10, rot=-.25, light=.7)
g = sns.FacetGrid(df, row="g", hue="g", aspect=15, height=.5, palette=pal)

g.map(sns.kdeplot, "x",
      bw_adjust=.5, clip_on=False,
      fill=True, alpha=1, linewidth=1.5)
g.map(sns.kdeplot, "x", clip_on=False, color="w", lw=2, bw_adjust=.5)

g.refline(y=0, linewidth=2, linestyle="-", color=None, clip_on=False)

def label(x, color, label):
    ax = plt.gca()
    ax.text(0, .2, label, fontweight="bold", color=color,
            ha="left", va="center", transform=ax.transAxes)

g.map(label, "x")

g.figure.subplots_adjust(hspace=-.25)

g.set_titles("")
g.set(yticks=[], ylabel="")
g.despine(bottom=True, left=True)


绘制热力图

将数据的大小用热力图进行呈现，使用sns.heatmap命令。

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

# Load the example flights dataset and convert to long-form
flights_long = sns.load_dataset("flights")
flights = flights_long.pivot("month", "year", "passengers")

# Draw a heatmap with the numeric values in each cell
f, ax = plt.subplots(figsize=(9, 6))
sns.heatmap(flights, annot=True, fmt="d", linewidths=.5, ax=ax)

04. Pyecharts
官网https://pyecharts.org/#/

Echarts 是一个由百度开源的数据可视化，凭借着良好的交互性，精巧的图表设计，得到了众多开发者的认可。而 Python 是一门富有表达力的语言，很适合用于数据处理。当数据分析遇上数据可视化时，pyecharts 诞生了。

Pyecharts具有简洁的 API 设计，使用如丝滑般流畅，支持链式调用，囊括了 30+ 种常见图表，应有尽有，支持主流 Notebook 环境，Jupyter Notebook 和 JupyterLab，拥有高度灵活的配置项，可轻松搭配出精美的图表。

Pyecharts强大的数据交互功能，使数据表达信息更加生动，增加了人机互动效果，并且数据呈现效果可直接导出为html文件，增加数据结果交互的机会，使得信息沟通更加容易。



绘制地图

Pyecharts有着丰富的图表素材，支持链式调用，如下是使用Pyecharts的地理图表功能，空间上直观显示数据可视化效果。

from pyecharts import options as opts
from pyecharts.charts import Map
from pyecharts.faker import Faker

c = (
    Map()
    .add("商家A", [list(z) for z in zip(Faker.provinces, Faker.values())], "china")
    .set_global_opts(
        title_opts=opts.TitleOpts(title="Map-VisualMap（分段型）"),
        visualmap_opts=opts.VisualMapOpts(max_=200, is_piecewise=True),
    )
    .render("map_visualmap_piecewise.html")
)

绘制雷达图

使用Radar命令绘制出雷达图，用来显示多变量数据的图形方法。

from pyecharts import options as opts
from pyecharts.charts import Radar

v1 = [[4300, 10000, 28000, 35000, 50000, 19000]]
v2 = [[5000, 14000, 28000, 31000, 42000, 21000]]
c = (
    Radar()
    .add_schema(
        schema=[
            opts.RadarIndicatorItem(name="销售", max_=6500),
            opts.RadarIndicatorItem(name="管理", max_=16000),
            opts.RadarIndicatorItem(name="信息技术", max_=30000),
            opts.RadarIndicatorItem(name="客服", max_=38000),
            opts.RadarIndicatorItem(name="研发", max_=52000),
            opts.RadarIndicatorItem(name="市场", max_=25000),
        ]
    )
    .add("预算分配", v1)
    .add("实际开销", v2)
    .set_series_opts(label_opts=opts.LabelOpts(is_show=False))
    .set_global_opts(
        legend_opts=opts.LegendOpts(selected_mode="single"),
        title_opts=opts.TitleOpts(title="Radar-单例模式"),
    )
    .render("radar_selected_mode.html")
)


以上介绍pandas如何绘制图表，同时引申matplotlib库的使用，并且介绍Seaborn和Pyecharts这两个数据可视化库，加以了解Python数据可视化内容，同时在数据可视化中学习多表绘制和设置全局变量，相信通过以上的学习，一定能对你学习Python数据可视化有所启发。
一、设计要求
项目背景
本项目旨在通过数据分析和可视化的方法，对海底捞门店的营业数据进行深入的探索和理解。数据来源于Excel文件《海底捞门店数据.xlsx》。项目包括数据预处理、缺失值处理、异常值处理、重复值处理、数据转换、分组统计分析和数据可视化。
主要功能
1. 数据读取与预览
• 从Excel文件中读取数据，展示数据的前几行，提供数据的基本信息，包括行列数、数据类型和非空数统计。
2. 缺失值处理
• 统计数据中的缺失值总数。
• 提供两种处理缺失值的方法：删除含有缺失值的记录和用众数填充缺失值。
3.异常值处理
• 使用箱型图可视化数据，识别异常值。
• 提供两种去除异常值的方法：四分位数间距法（IQR）和3σ原则。
4.重复值处理
• 检查并删除数据中的重复值。
5.数据转换
• 将“省份”列转换为数值型数据，便于后续分析。
6.分组统计分析
• 按省份分组统计各省店铺数量。
• 按营业时长分组统计各时长区间内的店铺数量。
7.数据可视化
• 可视化各省店铺数量分布。
• 可视化营业时长分布。
• 可视化开始营业时间分布。
• 可视化结束营业时间分布。
二、设计思路
1. 导入库和设置
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.sans-serif']=['SimHei'] # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False # 用来正常显示负号
• 导入必要的库：pandas用于数据处理，matplotlib和seaborn用于数据可视化。
• 设置绘图时中文字体的显示，确保中文标签能正常显示。

2. 读取数据
file_path = '海底捞门店数据.xlsx'
df = pd.read_excel(file_path, engine='openpyxl')
• 从Excel文件中读取数据到一个DataFrame中。
3. 数据预览和基本信息
print("数据预览：")
print("缺失值总数:")
print(df.isnull().sum())
• 打印数据的前几行，显示数据的基本信息（行列数、数据类型和非空数）。
• 统计缺失值的总数。
4. 处理缺失值
# 删除含有缺失值的记录
# 代码略....
print(df_dropna.isnull().sum())
# 用众数填充缺失值
df_fillna = df.fillna(df.mode().iloc[0])
print(df_fillna.isnull().sum())
• 处理缺失值的方法包括：
• 删除含有缺失值的记录。
• 用众数填充缺失值。

5. 处理异常值
# 箱型图识别异常值
plt.figure(figsize=(10, 6))
# 代码略....
plt.show()
# 四分位数间距法去除异常值
IQR = Q3 - Q1
df_no_outliers = df[~((df['营业时长'] < (Q1 - 1.5 * IQR)) | (df['营业时长'] > (Q3 + 1.5 * IQR)))]
print("去除异常值后的数据行列数: ", df_no_outliers.shape)
# 3σ原则去除异常值
mean = df['营业时长'].mean()
print("3σ原则去除异常值后的数据行列数: ", df_no_outliers_sigma.shape)
• 使用箱型图可视化数据，识别异常值。
• 使用四分位数间距法（IQR）和3σ原则去除异常值。

6. 处理重复值
df_no_duplicates = df.drop_duplicates()
print("删除重复值后的数据行列数: ", df_no_duplicates.shape)
• 删除重复值。
7. 数据转换
print("转换后的数据预览：")
print(df.head())
• 将“省份”列转换为数值型数据，便于后续分析。

8. 数据分组和统计分析
# 按省份分组统计各省店铺数量
print("按省份分组统计：")
print(province_group)
# 按营业时间长度分组统计
time_group = df.groupby('营业时长')['店名'].count().reset_index()
print("按营业时间长度分组统计：")
print(time_group)

• 按省份和营业时长分组，统计各组的店铺数量。
9. 数据可视化
# 店铺数量按省份分布
plt.figure(figsize=(14, 7))
# 代码略....
# 代码略....
plt.show()

# 营业时长分布
plt.figure(figsize=(10, 6))
# 代码略....
# 代码略....
plt.show()
# 开始营业时间分布
plt.figure(figsize=(10, 6))
# 代码略....
# 代码略....
plt.show()

# 结束营业时间分布
# 代码略....
# 代码略....
plt.show()
• 可视化数据，展示各省店铺数量分布、营业时长分布、开始营业时间分布和结束营业时间分布。
总结
这段代码通过读取、预览、处理和分析数据，最后进行可视化展示。其设计思路清晰、结构完整，覆盖了数据处理和分析的多个方面，包括缺失值处理、异常值处理、重复值处理、数据转换、数据分组统计和数据可视化。使用 Python 实现数据可视化（完整代码）
当我们能够充分理解数据，并能够轻松向他人解释数据时，数据才有价值；我们的读者可以通过可视化互动或其他数据使用方式来探寻一个故事的背后发生了什么，因此，数据可视化至关重要。

数据可视化的目的其实就是直观地展现数据，例如让花费数小时甚至更久才能归纳的数据量，转化成一眼就能读懂的指标；通过加减乘除、各类公式权衡计算得到的两组数据差异，在图中通过颜色差异、长短大小即能形成对比。

在本文中，我们将着眼于 5 个数据可视化方法，并使用 Python Matplotlib 为他们编写一些快速简单的函数。

首先分享一个很棒的图表，可帮助你在工作中选择正确的可视化方法。

图片

散点图
散点图非常适合展示两个变量之间的关系，你可以直接看到数据的原始分布，还可以通过对组进行简单地颜色编码来查看不同组数据的关系。

图片

想要可视化三个变量之间的关系？ 没问题！ 仅需使用另一个参数（如点大小）就可以对第三个变量进行编码。

图片

现在开始讨论代码。首先用别名 “plt” 导入 Matplotlib 的 pyplot 。要创建一个新的点阵图，我们可调用 plt.subplots() 。我们将 x 轴和 y 轴数据传递给该函数，然后将这些数据传递给 ax.scatter() 以绘制散点图。

我们还可以设置点的大小、点颜色和 alpha 透明度。你甚至可以设置 Y 轴为对数刻度。标题和坐标轴上的标签可以专门为该图设置。

import matplotlib.pyplot as pltimport numpy as npdef scatterplot(x_data, y_data, x_label="", y_label="", title="", color = "r", yscale_log=False):
   # Create the plot object
   _, ax = plt.subplots()    # Plot the data, set the size (s), color and transparency (alpha)
   # of the points
   ax.scatter(x_data, y_data, s = 10, color = color, alpha = 0.75)    if yscale_log == True:
       ax.set_yscale('log')    # Label the axes and provide a title
   ax.set_title(title)
   ax.set_xlabel(x_label)
   ax.set_ylabel(y_label)
折线图

当你可以看到一个变量随着另一个变量明显变化的时候，比如说它们有一个大的协方差，那最好使用折线图。如下图，我们可以清晰地看到对于所有的主线随着时间都有大量的变化。另外，我们也可以通过彩色编码进行分组。

图片

这里是折线图的代码。它和上面的散点图很相似，只是在一些变量上有小的变化。

def lineplot(x_data, y_data, x_label="", y_label="", title=""):
   # Create the plot object
   _, ax = plt.subplots()    # Plot the best fit line, set the linewidth (lw), color and
   # transparency (alpha) of the line
   ax.plot(x_data, y_data, lw = 2, color = '#539caf', alpha = 1)    # Label the axes and provide a title
   ax.set_title(title)
   ax.set_xlabel(x_label)
   ax.set_ylabel(y_label)
直方图

直方图对于查看（或真正地探索）数据点的分布是很有用的。查看下面我们以频率和 IQ 做的直方图。我们可以清楚地看到朝中间聚集，并且能看到中位数是多少。

我们也可以看到它呈正态分布。使用直方图真得能清晰地呈现出各个组的频率之间的相对差别。

组的使用（离散化）真正地帮助我们看到了“更加宏观的图形”,然而当我们使用所有没有离散组的数据点时，将对可视化可能造成许多干扰，使得看清真正发生了什么变得困难。

图片

下面是在 Matplotlib 中的直方图代码。有两个参数需要注意一下：首先，参数 n_bins 控制我们想要在直方图中有多少个离散的组。更多的组将给我们提供更加完善的信息，但是也许也会引进干扰，使得我们远离全局；另一方面，较少的组给我们一种更多的是“鸟瞰图”和没有更多细节的全局图。

其次，参数 cumulative 是一个布尔值，允许我们选择直方图是否为累加的，基本上就是选择是 PDF（Probability Density Function，概率密度函数）还是 CDF（Cumulative Density Function，累积密度函数）。

def histogram(data, n_bins, cumulative=False, x_label = "", y_label = "", title = ""):
   _, ax = plt.subplots()
   ax.hist(data, n_bins = n_bins, cumulative = cumulative, color = '#539caf')
   ax.set_ylabel(y_label)
   ax.set_xlabel(x_label)
   ax.set_title(title)
想象一下我们想要比较数据中两个变量的分布。有人可能会想你必须制作两张直方图，并且把它们并排放在一起进行比较。然而，实际上有一种更好的办法：我们可以使用不同的透明度对直方图进行叠加覆盖。

看下图，均匀分布的透明度设置为 0.5 ，使得我们可以看到他背后的图形。这样我们就可以直接在同一张图表里看到两个分布。

图片

对于重叠的直方图，需要设置一些东西。首先，我们设置可同时容纳不同分布的横轴范围。根据这个范围和期望的组数，我们可以真正地计算出每个组的宽度。最后，我们在同一张图上绘制两个直方图，其中有一个稍微更透明一些。

# Overlay 2 histograms to compare themdef overlaid_histogram(data1, data2, n_bins = 0, data1_name="", data1_color="#539caf", data2_name="", data2_color="#7663b0", x_label="", y_label="", title=""):
   # Set the bounds for the bins so that the two distributions are fairly compared
   max_nbins = 10
   data_range = [min(min(data1), min(data2)), max(max(data1), max(data2))]
   binwidth = (data_range[1] - data_range[0]) / max_nbins    if n_bins == 0
   bins = np.arange(data_range[0], data_range[1] + binwidth, binwidth)    else: 
   bins = n_bins    # Create the plot
   _, ax = plt.subplots()
   ax.hist(data1, bins = bins, color = data1_color, alpha = 1, label = data1_name)
   ax.hist(data2, bins = bins, color = data2_color, alpha = 0.75, label = data2_name)
   ax.set_ylabel(y_label)
   ax.set_xlabel(x_label)
   ax.set_title(title)
   ax.legend(loc = 'best')
柱状图
当你试图将类别很少（可能小于10）的分类数据可视化的时候，柱状图是最有效的。因为你可以很容易地看到基于柱的类别之间的区别（比如大小)；分类也很容易划分和用颜色进行编码。

柱状图分为三种：常规的，分组的，堆叠的。

常规的柱状图，在 barplot() 函数中，xdata 表示 x 轴上的标记，ydata 表示 y 轴上的杆高度。误差条是一条以每条柱为中心的额外的线，可以画出标准偏差。

图片

分组的柱状图让我们可以比较多个分类变量。我们比较的第一个变量是不同组的分数是如何变化的（组是G1，G2，……等等)。我们也在比较性别本身和颜色代码。

图片

堆叠柱状图可以很好地观察不同变量的分类。下图比较了每天的服务器负载。通过颜色编码后的堆栈图，我们可以很容易地看到和理解哪些服务器每天工作最多，以及与其他服务器进行比较负载情况如何。此代码的代码与分组的条形图相同。我们循环遍历每一组，但这次我们把新柱放在旧柱上，而不是放在它们的旁边。

图片

def barplot(x_data, y_data, error_data, x_label="", y_label="", title=""):
   _, ax = plt.subplots()
   # Draw bars, position them in the center of the tick mark on the x-axis
   ax.bar(x_data, y_data, color = '#539caf', align = 'center')
   # Draw error bars to show standard deviation, set ls to 'none'
   # to remove line between points
   ax.errorbar(x_data, y_data, yerr = error_data, color = '#297083', ls = 'none', lw = 2, capthick = 2)
   ax.set_ylabel(y_label)
   ax.set_xlabel(x_label)
   ax.set_title(title)

def stackedbarplot(x_data, y_data_list, colors, y_data_names="", x_label="", y_label="", title=""):
   _, ax = plt.subplots()
   # Draw bars, one category at a time
   for i in range(0, len(y_data_list)):
       if i == 0:
           ax.bar(x_data, y_data_list[i], color = colors[i], align = 'center', label = y_data_names[i])
       else:
           # For each category after the first, the bottom of the
           # bar will be the top of the last category
           ax.bar(x_data, y_data_list[i], color = colors[i], bottom = y_data_list[i - 1], align = 'center', label = y_data_names[i])
   ax.set_ylabel(y_label)
   ax.set_xlabel(x_label)
   ax.set_title(title)
   ax.legend(loc = 'upper right')

def groupedbarplot(x_data, y_data_list, colors, y_data_names="", x_label="", y_label="", title=""):
   _, ax = plt.subplots()
   # Total width for all bars at one x location
   total_width = 0.8
   # Width of each individual bar
   ind_width = total_width / len(y_data_list)
   # This centers each cluster of bars about the x tick mark
   alteration = np.arange(-(total_width/2), total_width/2, ind_width)

   # Draw bars, one category at a time
   for i in range(0, len(y_data_list)):
       # Move the bar to the right on the x-axis so it doesn't
       # overlap with previously drawn ones
       ax.bar(x_data + alteration[i], y_data_list[i], color = colors[i], label = y_data_names[i], width = ind_width)
   ax.set_ylabel(y_label)
   ax.set_xlabel(x_label)
   ax.set_title(title)
   ax.legend(loc = 'upper right')
箱形图
直方图很好地可视化了变量的分布。但是如果我们需要更多的信息呢？也许我们想要更清晰的看到标准偏差？也许中值与均值有很大不同，我们有很多离群值？如果有这样的偏移和许多值都集中在一边呢？

这就是箱形图所适合干的事情了。箱形图给我们提供了上面所有的信息。实线框的底部和顶部总是第一个和第三个四分位（比如 25% 和 75% 的数据），箱体中的横线总是第二个四分位（中位数）。像胡须一样的线(虚线和结尾的条线）从这个箱体伸出，显示数据的范围。

由于每个组/变量的框图都是分别绘制的，所以很容易设置。xdata 是一个组/变量的列表。Matplotlib 库的 boxplot() 函数为 ydata 中的每一列或每一个向量绘制一个箱体。因此，xdata 中的每个值对应于 ydata 中的一个列/向量。我们所要设置的就是箱体的美观。

图片

图片

以上就是使用 Matplotlib 来实现数据可视化方法。数据分析和可视化的工具。Python作为一种功能强大且易于学习的编程语言，已经成为了进行这些任务的首选语言之一。在这篇文章中，我们将探讨如何使用Python进行数据分析和可视化。

首先，我们需要了解Python中的基本数据结构。Python提供了多种内置的数据类型，如列表、元组、字典和集合。这些数据类型可以帮助我们有效地存储和组织数据。例如，我们可以使用列表来存储一组数值，使用字典来存储键值对等。

接下来，我们将介绍一些常用的数据处理和分析方法。其中，Pandas是一个广泛使用的Python库，它提供了强大的数据处理功能。通过Pandas，我们可以方便地读取、清洗、转换和分析数据。此外，我们还可以使用NumPy进行数值计算，使用Matplotlib和Seaborn进行数据可视化等。

在进行数据分析时，我们通常需要对数据进行预处理。这包括处理缺失值、异常值和重复值等。Pandas提供了丰富的函数和方法来处理这些问题。例如，我们可以使用dropna()函数删除包含缺失值的行或列，使用fillna()函数填充缺失值等。

一旦我们完成了数据预处理，就可以进行数据分析了。数据分析的目标是从数据中发现有用的信息和模式。我们可以使用Pandas的各种函数和方法来进行数据分析。例如，我们可以使用groupby()函数对数据进行分组操作，使用pivot_table()函数创建数据透视表等。

最后，我们将介绍如何使用Matplotlib和Seaborn进行数据可视化。数据可视化是一种将数据以图形的方式展示出来的技术，它可以帮助我们更好地理解和解释数据。Matplotlib是一个功能强大的绘图库，它提供了丰富的绘图函数和方法。而Seaborn则是建立在Matplotlib之上的一个高级绘图库，它提供了更美观和易用的绘图功能。

下面是一个简单的代码示例，展示了如何使用Python进行数据分析和可视化：

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 读取数据
data = pd.read_csv('data.csv')

# 数据预处理
data = data.dropna()  # 删除缺失值

# 数据分析
grouped_data = data.groupby('category').mean()  # 按类别计算平均值

# 数据可视化
plt.figure(figsize=(10, 6))
sns.barplot(x='category', y='value', data=grouped_data)
plt.title('Average Value by Category')
plt.show()
在这个示例中，我们首先导入了所需的库，然后读取了一个CSV文件并将其存储在一个Pandas DataFrame中。接着，我们进行了数据预处理，删除了包含缺失值的行。然后，我们按类别对数据进行了分组，并计算了每个类别的平均值。最后，我们使用Matplotlib和Seaborn创建了一个条形图，展示了每个类别的平均使用Python进行数据处理与可视化——以气温数据分析为例
在这个数据驱动的时代，数据处理和可视化已成为数据分析师、科学家和工程师的重要技能。本文将介绍如何使用Python进行气温数据的处理与可视化，从数据读取、清洗、分析到最终的可视化展示，全程包含代码演示。

1. 环境准备
首先，确保你已经安装了必要的Python库。你可以使用pip来安装这些库：

pip install pandas matplotlib seaborn
我们将使用pandas进行数据处理，matplotlib和seaborn进行可视化。

2. 数据读取
假设我们有一个CSV文件temperature_data.csv，其中包含日期和气温数据。数据格式如下：

date,temperature
2023-01-01,5.6
2023-01-02,6.2
...
使用pandas读取数据：

import pandas as pd

# 读取CSV文件
data = pd.read_csv('temperature_data.csv', parse_dates=['date'])
data.set_index('date', inplace=True)

print(data.head())
这段代码将CSV文件读取为DataFrame，并将date列解析为日期格式，同时将其设置为索引。

3. 数据清洗
数据清洗是数据处理的重要步骤，通常包括处理缺失值、异常值等。假设我们的数据中有一些缺失值，我们可以使用以下代码进行处理：

# 检查缺失值
print(data.isnull().sum())

# 填充缺失值（例如，使用前一个有效值填充）
data.fillna(method='ffill', inplace=True)

# 再次检查缺失值
print(data.isnull().sum())
4. 数据分析
在进行可视化之前，我们可以进行一些基本的数据分析，例如计算平均气温、最高气温和最低气温：

# 计算平均气温
mean_temp = data['temperature'].mean()
print(f'平均气温: {mean_temp:.2f}°C')

# 计算最高气温
max_temp = data['temperature'].max()
print(f'最高气温: {max_temp:.2f}°C')

# 计算最低气温
min_temp = data['temperature'].min()
print(f'最低气温: {min_temp:.2f}°C')
5. 数据可视化
接下来，我们使用matplotlib和seaborn进行可视化。

5.1 折线图
首先，绘制气温随时间变化的折线图：

import matplotlib.pyplot as plt

# 绘制折线图
plt.figure(figsize=(10, 5))
plt.plot(data.index, data['temperature'], marker='o', linestyle='-')
plt.title('气温随时间变化')
plt.xlabel('日期')
plt.ylabel('气温 (°C)')
plt.grid(True)
plt.show()
5.2 箱线图
箱线图可以帮助我们识别数据中的异常值：

import seaborn as sns

# 绘制箱线图
plt.figure(figsize=(10, 5))
sns.boxplot(x=data['temperature'])
plt.title('气温箱线图')
plt.xlabel('气温 (°C)')
plt.show()
5.3 热力图
如果我们想按月份查看气温分布，可以使用热力图：

# 添加月份列
data['month'] = data.index.month

# 绘制热力图
plt.figure(figsize=(10, 5))
sns.heatmap(data.groupby('month')['temperature'].mean().unstack(), annot=True, cmap='coolwarm', fmt='.1f')
plt.title('每月平均气温热力图')
plt.xlabel('月份')
plt.ylabel('年份（假设数据为同一年）')
plt.show()
注意：这里的unstack()方法用于将分组后的数据转换为一个适合热力图的格式。由于我们的示例数据只有一年的数据，所以年份标签可能不太准确，但在实际应用中，你可以根据具体情况进行调整。

6. 结论
通过本文的介绍，我们学习了如何使用Python进行气温数据的处理与可视化。从数据读取、清洗、分析到可视化，每一步都包含了详细的代码演示。希望这些内容能够帮助你更好地理解和应用Python进行数据处理与可视化。Matplotlib这个绘图module--数据的可视化（不就是画个图嘛，“可视化“，应该叫dashboard）
从以往的认知中，猜想这个module到底应该是做什么的？绘图，什么是绘图，赫然让想起了小学老师教导的“点动成线，线动成面”。小学学习的柱状图，饼图，想matplotlib也应该是做这样的事情的。

绘制折线图
import matplotlib.pyplot as plt
x = [1,2,3,4,5]
y = [1,4,9,16,25]
plt.plot(x,y)

plt.title("Square Numbers")
plt.xlabel("Value")
plt.ylabel("Square of Value")

plt.show()

这和传说中的helloworld有异曲同工之妙啊。这与print("hello world")有什么大致的区别的吗？既然知道了折线图的画法，不妨大胆一些，那些使用坐标的绘图是不是替换一下plot这个方法呢？不妨试一下。

绘制柱状图

使用bar这个方法
​
绘制散点图

还是替换了一个方法而已
接下来要思考一个问题了，就是plot函数，bar函数，scatter函数。处理的是两个list。

当我意识这几个函数是并行计算的时候，我瞬间不知道并行计算如何处理x,y这两个list了。瞬间我就意识到这并不是并行执行了。应该是先处理x，然后处理y。

推导出来的步骤：

根据x的数量以及最大值，绘制x坐标
根据y的数量以及最大值，绘制y的坐标
在x轴标注投影坐标
在画布标注坐标的位置
绘制饼图
import matplotlib.pyplot as plt

# 数据
labels = ['A', 'B', 'C', 'D']
sizes = [15, 30, 45, 10]

# 饼图设置
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)

# 等轴比例
ax1.axis('equal')

# 显示图形
plt.show()

什么是饼图，姑且只能知道的是饼图是显示数据量和百分比的关系。更甚的理解大可不必思考。图表的意义在于更加直观地明白产品的本质。而不是将让本来简单的事物变得复杂。

绘制热力图
import matplotlib.pyplot as plt
import numpy as np

# 生成随机数据
data = np.random.rand(10, 10)

# 绘制热力图
plt.imshow(data, cmap='hot', interpolation='nearest')
plt.colorbar()
plt.show()

绘制等高线图
import matplotlib.pyplot as plt
import numpy as np

# 创建数据
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = np.sin(np.sqrt(X**2 + Y**2))

# 绘制等高线图
plt.contour(X, Y, Z, levels=20, cmap='RdGy')
plt.colorbar()

# 添加标题和标签
plt.title('Contour Plot')
plt.xlabel('X')
plt.ylabel('Y')

# 显示图形
plt.show()

绘制直方图
import matplotlib.pyplot as plt
import numpy as np

# 生成随机数据
data = np.random.normal(0, 1, 1000)

# 绘制直方图
plt.hist(data, bins=30, edgecolor='black')

# 添加标题和坐标轴标签
plt.title('Histogram of Random Data')
plt.xlabel('Value')
plt.ylabel('Frequency')

# 显示图形
plt.show()

​
绘制雷达图
import numpy as np
import matplotlib.pyplot as plt

# 数据
labels = ['A', 'B', 'C', 'D', 'E']
data = [4, 5, 7, 3, 6]

# 计算角度
angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)
angles = np.concatenate((angles, [angles[0]]))

# 绘图
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)
ax.plot(angles, data, 'o-', linewidth=2)
ax.fill(angles, data, alpha=0.25)

# 设置雷达图的标签
ax.set_thetagrids(angles[:-1] * 180/np.pi, labels)

# 设置极径范围
ax.set_ylim([0, 8])

# 显示图形
plt.show()


绘制动态图
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# 创建一个空的图像对象
fig, ax = plt.subplots()

x = np.linspace(0, 2*np.pi, 200)
y = np.sin(x)
line, = ax.plot(x, y)

def update(frame):
    line.set_ydata(np.sin(x + frame/10))
    return line,


ani = FuncAnimation(fig, update, frames=range(200), interval=20)


# 显示动画
plt.show()
绘制3D图形
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 生成数据
x = np.random.randn(100)
y = np.random.randn(100)
z = np.random.randn(100)

# 绘制3D散点图
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(x, y, z, c='r', marker='o')
ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')
plt.show()


简介： 本文介绍了一个基于Python的中国天气网数据采集与可视化分析系统，通过requests和BeautifulSoup库实现数据爬取，利用matplotlib、numpy和pandas进行数据可视化，提供了温湿度变化曲线、空气质量图、风向雷达图等分析结果，有效预测和展示了未来天气信息。
概要
天气预报我们每天都会关注，我们可以根据未来的天气增减衣物、安排出行，每天的气温、风速风向、相对湿度、空气质量等成为关注的焦点。本次使用python中requests和BeautifulSoup库对中国天气网当天和未来14天的数据进行爬取，保存为csv文件，之后用matplotlib、numpy、pandas对数据进行可视化处理和分析，得到温湿度度变化曲线、空气质量图、风向雷达图等结果，为获得未来天气信息提供了有效方法。

1、数据获取
1.1请求网站链接

首先查看中国天气网的网址：http://www.weather.com.cn/weather/101280701.shtml 这里就访问本地的天气网址，如果想爬取不同的地区只需修改最后的101280701地区编号即可，前面的weather代表是7天的网页，weather1d代表当天，weather15d代表未来14天。这里就主要访问7天和14天的中国天气网。采用requests.get()方法，请求网页，如果成功访问，则得到的是网页的所有字符串文本。这就是请求过程。

def getHTMLtext(url):       
 """请求获得网页内容"""  
 try:           
  r = requests.get(url, timeout = 30)           
  r.raise_for_status()           
  r.encoding = r.apparent_encoding           
  print("成功访问")           
  return r.text       
 except:           
  print("访问错误")           
  return" "
提取有用信息

这里采用BeautifulSoup库对刚刚获取的字符串进行数据提取，首先对网页进行检查，找到需要获取数据的标签：



可以发现7天的数据信息在div标签中并且id=“7d”，并且日期、天气、温度、风级等信息都在ul和li标签中，所以我们可以使用BeautifulSoup对获取的网页文本进行查找div标签id=“7d”，找出他包含的所有的ul和li标签，之后提取标签中相应的数据值，保存到对应列表中。

这里要注意一个细节就是有时日期没有最高气温，对于没有数据的情况要进行判断和处理。另外对于一些数据保存的格式也要提前进行处理，比如温度后面的摄氏度符号，日期数字的提取，和风级文字的提取，这需要用到字符查找及字符串切片处理。

def get_content(html):  
"""处理得到有用信息保存数据文件"""  
final = []          # 初始化一个列表保存数据  
bs = BeautifulSoup(html, "html.parser")  # 创建BeautifulSoup对象  
body = bs.body  
data = body.find('div', {'id': '7d'})   # 找到div标签且id = 7d
下面爬取当天的数据

data2 = body.find_all('div',{'class':'left-div'})  
text = data2[2].find('script').string   
text = text[text.index('=')+1 :-2]   # 移除改var data=将其变为json数据  
jd = json.loads(text)  
dayone = jd['od']['od2']     # 找到当天的数据  
final_day = []           # 存放当天的数据  
count = 0  
for i in dayone:  
temp = []  
if count <=23:  
temp.append(i['od21'])     # 添加时间 
temp.append(i['od22'])     # 添加当前时刻温度  
temp.append(i['od24'])     # 添加当前时刻风力方向  
temp.append(i['od25'])     # 添加当前时刻风级 
temp.append(i['od26'])     # 添加当前时刻降水量  
temp.append(i['od27'])     # 添加当前时刻相对湿度  
temp.append(i['od28'])     # 添加当前时刻控制质量  
#print(temp)  
final_day.append(temp)  
count = count +1
下面爬取7天的数据

ul = data.find('ul')      # 找到所有的ul标签  
li = ul.find_all('li')      # 找到左右的li标签  
i = 0     # 控制爬取的天数  
for day in li:          # 遍历找到的每一个li  
if i < 7 and i > 0:  
temp = []          # 临时存放每天的数据  
date = day.find('h1').string     # 得到日期  
date = date[0:date.index('日')]   # 取出日期号  
temp.append(date)      

inf = day.find_all('p')      # 找出li下面的p标签,提取第一个p标签的值，即天气  
temp.append(inf[0].string)  


    tem_low = inf[1].find('i').string   # 找到最低气温  

    if inf[1].find('span') is None:   # 天气预报可能没有最高气温  
        tem_high = None  
    else:  
        tem_high = inf[1].find('span').string  # 找到最高气温  
    temp.append(tem_low[:-1])  
    if tem_high[-1] == '℃':  
     temp.append(tem_high[:-1])  
    else:  
     temp.append(tem_high)  
    wind = inf[2].find_all('span')  # 找到风向  
    for j in wind:  
     temp.append(j['title'])  
    wind_scale = inf[2].find('i').string # 找到风级  
    index1 = wind_scale.index('级')  
    temp.append(int(wind_scale[index1-1:index1]))  
    final.append(temp)  
i = i + 1  

return final_day,final
同样对于/weather15d：15天的信息，也做同样的处理，这里经过查看后发现他的15天网页中只有8-14天，前面的1-7天在/weather中，这里就分别访问两个网页将爬取得到的数据进行合并得到最终14天的数据。- 前面是未来14天的数据爬取过程，对于当天24小时的天气信息数据，经过查找发现他是一个json数据，可以通过json.loads()

方法获取当天的数据，进而对当天的天气信息进行提取。



1.2 保存csv文件

前面将爬取的数据添加到列表中，这里引入csv库，利用f_csv.writerow(header)和f_csv.writerows(data)方法，分别写入表头和每一行的数据，这里将1天和未来14天的数据分开存储，分别保存为weather1.csv和weather14.csv，下面是他们保存的表格图：





2. 可视化分析
2.1 当天温度变化曲线图

采用matplotlib中plt.plot()方法绘制出一天24小时的温度变化曲线，并用plt.text()方法点出最高温和最低温，并画出平均温度线，下图为温度变化曲线图：(代码见附录)



分析可以发现这一天最高温度为20℃，最低温度为13℃，并且平均温度在15.8℃左右，通过对时间分析，发现昼夜温差7℃，低温分布在凌晨，高温分布在中午到下午的时间段。

2.2 当天相对湿度变化曲线图

采用matplotlib中plt.plot()方法绘制出一天24小时的湿度变化曲线，并画出平均相对湿度线，下图为湿度变化曲线图：(代码见附录)



分析可以发现这一天最高相对湿度为69%，最低相对湿度为29℃，并且平均相对湿度在46%左右，通过对时间分析，清晨的湿度比较大，而下午至黄昏湿度较小。

2.3 温湿度相关性分析图

经过前面两个图的分析我们可以感觉到温度和湿度之间是有关系的，为了更加清楚直观地感受这种关系，使用plt.scatter()方法将温度为横坐标、湿度为纵坐标，每个时刻的点在图中点出来，并且计算相关系数，下图为结果图：



分析可以发现一天的温度和湿度具有强烈的相关性，他们呈负相关，这就说明他们时间是负相关关系，并且进一步分析，当温度较低时，空气中水分含量较多，湿度自然较高，而温度较高时，水分蒸发，空气就比较干 燥，湿度较低，符合平时气候现象。

2.4 空气质量指数柱状图

空气质量指数AQI是定量描述空气质量状况的指数，其数值越大说明空气污染状况越重，对人体健康的危害也就越大。一般将空气质量指数分为6个等级，等级越高说明污染越严重，下面使用plt.bar方法对一天24小时的空气质量进行了柱状图绘制，并且根据6个等级的不同，相应的柱状图的颜色也从浅到深，也表明污染逐步加重，更直观的显示污染情况，并且也将最高和最低的空气质量指数标出，用虚线画出平均的空气质量指数，下图是绘制结果图：



上面这张是南方珠海的控制质量图，可以看出空气质量指数最大也是在健康范围，说明珠海空气非常好，分析可以发现这一天最高空气质量指数达到了57，最低则只有35，并且平均在485左右，通过时间也可以发现，基本在清晨的时候是空气最好的时候（6-15点），在下午是空气污染最严重的时候，所以早上一般可以去外面呼吸新鲜的空气，那时污染最小。

而下面这个空气质量图是选取的北方的一个城市，可以看到这里的环境远远比不上珠海。



风向风级雷达图

统计一天的风力和风向，由于风力风向使用极坐标的方式展现较好，所以这里采用的是极坐标的方式展现一天的风力风向图，将圆分为8份，每一份代表一个风向，半径代表平均风力，并且随着风级增高，蓝色加深，最后结果如下所示：





分析可以发现这一天东北风最多，平均风级达到了1.75级，西南风也有小部分1.0级，其余空白方向无来风。

2.5 未来14天高低温变化曲线图

统计未来14天的高低温度变化，并绘制出他们的变化曲线图，分别用虚线将他们的平均气温线绘制出来，最后结果如下所示：



分析可以发现未来14天高温平均气温为17.5℃，温度还是比较适中，但是未来的第3天有降温，需要做好降温准备，低温前面处于平稳趋势，等到第3天开始下降，伴随着高温也下降，整体温度下降，低温平均在12℃左右。

2.6 未来14天风向风级雷达图

统计未来14天的风向和平均风力，并和前面一样采用极坐标形式，将圆周分为8个部分，代表8个方向，颜色越深代表风级越高，最后结果如下所示：





分析可以发现未来14天西北风、东北风所占主要风向，风级最高达到了5级，最低的平均风级也有3级。

2.7 未来14天气候分布饼图

统计未来14天的气候，并求每个气候的总天数，最后将各个气候的饼图绘制出来，结果如下所示：



分析可以发现未来14天气候基本是“多云”、“晴转多云”和“晴”，多云的天数较多,结合前面的气温分布图可以看出在第3天气温高温下降，可以推测当天晴转阴，导致气温下降。

3、结论
1.首先根据爬取的温湿度数据进行的分析，温度从早上低到中午高再到晚上低，湿度和温度的趋势相反，通过相关系数发现温度和湿度有强烈的负相关关系，经查阅资料发现因为随着温度升高水蒸汽蒸发加剧，空气中水分降低湿度降低。当然，湿度同时受气压和雨水的影响，下雨湿度会明显增高。

2.经查阅资料空气质量不仅跟工厂、汽车等排放的烟气、废气等有关，更为重要的是与气象因素有关。由于昼夜温差明显变化，当地面温度高于高空温度时，空气上升，污染物易被带到高空扩散；当地面温度低于一定高度的温度时，天空形成逆温层，它像一个大盖子一样压在地面上空，使地表空气中各种污染物不易扩散。一般在晚间和清晨影响较大，而当太阳出来后，地面迅速升温，逆温层就会逐渐消散，于是污染空气也就扩散了。

3.风是由气压在水平方向分布的不均匀导致的。风受大气环流、地形、水域等不同因素的综合影响，表现形式多种多样，如季风、地方性的海陆风、山谷风等，一天的风向也有不同的变化，根据未来14天的风向雷达图可以发现未来所有风向基本都有涉及，并且没有特别的某个风向，原因可能是近期没有降水和气文变化不大，导致风向也没有太大的变化规律。

4.天气是指某一个地区距离地表较近的大气层在短时间内的具体状态。跟某瞬时内大气中各种气象要素分布的综合表现。根据未来14天的天气和温度变化可以大致推断出某个时间的气候，天气和温度之间也是有联系的。

4、代码框架
代码主要分为weather.py：对中国天气网进行爬取天气数据并保存csv文件；data1_analysis.py：对当天的天气信息进行可视化处理；data14_analysis.py：对未来14天的天气信息进行可视化处理。下面是代码的结构图：

简介： 本文设计并实现了一个基于Python的当当二手书数据分析与可视化系统，通过数据收集、清洗、聚类分析和可视化展示，为二手书市场提供全面的数据分析和决策支持，以促进资源循环利用和市场效率优化。
1.1 研究背景及现状
1.1.1 研究背景

生态文明建设是我国的基本国情之一，资源利用作为应该重要的环节[1]。然而随着大学校园内掀起倡导的低碳环保热潮，高校学生教材及其他书籍的目前的处理方式已被大多人所关注[2]。从循环利用资源的角度出发[3]， 大学校园二手市场应运而生也成为了必然[4]。

当前激烈的社会竞争使高校学生对书籍的需求日益增加,而闲置书籍导致的资源浪费十分不利于环境保护,建立二手书交易平台将可以有效解决这个问题。本文首先提出网络二手书交易平台的必要性,然后根据调查问卷数据对平台的设计和实现进行可行性分析,最后就具体营运方案进行设计,相信二手书交易平台有着广阔的市场前景和发展空间[5]。

在如此广阔的发展前景下，我们需要运用更加先进的技术对市场进行分析，所以对二手书市场的分析能够给用户和商家增加更加直观的数据呈现，并且能够给看不懂数据的能够呈现最直观的图表，并且进行比较。

1.1.2 研究现状

随着二手书交易市场的不断扩大，对二手书市场数据进行数据清洗和数据预处处理越来越重要[6],然而Python作为一种强大而灵活的编程语言，具备丰富的数据处理和可视化工具，因此成为了二手书数据分析的首选工具。

国内现状：图书销量数据可视化：国内学者在使用聚类分析算法对销量数据进行分类，对潜在有价值的销量的规律和趋势进行分析，利用地理信息系统和数据可视化技术，绘制了图书销售的热力图和区域分布图，帮助商家合理规划销售策略和资源配置。

国外现状：图书销售数据可视化：国外学者利用时间序列分析方法对图书销售数据进行研究，发现销售数据中存在的周期性和趋势性。Mary Johnson 等人则利用地理信息系统和数据可视化技术，绘制了用户分布地图和销售热点地图，帮助商家了解和满足消费者的地理位置需求。

1.2 研究目的、意义及方法
1.2.1 研究目的

具体研究目的如下：

市场需求预测：通过对二手书市场数据的分析，研究目的是预测不同类别二手书的需求量和销售趋势，为卖家提供有针对性的库存管理和定价策略，优化市场运作效率。

用户行为分析：研究旨在分析用户在二手书交易中的行为模式和偏好，包括购买偏好、搜索习惯、评价倾向等，通过挖掘用户数据，提供个性化推荐和增强用户体验。

市场竞争分析：通过对二手书市场中各个参与者的数据进行比较和分析，研究目的是了解不同卖家之间的竞争态势和优势，为市场参与者提供决策依据和竞争战略。

可视化呈现：研究旨在将二手书数据分析结果以直观的图表、图像和可交互的界面形式展示出来，提供给用户、管理者和研究人员一个更清晰、易懂的数据视觉化工具，帮助他们更好地理解和利用数据。

1.2.2 研究意义

随着校园环保热潮的兴起与在校大学生人数的日益增长，环境保护与学生对课本需求量增多的矛盾日益突出。鼓励大学生循 环利用图书，在增强环保意识的同时，用实际行动践行可持续发展理念[7]，为解决该问题，节约资源， 使其畅循环，践行低碳环保理念，调查分析了消费者的迫切需求、高校二手书交易市场遇到的瓶颈以及 交易途径的可行性，提出了应对策略和方案[8]。

因此，本文基于Python语言和相关开发工具，从多个维度深入研究二手书市场数据，并经过数据可视化的方式将结果呈现出来。这样，可以二手书交易平台提供全面的市场分析和研究结果，也能为消费者提供有益的参考和决策支持，可为用户购买 专业书籍节省找寻的时间和精力[9]。

1.2.3 研究方法

本文的研究方法如下：

文献调研与学习：通过查阅相关资料，深入了解Python编程语言以及数据分析与可视化领域的知识。阅读相关研究论文、学术文章和专业书籍，掌握数据获取、清洗、处理和可视化的方法和技术。

数据获取与处理：收集二手书市场的相关数据，包括销售记录、价格变动、用户评价等。利用Python编程语言，结合适当的库和工具，进行数据清洗、整理和预处理，确保数据的质量和准确性。

数据分析与建模：运用Python中的数据分析库和算法，对二手书市场的数据进行统计分析、挖掘隐藏规律和趋势。例如，可以使用Pandas进行数据处理和分析，使用NumPy进行数值计算。

可视化展示：基于Python的可视化库（如Matplotlib、Seaborn和Plotly）或交互式可视化工具（ECharts），将数据分析结果以直观、易懂的图表、图像和界面形式展示出来[9]。通过可视化呈现，使数据更易于理解和解释，提供更直观的信息展示和决策支持。

实验与改进：通过实际操作和调试，不断改进分析过程和结果。在研究中遇到困难时，可以利用在线资源、论坛或与同行进行交流，寻求解决方案和技术支持。

1.3 研究内容
本研究旨在基于Python开发一个二手书的数据分析与可视化系统，以实现对二手书市场的全面分析和可视化展示。具体研究内容如下：

数据收集与清洗：使用Python的Requests库进行数据爬取，从在线书店、交易平台和社交媒体等渠道获取相关的二手书数据。接着，利用pandas库对爬取的数据进行清洗和预处理，去除重复项和异常值，确保数据的准确性和完整性[10]。

数据存储与管理：将清洗后的数据存储到MySQL数据库中，利用Python编程语言连接和操作数据库，实现数据的持久化存储和高效查询。此外，构建一个数据管理后台，具备增删改查功能，并支持多用户登录，以确保数据的安全性和权限管理。

数据分析与聚类：利用Python进行数据分析，应用聚类算法对二手书市场的数据进行分类和分群。通过训练和确定最优聚类数，将二手书数据进行有意义的分类，揭示潜在的市场细分和特征。

可视化展示与Web应用：利用Echarts库进行数据可视化展示，在HTML+JS+CSS技术的支持下，使用Flask框架构建一个Web应用界面。通过可视化图表和交互式界面，呈现对二手书市场整体情况、不同类型二手书的销售情况和价格走势等数据分析结果。

用户行为与竞争分析：通过对用户行为模式和市场竞争状况的分析，提供针对性的推荐和决策支持。利用可视化展示工具，将用户行为和市场竞争的分析结果以直观的方式展示，帮助用户理解二手书市场的趋势和潜在机会。

1.4 结构
结构如下：

第1章 引言 介绍二手书市场的重要性和发展趋势，以及数据分析与可视化在该领域的应用价值。明确研究目标，概述研究内容和方法。

第2章 相关开发工具介绍，并且介绍相关技术。

第3章 项目框架设计，对整个项目的流程和爬虫流程进行一个简单的说明，并且使用流程图进行表示说明。

第4章 数据收集与处理 使用Python的Requests库实现二手书市场数据的爬取，、利用pandas库对爬取的数据进行清洗和预处理，去除重复项和异常值，确保数据的准确性和完整性。

第5章数据分析与可视化展示使用Python进行数据分析，采用聚类算法对二手书市场数据进行分类和分群，确定最优聚类数，并进行模型训练。使用Echarts库实现对二手书市场整体情况、不同类型二手书的销售情况和价格走势等数据分析结果的可视化展示。利用Flask框架构建一个基于HTML+JS+CSS的Web应用界面，支持用户交互和动态展示。

第6章利用Flask框架构建一个基于HTML+JS+CSS的Web应用界面，支持用户交互和动态展示，呈现出可视化大屏和后端管理系统。

第7章总结与讨论 对二手书市场的整体情况、不同类型的二手书销售情况、价格走势、用户行为和市场竞争等进行分析和可视化展示，并对结果进行讨论和解释。总结研究工作和成果，强调研究的创新点和应用价值。展望未来可能的改进和扩展方向，提出进一步研究的建议。

5 数据分析
5.1需求分析
5.1.1二手书各省市数量分布分析

提取出非缺失的地址数据，并进行地址标准化处理。例如，将"北京市"替换为"北京省"，将"上海市"替换为"上海省"等。根据省份或直辖市对图书进行分组，并统计每个省份或直辖市对应的图书数量。将省份或直辖市和对应的图书数量存储在列表中，每个元素是一个字典，包含省份或直辖市和图书数量的键值对。主要代码如下：



运行结果如下图5-4所示：

5.1.2 二手书前十年份排行分析

从MySQL数据库中查询二手书数据，并使用Pandas库将查询结果存储为DataFrame。对数据进行预处理，包括替换缺失值、去重等操作。提取出非缺失的年份数据，并将年份进行切割处理，只保留年份的第一个部分。对处理后的年份数据进行统计，统计每个年份对应的图书数量。根据图书数量进行降序排序，得到前十个年份及其对应的图书数量。将前十个年份和图书数量存储在列表中，每个元素是一个字典，包含年份和图书数量的键值对。主要代码如下：



5.1.3二手书前十年出版社排行分析

根据出版社对图书进行分组，并统计每个出版社对应的图书数量。根据图书数量进行降序排序，得到前十个出版社及其对应的图书数量。将前十个出版社和图书数量存储在列表中，每个元素是一个字典，包含出版社和图书数量的键值对。主要代码如下：



5.1.4二手书包装占比分析

根据包装方式对图书进行分组，并统计每种包装方式对应的图书数量。将包装方式和对应的图书数量存储在列表中，每个元素是一个字典，包含包装方式和图书数量的键值对。通过以上步骤，可以将二手书的包装占比分析结果可视化展示，帮助用户了解不同包装方式在二手书市场的分布情况。主要代码如下：



5.1.5二手书价格分布分析

将价格字段转换为数值类型，以便后续的分析。设定价格区间，并使用pd.cut()函数对价格进行分组，统计每个价格区间内的图书数量。将价格区间和对应的图书数量存储在列表中，每个元素是一个字典，包含价格区间和图书数量的键值对。主要代码如下：



运行结果如下图5-5所示：

5.1.6 发货时间分布分析

根据发货时间字段的不同取值，将发货时间进行转换。如果发货时间包含"小时"，则提取出小时数作为发货时间；如果发货时间包含"天内"，则将天数乘以24得到发货时间（单位为小时）。设定发货时间区间，并使用pd.cut()函数对发货时间进行分组，统计每个时间区间内的图书数量。将时间区间和对应的图书数量存储在列表中，每个元素是一个字典，包含时间区间和图书数量的键值对。通过以上步骤，可以将二手书的发货时间分布分析结果可视化展示，帮助用户了解不同发货时间区间内的二手书数量情况。主要代码如下：



5.1.7店铺等级分析

根据店铺等级对图书进行分组，并统计每个店铺等级对应的图书数量。将店铺等级和对应的图书数量存储在列表中，每个元素是一个字典，包含店铺等级和图书数量的键值对。最后，在可视化大屏的前端页面中展示该数据，通过ECharts库生成柱状图或其他形式的图表，以直观地显示不同店铺等级的二手书数量情况。

通过以上步骤，可以将二手书的店铺等级分析结果可视化展示，帮助用户了解不同店铺等级在二手书市场中的分布情况。这样的分析可以为买家提供参考，选择信誉良好、高等级的店铺购买二手书；同时，也为卖家了解自己店铺在市场中的竞争力和知名度提供参考依据。主要代码如下：



5.2 聚类分析
5.2.1 轮廓法和肘部法选择最优聚类簇

肘部法通过计算不同聚类数量下的聚类误差平方和（SSE）来评估聚类的质量。其公式如表示：



其中Ci表示簇，k表示聚类中心的个数，p表示某个簇内的样本，m表示质心点。通过观察聚类数量与聚类内部方差（或其他类似指标）之间的关系，来找到一个“肘部”，这个“肘部”对应的聚类数量通常被认为是最佳的。在肘部法中，随着聚类数量的增加，聚类内部方差通常会逐渐减少。但是，当聚类数量增加到某个点时，进一步增加聚类数量不再显著减少内部方差。这个点就是肘部，也就是在这个点之后，内部方差的减少程度变得较缓慢。

轮廓系数法是通过计算轮廓系数来实现的。轮廓系数衡量了样本与其所在聚类以及其他聚类之间的相似度，数值范围在[-1, 1]之间。具体而言，对于每个样本，轮廓系数是通过计算每一个样本i与同一聚类中其他样本的平均距离（称为ai）和计算每一个样本i到最近邻聚类中所有样本的平均距离（称为bi）来得到的。轮廓系数可以用以下公式表示：



其中，a越小且b越大，轮廓系数越接近1，表示样本与其所在聚类更相似；反之，轮廓系数越接近-1，表示样本与其所在聚类不相似。

利用Python画出肘部法则图和轮廓系数图，如图5.1展示了肘部法确定最优聚类数目的结果。如图5.1展示了轮廓系数法确定最优聚类数目的结果。



图5.1肘部法则图图

从图5.2所示，当K为3时，图中斜率突然由大变小，图像出现“肘部”，所以K=3是最佳聚类。



图5.2轮廓法聚类簇折线图

从图5-9所示 ，当K为3时，轮廓系数达到了峰值，说明最佳聚类为3。主要代码如下：



5.2.2聚类分析实现

特征选择和数据预处理：根据需求选择合适的特征列，并进行数据预处理。这可能包括缺失值处理、标准化、归一化等操作，以确保数据在聚类分析之前处于合适的状态。

自定义K-means聚类算法：编写自定义的K-means聚类算法函数。该函数应接受数据集、聚类数目和最大迭代次数作为输入，并返回聚类结果和最终的聚类中心。在函数内部，需要随机初始化聚类中心、迭代计算每个样本点与聚类中心之间的距离，将样本点分配到最近的聚类中心，并更新聚类中心的位置，直到满足停止条件（如达到最大迭代次数或聚类中心不再发生变化）。

进行聚类分析：使用最优的聚类数对数据集'标题、作者、年份','店铺等级', '价格', '小时','评级进行聚类分析。调用自定义的K-means聚类算法函数，并将数据集和最优的聚类数作为输入。获取聚类结果和最终的聚类中心。

进一步的数据分析和可视化展示：根据需求，可以对不同的聚类簇进行进一步的数据分析和可视化展示。例如，统计每个聚类簇的数量、绘制散点图以显示不同聚类簇的分布情况等。主要代码如下，聚类分析图如图5.2所示：





图5.2聚类分析图

KMeans 聚类分析，并将每个数据点分配到了不同的聚类中心（或者叫做簇）。从输出结果可以看出，每个数据点都被标记为一个聚类标签（cluster_label）。从图5-10可以看出，数据点被分为了3个不同的聚类（0到2），每个聚类被用不同的颜色进行可视化展示。每个聚类中心用加号标记，并且每个数据点与其对应的聚类中心之间都画了虚线，表示数据点到聚类中心的距离。由于数据点的特征较多，这里使用了 t-SNE 算法进行了降维，将数据点投影到了二维平面上以便进行可视化展示。

其中第一类的书籍具有以下特点：标题多为一些关于日常生活、情感、诗词或文学作品的书籍，例如《学会选择：懂得放弃》、《每天进步一点点》、《美是世间治愈一切的良药》等。作者是一些文学作家、诗人、心理学家等。

第二类这一类的书籍具有以下特点：标题与历史、传记、人物故事、文化历史等相关，例如《锦囊妙记安天下：细说历史上那些谋士》、《大唐良相李吉甫》等。作者是一些历史学家、文化评论家、传记作家等，或者一些历史文化方面的作品。

第三类这一类的书籍可能具有以下特点：标题涉及诗歌、文学作品、艺术创作等，例如《中国诗词大会(第4季下)》、《艺术在路上（精装）》等。作者是一些诗人、文学家、艺术评论家等，或者一些文学、艺术作品

6 数据可视化展示
6.1前端大屏可视化实现
6.1.1发货时间分析

我们可以看到大部分二手书的发货时间集中在20-50小时和10-20小时的区间内。这可能是因为这些发货时间范围更符合卖家的处理时间和物流配送速度。此外，发货时间超过100小时的二手书数量很少，说明大部分卖家在较短时间内完成发货。这些结论可以为买家提供参考，可以根据发货时间的分布情况，在可接受的范围内选择合适的二手书购买。同时，也可以鼓励卖家减少发货时间，提高交易效率和用户体验。如下图6.1所示



图6.1二手书发货时间图

6.1.2不同二手年份分析

我们可以看到不同年份的二手书数量呈现出一定的波动趋势。从2010年开始，二手书数量逐年增加，直至2022年达到峰值，然后在2023年略有下降。这可能反映了二手书市场的发展和变化，也与读者对不同年份的图书需求有关。这些结论可以为读者提供参考，在购买二手书时可以注意不同年份的图书数量情况。同时，也可以为卖家或平台管理者提供参考，了解不同年份的二手书市场需求，调整库存和采购策略。如下图6-2所示



图6.2前十年年二手书数据分析图

6.1.3不同出版社分析

我们可以看到大部分二手书的出版社信息未知。这可能是因为在数据采集过程中缺少出版社信息或者数据处理过程中的缺失问题。而对于已知的出版社中，中国社会科学出版社、中信出版社、人民邮电出版社和机械工业出版社是数量较多的出版社。

这些结论可以为买家提供参考，可以根据自己对出版社的偏好，在购买二手书时选择信誉良好或热门的出版社出版的图书。同时，也可以为卖家或平台管理者提供参考，了解不同出版社的图书销量情况和市场需求，进行库存管理和采购策略的优化。如图6.3所示：



图6.3二手书出版设排名图

6.1.4地区分布分析

我们可以看到不同地区的二手书分布情况。河北地区的二手书数量最多，达到25221本，其次是北京、江苏和湖北等地。而一些地区的二手书数量较少，例如海南、西藏和宁夏等。

这些结论可以为买家提供参考，在选择二手书时可以关注自己所在地区或感兴趣地区的二手书供应情况。同时，对于卖家或平台管理者来说，了解地区分布情况可以帮助他们了解市场需求和地域特点，进行库存管理和物流配送的优化。如图6.4所示：

图6.4地区二手书分布图

6.1.5价格分布分析

我们可以看到价格区间为10-20和20-50的二手书数量最多，分别有19694本和18140本。这表明在这两个价格区间内有更多的二手书供应和交易活动。而价格较低的二手书（小于10元）也有相当数量的供应，可能是一些折旧程度较高或较老的图书。

此外，价格区间为50-100、100-150、150-500和大于500的二手书数量逐渐减少，说明价格较高的二手书相对较少，可能是因为价格较高的二手书需求较小或者市场竞争较少。如图6.5所示。



图6.5价格分析折现图

6.1.6店铺分布分析

我们可以看到不同等级的店铺数量分布情况。lv1和lv9是店铺数量最少的两个等级，而lv8是店铺数量最多的等级。这可能反映了店铺等级与店铺规模和销售活动之间的关系。通常来说，等级越高的店铺，其规模和销售活动往往更大。

这些结论可以为买家提供参考，在选择二手书时可以关注不同等级店铺的信誉和可靠性。对于卖家或平台管理者来说，了解店铺等级分布情况可以帮助他们了解市场竞争态势和店铺规模，制定相应的经营策略和促销活动。如图6.6所示。



图6.6店铺等级分析图

图6.7可二手书视化大屏

6.2后台系统实现
6.2.1登录注册页面

（1）登录

用户通过在登录页面输入账号和密码，并点击登录按钮。在后端的/login路由中，从前端请求的表单数据中获取账号和密码。调用user_service.get_user()函数，传入账号和密码进行验证。如果验证成功（即返回结果大于0），则将登录状态和角色信息存储在session中，表示用户已登录。返回响应状态码200给前端，表示登录成功。登录效果如下图6.8所示：



图6.8登录页面效果图

（2）注册

用户通过在注册页面填写用户名、账号和密码，并点击注册按钮。在后端的/user/reg路由中，从前端请求的表单数据中获取用户名、账号和密码。调用user_service.add_user()函数，将用户信息插入数据库中。返回响应状态码200给前端，表示注册成功。注册效果如下图6.9所示。



图6.9注册效果图

6.2.2用户信息管理

用户管理页面通过路由为进行访问，用户访问时会根据从前端请求中获取请求参数，并通过路由指向执行user_manager()视图函数，渲染生成用户管理页面，这个页面能展示所有用户信息，可进行添加、编辑和删除操作。当添加用户时，会指向添加用户的路由，并执行处理函数为user_add()，插入用户信息添加到数据库。同理，编辑及删除功能也是如此，从而实现用户数据管理功能。用户信息管理效果如下图6.10所示：



图6.10用户信息管理效果图

6.2.3二手书数据管理

二手书数据管理页面的路由为/html/notice，当用户访问该路径时，会执行名为notice_manager()的视图函数。在notice_manager()函数中，调用render_template()函数，将渲染后的二手书数据管理页面返回给前端。在二手书数据管理页面中，可以展示所有二手书的数据，并提供相应的操作，如添加、编辑和删除二手书数据。二手书数据管理效果如下图6.11所示：



图6.11二手书数据管理效果图数据分析和可视化是现代数据科学领域的核心技能之一。Python作为一种流行的编程语言，提供了丰富的库和工具来简化这一过程。在本文中，我们将探索如何使用Python进行数据分析和可视化，涵盖从数据导入、清洗到最终的可视化展示。

首先，我们需要导入数据。Python中的Pandas库是一个强大的数据分析工具，它可以方便地读取各种格式的数据文件。例如，我们可以使用以下代码导入一个CSV文件：

import pandas as pd

data = pd.read_csv('data.csv')
一旦数据被导入，下一步通常是数据清洗。这可能包括处理缺失值、转换数据类型、重命名列等。Pandas提供了丰富的方法来进行这些操作。例如，删除含有缺失值的行可以使用dropna()方法：

clean_data = data.dropna()
数据清洗后，我们通常需要进行一些统计分析或计算，以便更好地理解数据。Pandas支持广泛的数据操作，包括分组、聚合、排序等。例如，我们可以计算每组的平均值：

grouped_data = clean_data.groupby('category').mean()
接下来是数据可视化的部分。Matplotlib是Python中一个广泛使用的绘图库，它提供了创建各种图表的功能。而Seaborn是基于Matplotlib的高级接口，专门用于统计图形的绘制。以下是使用这两个库创建简单折线图的例子：

import matplotlib.pyplot as plt
import seaborn as sns

plt.plot(clean_data['x'], clean_data['y'])
plt.title('X vs Y')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
除了基本的图表，我们还可以利用Seaborn创建更复杂的统计图表，如箱型图、散点图矩阵等，这些都是分析数据分布和关系的有力工具。

最后，我们应该关注图表的设计原则，比如颜色选择、标签清晰度、图表布局等，这些都会影响图表的信息传递效率。良好的可视化设计可以使复杂数据变得易于理解。

总结来说，Python提供了一套完整的工具链来支持数据分析和可视化的过程。通过本文的介绍和示例，你可以看到如何从原始数据出发，经过一系列的处理和分析，最终得到有意义的可视化结果。每一步都是构建在之前步骤的基础上，逐渐深入数据的内在结构，揭示数据背后的故事。希望本文能成为你探索数据分析和可视化世界的跳板，开启你的数据科学之旅。简介： 本文介绍了一个基于Python的穷游网酒店数据采集与可视化分析系统，通过爬虫技术自动抓取酒店信息，并利用数据分析算法和可视化工具，提供了全国主要城市酒店的数量、星级、价格、评分等多维度的深入洞察，旨在为旅行者和酒店经营者提供决策支持。
1 需求分析
1.1 用户需求
1.1.1 背景与现状
穷游网是国内知名的旅游社区，在其网站上，用户可以自由分享旅行经验和攻略，也可以浏览其他用户的经验和攻略，以便更好地规划自己的旅行。而酒店信息是旅行攻略中不可或缺的一部分，因此穷游网也提供了丰富的酒店信息供用户参考。

随着旅游行业的发展，越来越多的人选择通过网络预订酒店，因此酒店信息的质量和准确性也变得越来越重要。在这个背景下，穷游网的酒店数据采集与分析显得尤为重要。

针对这一问题，穷游网采用了基于Python的数据采集与分析技术。通过爬虫技术，穷游网可以自动抓取各类酒店信息，包括酒店名称、地址、价格、评分等，同时还可以通过数据分析算法对这些信息进行处理和分析，以便更好地为用户提供准确的酒店信息。

目前，穷游网的酒店数据采集与分析工作已经取得了显著的成效。首先，通过数据采集技术，穷游网可以获取更全面、更准确的酒店信息，大大提高了用户的满意度和信任度。其次，通过数据分析技术，穷游网可以对酒店信息进行多维度的分析和比较，以便用户更好地选择适合自己的酒店。

因此，基于Python的穷游网酒店数据采集与分析技术，为用户提供了更好的旅行体验，也为旅游行业的发展做出了贡献。随着技术的不断更新和发展，相信这一技术将会在未来得到更广泛的应用和发展。

1.1.2 目标
穷游网酒店数据采集与分析的目标是提供用户准确、全面、可靠的酒店信息，以帮助用户更好地规划和选择旅行。首先，数据采集的目标是获取尽可能多的酒店信息。通过爬虫技术，穷游网可以自动抓取各类酒店的基本信息，包括酒店名称、地址、价格、评分等。通过大规模的数据采集，穷游网可以提供更全面、更详细的酒店信息，满足用户对酒店信息的多样化需求。其次，数据分析的目标是对酒店信息进行处理和分析，以提供更准确、更有价值的信息给用户。通过数据分析算法，可以对酒店的评分、价格等指标进行统计和比较，帮助用户更好地了解酒店的性价比，从而做出更明智的选择。最终目标是提高用户的旅行体验。通过提供准确、全面的酒店信息，用户可以在旅行前更好地了解酒店的情况，避免不必要的麻烦和烦恼。同时，通过数据分析与可视化，用户可以更好地选择适合自己的酒店，提高旅行的满意度和舒适度。

所以，穷游网酒店数据采集与分析的目标是为用户提供更好的旅行体验。通过准确的数据采集和深度的数据分析，穷游网可以为用户提供更全面、更准确、更有价值的酒店信息，帮助用户做出更明智的选择，提高旅行的质量和满意度。

1.2 功能需求
1.2.1 主要板块
（1）数据获取板块

主要负责从互联网上自动抓取酒店信息，并将其存储到数据库中。通过使用Python的爬虫技术，可以自动抓取穷游网上的全国主要城市的酒店信息，并将其转换为结构化的数据格式，以便后续的数据处理和分析。

（2）数据预处理板块

主要负责对采集到的数据进行清洗、去重、归一化等操作，以保证数据的准确性和可靠性。通过使用Python的数据处理库，可以对数据进行预处理，如去除重复数据、填充缺失值、标准化数据格式等。

（3）数据存储板块

主要负责将处理后的数据存储到数据库中，并对其进行分类、筛选和排序等操作。通过使用Python的数据库连接库，可以实现对MySQL数据库的连接和操作。

（4）数据分析与可视化板块

主要负责对酒店信息进行多维度的分析和比较，并将分析结果以图表、地图等形式直观地展现给用户。通过使用Python的数据分析和可视化库，可以实现对酒店信息的统计、聚类等功能，并将分析结果以图表、地图、饼图等形式呈现给用户。

1.2.2 主要方法
（1）数据获取；通过使用Python的爬虫库（如Requests）来实现对穷游网酒店页面的自动抓取。爬虫可以模拟浏览器行为，自动浏览网页并提取所需的酒店信息，如酒店名称、地址、评分、价格等。

（2）数据清洗：通过使用Python的数据处理库（如Pandas）对采集到的数据进行清洗、去重、归一化等操作。包括去除重复数据、处理缺失值、转换数据类型、标准化数据格式等，以确保数据的准确性和一致性。

（3）数据存储：通过使用Python的数据库连接库（如SQLAlchemy）将处理后的数据存储到数据库中。使用MySQL数据库根据需求进行数据的分类、筛选和排序，以方便后续的数据分析和查询。

（4）分析结果展示与说明：通过使用Python的数据分析和可视化库（如NumPy、pandas、pyecharts）对酒店数据进行统计、分析和可视化。可以进行各种分析，如平均价格、评分分布、地理位置分布等，并将结果以图表、地图、饼图等形式呈现给用户，以帮助用户更好地了解酒店的特点和性价比。

1.2.3 技术路线
本文结合研究内容和研究方法，制定具体技术路线图如图1.1所示。



图1.1技术路线示意图

2 开发环境及技术
2.1硬件设备
2.1.1 操作系统
操作系统windows7 64位及以上；500G物理内存；8G内存

2.1.2 其他
千兆宽带网络、安全软件、鼠标、键盘等。

2.2软件及IDE
2.2.1 MySql
MySQL是一种开源的关系型数据库管理系统（RDBMS），被广泛应用于Web应用程序的数据存储和管理。MySQL具有以下优点：

可靠性：MySQL具有良好的稳定性和可靠性，能够提供高可用性的数据存储和处理。它支持主从复制和故障转移等机制，保证了数据的持久性和可靠性。

易用性：MySQL具有简单、易学的操作界面和命令行工具，使得开发人员和管理员能够快速上手。同时，MySQL还提供了丰富的文档和社区支持，方便用户解决问题和学习使用。

扩展性：MySQL支持水平和垂直扩展，可以根据需求进行灵活的扩展和部署。它可以在单机环境下运行，也可以通过分布式部署实现高性能和高可扩展性。

性能优化：MySQL通过优化查询语句、索引设计、缓存管理等方式，提供了良好的性能和响应时间。它还支持多种存储引擎（如InnoDB、MyISAM等），可以根据具体需求选择合适的引擎来优化性能。

安全性：MySQL提供了多层次的安全机制，包括权限管理、数据加密、网络安全等，保护用户数据的安全和隐私。

2.2.2 PyCharm集成开发环境
PyCharm是一种专为Python开发而设计的集成开发环境（IDE），可以帮助开发人员轻松地进行数据采集与分析工作。PyCharm提供了智能代码补全、语法高亮、代码导航等功能，能够快速编写和修改Python代码。在数据采集过程中，可以方便地编写爬虫脚本，提取所需的酒店数据。PyCharm集成了调试器，可以帮助开发人员定位和修复代码中的错误。在数据分析阶段，可以通过调试工具逐步执行代码，观察变量的值和代码的执行路径，检查和验证数据处理逻辑。PyCharm支持各种数据分析库，如NumPy、Pandas、Matplotlib等，可以方便地进行数据处理、统计分析和可视化展示。开发人员可以使用这些库对采集到的酒店数据进行清洗、整理和分析。PyCharm支持插件扩展，可以根据需要安装和使用各种第三方插件，扩展IDE的功能。对于数据采集与分析任务，可以根据具体需求选择适合的插件，提供更多的功能和工具支持。

2.3主要技术
2.3.1 MySQL
MySQL是一种开源的关系型数据库管理系统，被广泛应用于各种规模的应用程序中，包括基于Python的穷游网酒店数据采集与分析。MySQL的安装和配置相对简单，具有用户友好的管理工具和命令行接口。对于开发人员来说，使用Python的MySQL驱动程序可以方便地连接和操作MySQL数据库。MySQL是一个开源的数据库管理系统，可以免费获取和使用。对于小型项目或者预算有限的项目来说，选择MySQL可以节约成本，并且能够获得活跃的社区支持和更新。MySQL与Python的兼容性非常好，有多个Python的MySQL驱动程序可供选择，如MySQLdb、PyMySQL和mysql-connector-python等。这使得在Python中连接和操作MySQL数据库变得非常简单和方便。

总之，MySQL作为一种可靠、高性能和易用的数据库管理系统，与Python的兼容性良好，适用于基于Python的穷游网酒店数据采集与分析。它具有良好的可靠性和稳定性、高性能、简单易用、可扩展性和安全性等优势。选择MySQL作为数据库技术可以提供稳定可靠的数据存储和高效的数据访问能力，满足数据采集与分析的需求。

2.3.2 Python
Python是一种简单易学、功能强大的编程语言，被广泛应用于各个领域，包括基于Python的穷游网酒店数据采集与分析。Python拥有大量的第三方库和工具，如NumPy、Pandas、Matplotlib、Scikit-learn等，可以满足各种数据采集和分析的需求。这些库提供了丰富的函数和方法，简化了数据处理、统计分析和可视化展示的工作。此外，Python的语法简单易学，对于新手来说上手难度较低。这使得开发人员可以快速编写和调试代码，提高开发效率。同时Python可以处理大规模的数据，提供了多种库和工具来支持大数据处理和分析。这使得在穷游网酒店数据采集与分析过程中，可以处理大量的酒店数据，并进行复杂的统计分析。

3 模块设计
3.1 数据获取方法
基于Python的穷游网酒店数据采集与分析使用requests库来获取数据。首先，需要分析穷游网酒店的数据接口，确定请求URL和参数。然后，使用requests库发送GET请求，并传递参数。接下来，可以使用json()方法将响应数据解析为Python对象（字典或列表）。最后，将提取的数据存入MySQL数据库中，实现持久存储。

3.2 数据预处理设计
（1）数据获取与加载：

将MySQL数据加载到Python中，使用Pandas库将数据转换为DataFrame格式进行处理。

（2）数据类型转换

检查数据的类型，进行数据类型转换。例如，将字符串转换为浮点型或整型。

（3）删除重复值

无重复数据。

（4）缺失值处理

检查数据中是否存在缺失值，并进行处理。使用Pandas的isnull()和dropna()方法来判断和删除缺失值。

（5）数据提取与转换

从原始数据中提取需要的字段，或者进行数据转换。例如，提取酒店位置信息的名称和距离。

（6）数据过滤与排序

对数据进行过滤和排序。例如，根据价格筛选出符合条件的酒店，或者按照评分对酒店进行排序。

3.3 数据分析思路设计
基于Python的穷游网酒店数据采集与分析，可以通过以下数据分析思路来进行综合分析，首先，对酒店数据进行基本统计分析，如计算平均价格、最低价格、最高价格、评分均值等指标。这可以帮助我们了解整体的数据特征，并进行初步的比较和排名。通过绘制直方图或密度图，可以观察酒店价格、评分、评论数量等各项指标的分布情况。可以从中发现异常值或离群点，进一步了解数据的偏态和分散程度。根据酒店所在的城市信息，可以对不同城市的酒店数据进行分组分析。比较各个城市的平均价格、评分分布、酒店数量等指标，找出热门城市和性价比较高的城市，为用户提供更好的参考和选择。将酒店按价格区间进行分类，统计各个价格区间内的酒店数量，使用条形图或饼图来可视化不同价格区间的酒店占比，帮助用户了解酒店价格分布情况。

4 系统实现与结论
4.1 各模块实现
4.1.1 获取数据
通过穷游网API接口爬取指定城市的酒店数据，并将数据保存数据库。通过循环遍历不同城市和不同页数，可以获取更多的酒店信息。具体实现如下：

首先自定义函数pachong，接受三个参数：url表示请求的URL地址，city_0表示城市关键词，ye表示页数。函数中使用for循环遍历页数（从0到ye-1）。在循环内，首先设置请求头(headers)，模拟浏览器发送请求。使用requests.get()方法发送GET请求，将响应结果以JSON格式返回，并赋值给变量res。从JSON响应中提取出酒店信息所在的数据节点soup。再次通过循环遍历soup，获取每个酒店的相关信息，并存储在list_1列表中。最后，调用自定义函数cun(list_1)来保存数据到数据库中。数据采集主要代码如图4.1所示，数据采集过程如图4.2所示。



图4.1 数据采集主要代码



图4.2 数据采集过程

4.1.2 数据存储
首先在本地数据库中创建名为 "穷游网" 的数据库和在该数据库中创建名为 "酒店数据" 的数据表。通过调用chuangku()、chuangbiao()两个函数，可以实现数据库和数据表的创建操作，然后调用cun 的函数，将传入的酒店数据列表 list 中的各个字段值插入到名为 "酒店数据" 的数据表中。通过调用该函数，可以实现将爬取到的酒店数据存入数据库的操作。最后未清洗前保存如数据库的内容如图4.3所示，共有2542条数据。



图4.3 未清洗前的数据

4.1.3 数据清洗与预处理模块实现
主要对数据库中的酒店数据进行清洗处理和计算。具体实现如下：

使用create_engine函数创建了一个数据库引擎，并连接到本地的MySQL数据库 "穷游网"。定义一个SQL查询语句，用于从数据库中获取酒店数据。然后使用pd.read_sql_query函数执行SQL查询，并将结果存储在名为df的DataFrame中。对价格字段进行分箱操作，将价格划分到不同的区间，并统计每个区间内的酒店数量。将分箱结果转换为列表形式，方便后续处理和展示。删除包含空值的行，确保数据的完整性。

按照城市和星级对酒店数据进行分组，并统计每个组别中的酒店数量。从上一步的结果中筛选出酒店数量大于10的组别，并排除不符合需求的组别。然后构建一个城市名称映射字典，使用map（）用于将部分城市名称替换为省份名称。最后使用正则表达式提取字符串中的数值，用于计算酒店的最大距离和最小距离。创建两个新的字段（最大距离和最小距离），并通过正则表达式提取距离数据，删除最小距离字段中的"."值，将最大距离和最小距离字段转换为浮点型数据。数据处理结果如图4.4



图4.4 数据清洗和处理结果

4.1.4 分析模块实现
实现了对穷游网酒店数据的描述性分析、统计和分组操作。首先通过describe()函数对数据进行描述性统计，包括计数、均值、标准差、最小值、25%分位数、50%分位数、75%分位数和最大值等信息的输出。然后利用counts()函数统计每个城市的酒店数量，并通过地图展示了不同城市的酒店数量情况。接着使用groupby()函数按星级进行分组，并计算了平均最小距离，最后通过柱形图展示了各个城市酒店离繁华地区平均最小距离情况。数据分析结果如图4.5所示：



图4.5 数据分析结果

4.1.5 可视化展示
1. 全国主要城市酒店数量

通过统计全国主要城市的酒店数量并进行可视化展示。利用pandas库对数据进行处理和分组，使用pyecharts将城市酒店数量以热力图方式呈现在地图上。通过这种分析和可视化方法，可以直观地了解各个城市的酒店数量分布情况，为旅行规划和市场调研等提供参考。



图4.6 全国主要城市酒店数量分布

从图里可以看出，广东地区酒店数量最多在800-1000家之间。

2. 全国各个城市不同星级酒店数据分布

利用pyecharts库对全国各个城市不同星级酒店数据进行分布柱形图可视化。通过对酒店数据按照城市和星级进行分组统计，并使用pyecharts绘制柱形图，展示了不同星级酒店在各个城市的分布情况。这样的可视化分析能够直观地呈现不同星级酒店的数量和分布情况，为用户提供了更好的了解和选择酒店的参考依据。



图4.7 全国各个城市不同星级酒店数据分布

由图可以看出广州酒店最多，二星酒店上海最多，三星酒店天津最多，四星酒店深圳最多，五星酒店广州最多。

3. 全国主要城市酒店售价区间占比



图4.8 全国主要城市酒店售价区间占比

由图可知，100-250元酒店售价占比最大，其次是价格500以上的。

4. 全国各个城市酒店评分与评论数关系



图4.9 全国各个城市酒店评分与评论数关系

由图可知评分越高的评论数越密集，不过大多数评论数在1000-2000之间。

5. 各个城市酒店离繁华地段最小距离



图4.10 各个城市酒店离繁华地段最小距离

通过对酒店数据提取最小距离字段，并按照城市进行分组统计，然后使用pyecharts绘制柱形图展示了各个城市酒店离繁华地段的最小距离情况。这样的可视化分析能够让用户直观地了解各个城市酒店与繁华地段的距离分布，由图可知大部分城市的最小距离小于1km。

2 结论
通过对穷游网酒店数据采集与分析，可以得出以下结论：

a.全国主要城市酒店数量分析：

根据数据统计，全国主要城市的酒店数量呈现差异性分布。一线及部分二线城市（如北京、上海、广州）的酒店数量较多，而三线及以下城市的酒店数量相对较少。这与城市发展水平、旅游资源和经济实力等因素相关。

b.全国各个城市不同星级酒店数据分布：

各个城市的不同星级酒店数据分布存在差异。一线城市的高星级酒店数量较多，而三线及以下城市则以中低星级酒店为主。这反映了一线城市的经济发展水平和消费能力较高，更多高品质酒店供应；而三线及以下城市则主要满足经济型酒店需求。

c.全国主要城市酒店售价区间占比：

通过对酒店售价进行分析，发现不同价格区间的酒店占比存在差异。高价位区间的酒店数量较少，而中低价位区间的酒店数量较多。这显示了在全国主要城市中，中低价位的经济型酒店更受欢迎，符合大众旅行者的消费需求。

d.全国各个城市酒店评分与评论数关系：

通过对酒店评分与评论数进行关联分析，发现评分较高的酒店通常伴随着较多的评论数。这表明用户对于服务质量好的酒店更愿意进行评价和推荐，而酒店的口碑和评价在用户选择酒店时起着重要作用。

e.各个城市酒店离繁华地段最小距离：

对酒店离繁华地段的最小距离进行分析，可以得出不同城市酒店与繁华地段的关系。一线城市的酒店通常更靠近商业中心和繁华地段，而三线及以下城市的酒店则相对较远。这反映了城市规模和商业发展对酒店分布的影响。。

5 总结
通过基于Python的穷游网酒店数据采集与分析，获得全国主要城市酒店数量、星级分布、售价区间、评分与评论数关系以及酒店距离繁华地段等方面的深入洞察。这些分析结果为旅行者和酒店经营者提供了决策支持和市场调研的依据，同时也展现了Python在数据处理和可视化方面的强大能力。另外穷游网酒店数据采集与分析具有应用领域创新、技术创新和分析角度与维度创新的特点。虽然存在一些未解决的问题或可改进的地方，但该设计仍为旅游行业的酒店市场研究和决策提供了有价值的工具和方法。下面将叙述其创新点和改进之处。

应用领域创新：

该毕业设计在旅游行业的酒店数据分析领域进行了创新。通过对穷游网酒店数据的采集与分析，提供了对全国主要城市酒店数量、星级分布、售价区间、评分与评论数关系以及酒店距离繁华地段等方面的深入洞察，为旅行者和酒店经营者提供了决策支持和市场调研的依据。

技术创新：

在技术层面上，该设计运用了Python编程语言以及相关的数据处理和可视化库，如pandas、numpy和pyecharts等，实现了对酒店数据的采集、清洗、分析和可视化展示。这些技术工具的应用使得数据处理更加高效和自动化。

分析角度与维度创新：

在分析角度和维度上，该设计从全国主要城市酒店数量、星级分布、售价区间、评分与评论数关系以及酒店距离繁华地段等多个方面进行了深入分析。通过对这些维度的研究，可以帮助用户更好地了解酒店市场、用户需求和消费者行为，从而制定合理的商业策略和旅行规划。

在数据分析过程中，可能存在一些未解决的问题或可改进的地方。例如：

数据质量问题：穷游网酒店数据的准确性和完整性可能存在一定的问题，如缺失值、错误的分类标签等，这可能会对分析结果产生一定的影响。在数据采集和清洗阶段，可以进一步加强数据验证和清洗工作，提高数据质量。

缺乏更多维度的分析：虽然该设计涵盖了城市数量、星级、价格、评分等多个维度，但仍有其他因素可能对酒店市场产生影响，比如季节性变化、特殊事件等。探索更多维度的分析可能对深入理解市场情况和用户行为提供更全面的视角。

可视化展示的多样性：在可视化展示方面，除了使用柱形图、折线图和热力图等常见方式外，还可以尝试其他图表类型或交互式可视化方式，以更好地呈现数据分析结果，提供更丰富的用户体验。

简单来说就是获取数据——>整理数据——>进行分析——>数据报告这几个环节。接下来和黑马程序员一起看看，Python在这些环节中能做什么。获取数据网络爬虫：使用第三方库（如BeautifulSoup、Scrapy）来爬取网页数据，通过HTTP请求获取网页内容，并提取需要的数据。数据库连接：使用库（如pymysql、psycopg2）连接数据库，并执行查询语句来获取数据。文件读取：使用Pandas库中的read_csv、read_excel等函数读取CSV、Excel等文件格式中的数据。数据整理数据清洗：使用Pandas库进行数据清洗和预处理，如处理缺失值（fillna）、处理重复值（drop_duplicates）、数据转换（apply）等。数据转换：使用Pandas库进行数据类型转换（astype）、日期时间处理（to_datetime）、字符串操作（str函数）等。数据分析数据探索性分析：使用Pandas和NumPy库进行描述性统计分析（describe）、频率统计（value_counts）、数据筛选和分组聚合（groupby）等。数据可视化：使用Matplotlib和Seaborn库绘制各种图表，如折线图、柱状图、散点图、箱线图等，来展示数据的分布、关联性和趋势。统计分析：使用Statsmodels和Scipy库进行统计模型拟合、假设检验、方差分析等统计分析任务。数据报告Jupyter Notebook：使用Jupyter Notebook结合Markdown文本和代码，编写数据分析报告，将数据、分析过程和结果整合在一起。数据可视化库：使用Matplotlib、Seaborn和Plotly等库创建图表和可视化，以呈现分析结果。文档处理库：使用Python的文本处理库（如Pandas、NLTK）来处理和分析文本数据，支持自然语言处理任务和文本挖掘。01 生成数据表
第一部分是生成数据表，常见的生成方法有两种，第一种是导入外部数据，第二种是直接写入数据。 Excel 中的文件菜单中提供了获取外部数据的功能，支持数据库和文本文件和页面的多种数据源导入。



python 支持从多种类型的数据导入。在开始使用 python 进行数据导入前需要先导入 pandas 库，为了方便起见，我们也同时导入 numpy 库。

1import numpy as np
2import pandas as pd

导入数据表
下面分别是从 excel 和 csv 格式文件导入数据并创建数据表的方法。代码是最简模式，里面有很多可选参数设置，例如列名称，索引列，数据格式等等。感兴趣的朋友可以参考 pandas 的
官方文档。

1df=pd.DataFrame(pd.read_csv('name.csv',header=1))
2df=pd.DataFrame(pd.read_excel('name.xlsx'))

创建数据表
另一种方法是通过直接写入数据来生成数据表，excel 中直接在单元格中输入数据就可以，python 中通过下面的代码来实现。生成数据表的函数是 pandas 库中的 DateFrame 函数，数据表一共有 6 行数据，每行有 6 个字段。在数据中我们特意设置了一些 NA 值和有问题的字段，例如包含空格等。后面将在数据清洗步骤进行处理。后面我们将统一以 DataFrame 的简称 df 来命名数据表。

1df = pd.DataFrame({"id":[1001,1002,1003,1004,1005,1006],
2                   "date":pd.date_range('20130102', periods=6),
3                   "city":['Beijing ', 'SH', ' guangzhou ', 'Shenzhen', 'shanghai', 'BEIJING '],
4                   "age":[23,44,54,32,34,32],
5                   "category":['100-A','100-B','110-A','110-C','210-A','130-F'],
6                   "price":[1200,np.nan,2133,5433,np.nan,4432]},
7                   columns =['id','date','city','category','age','price'])

这是刚刚创建的数据表，我们没有设置索引列，price 字段中包含有 NA 值，city 字段中还包含了一些脏数据。



02 数据表检查
第二部分是对数据表进行检查，python 中处理的数据量通常会比较大，比如我们之前的文章中介绍的纽约出租车数据和 Citibike 的骑行数据，数据量都在千万级，我们无法一目了然的 了解数据表的整体情况，必须要通过一些方法来获得数据表的关键信息。数据表检查的另一个目的是了解数据的概况，例如整个数据表的大小，所占空间，数据格式，是否有空值和重复项和具体的数据内容。为后面的清洗和预处理做好准备。

数据维度(行列)
Excel 中可以通过 CTRL+向下的光标键，和 CTRL+向右的光标键来查看行号和列号。Python 中使用 shape 函数来查看数据表的维度，也就是行数和列数，函数返回的结果(6,6)表示数据表有 6 行，6 列。下面是具体的代码。

1#查看数据表的维度
2df.shape
3(6, 6)

数据表信息
使用 info 函数查看数据表的整体信息，这里返回的信息比较多，包括数据维度，列名称，数据格式和所占空间等信息。

#数据表信息
df.info()

&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 6 entries, 0 to 5
Data columns (total 6 columns):
id          6 non-null int64
date        6 non-null datetime64[ns]
city        6 non-null object
category    6 non-null object
age         6 non-null int64
price       4 non-null float64
dtypes: datetime64[ns](1), float64(1), int64(2), object(2memory usage: 368.0+ bytes

查看数据格式
Excel 中通过选中单元格并查看开始菜单中的数值类型来判断数据的格式。Python 中使用 dtypes 函数来返回数据格式。



Dtypes 是一个查看数据格式的函数，可以一次性查看数据表中所有数据的格式，也可以指定一列来单独查看。

#查看数据表各列格式
df.dtypes

id                   int64
date        datetime64[ns]
city                object
category            object
age                  int64
price              float64
dtype: object
#查看单列格式
df['B'].dtype

dtype('int64')

查看空值
Excel 中查看空值的方法是使用“定位条件”功能对数据表中的空值进行定位。“定位条件”在“开始”目录下的“查找和选择”目录中。



Isnull 是 Python 中检验空值的函数，返回的结果是逻辑值，包含空值返回 True，不包含则返回 False。可以对整个数据表进行检查，也可以单独对某一列进行空值检查。

#检查数据空值
df.isnull()

#检查特定列空值
df['price'].isnull()

0    False
1     True
2    False
3    False
4     True
5    False
Name: price, dtype: bool

查看唯一值
Excel 中查看唯一值的方法是使用“条件格式”对唯一值进行颜色标记。Python 中使用 unique 函数查看唯一值。



Unique 是查看唯一值的函数，只能对数据表中的特定列进行检查。下面是代码，返回的结果是该列中的唯一值。类似与 Excel 中删除重复项后的结果。

#查看 city 列中的唯一值
df['city'].unique()

array(['Beijing ', 'SH', ' guangzhou ', 'Shenzhen', 'shanghai', 'BEIJING '], dtype=object)

查看数据表数值
Python 中的 Values 函数用来查看数据表中的数值。以数组的形式返回，不包含表头信息。

#查看数据表的值
df.values

array([[1001, Timestamp('2013-01-02 00:00:00'), 'Beijing ', '100-A', 23,
        1200.0],
       [1002, Timestamp('2013-01-03 00:00:00'), 'SH', '100-B', 44, nan],
       [1003, Timestamp('2013-01-04 00:00:00'), ' guangzhou ', '110-A', 54,
        2133.0],
       [1004, Timestamp('2013-01-05 00:00:00'), 'Shenzhen', '110-C', 32,
        5433.0],
      [1005, Timestamp('2013-01-06 00:00:00'), 'shanghai', '210-A', 34,
        nan],
      [1006, Timestamp('2013-01-07 00:00:00'), 'BEIJING ', '130-F', 32,
        4432.0]], dtype=object)

查看列名称
Colums 函数用来单独查看数据表中的列名称。

#查看列名称
df.columns

Index(['id', 'date', 'city', 'category', 'age', 'price'], dtype='object')

查看前 10 行数据
Head 函数用来查看数据表中的前 N 行数据，默认 head()显示前 10 行数据，可以自己设置参数值来确定查看的行数。下面的代码中设置查看前 3 行的数据。

`#查看前 3 行数据``df.head(``3``)`

查看后 10 行数据
Tail 行数与 head 函数相反，用来查看数据表中后 N 行的数据，默认 tail()显示后 10 行数据，可以自己设置参数值来确定查看的行数。下面的代码中设置查看后 3 行的数据。

`#查看最后 3 行``df.tail(``3``)`

03 数据表清洗
第三部分是对数据表中的问题进行清洗。主要内容包括对空值，大小写问题，数据格式和重复值的处理。这里不包含对数据间的逻辑验证。

处理空值(删除或填充)
我们在创建数据表的时候在 price 字段中故意设置了几个 NA 值。对于空值的处理方式有很多种，可以直接删除包含空值的数据，也可以对空值进行填充，比如用 0 填充或者用均值填充。还可以根据不同字段的逻辑对空值进行推算。

Excel 中可以通过“查找和替换”功能对空值进行处理，将空值统一替换为 0 或均值。也可以通过“定位”空值来实现。



Python 中处理空值的方法比较灵活，可以使用 Dropna 函数用来删除数据表中包含空值的数据，也可以使用 fillna 函数对空值进行填充。下面的代码和结果中可以看到使用 dropna 函数后，包含 NA 值的两个字段已经不见了。返回的是一个不包含空值的数据表。

#删除数据表中含有空值的行
df.dropna(how='any')

除此之外也可以使用数字对空值进行填充，下面的代码使用 fillna 函数对空值字段填充数字 0。

#使用数字 0 填充数据表中空值
df.fillna(value=0)

我们选择填充的方式来处理空值，使用 price 列的均值来填充 NA 字段，同样使用 fillna 函数，在要填充的数值中使用 mean 函数先计算 price 列当前的均值，然后使用这个均值对 NA 进行填
充。可以看到两个空值字段显示为 3299.5

 #使用 price 均值对 NA 进行填充
 df['price'].fillna(df['price'].mean())
 
 0    1200.0
 1    3299.5
 2    2133.0
 3    5433.0
 4    3299.5
 5    4432.0
Name: price, dtype: float64

清理空格
除了空值，字符中的空格也是数据清洗中一个常见的问题，下面是清除字符中空格的代码。

#清除 city 字段中的字符空格
df['city']=df['city'].map(str.strip)

大小写转换
在英文字段中，字母的大小写不统一也是一个常见的问题。Excel 中有 UPPER，LOWER 等函数，python 中也有同名函数用来解决大小写的问题。在数据表的 city 列中就存在这样的问题。我们将 city 列的所有字母转换为小写。下面是具体的代码和结果。

#city 列大小写转换
df['city']=df['city'].str.lower()

更改数据格式
Excel 中通过“设置单元格格式”功能可以修改数据格式。Python 中通过 astype 函数用来修改数据格式。

Python 中 dtype 是查看数据格式的函数，与之对应的是 astype 函数，用来更改数据格式。下面的代码中将 price 字段的值修改为 int 格式。

 #更改数据格式
 df['price'].astype('int')
 
 0    1200
 1    3299
 2    2133
 3    5433
 4    3299
 5    4432
Name: price, dtype: int32

更改列名称
Rename 是更改列名称的函数，我们将来数据表中的 category 列更改为 category-size。下面是具体的代码和更改后的结果。

#更改列名称
df.rename(columns={'category': 'category-size'})

删除重复值
很多数据表中还包含重复值的问题，Excel 的数据目录下有“删除重复项”的功能，可以用来删除数据表中的重复值。默认 Excel 会保留最先出现的数据，删除后面重复出现的数据。



Python 中使用 drop_duplicates 函数删除重复值。我们以数据表中的 city 列为例，city 字段中存在重复值。默认情况下 drop_duplicates()将删除后出现的重复值(与 excel 逻辑一致)。增加 keep=’last’参数后将删除最先出现的重复值，保留最后的值。下面是具体的代码和比较结果。

原始的 city 列中 beijing 存在重复，分别在第一位和最后一位。

df['city']
0      beijing
1           sh
2    guangzhou
3     shenzhen
4     shanghai
5      beijing
Name: city, dtype: object

使用默认的 drop_duplicates()函数删除重复值，从结果中可以看到第一位的 beijing 被保留，最后出现的 beijing 被删除。

#删除后出现的重复值
df['city'].drop_duplicates()
0      beijing
1           sh
2    guangzhou
3     shenzhen
4     shanghai
Name: city, dtype: object

设置 keep=’last‘’参数后，与之前删除重复值的结果相反，第一位出现的 beijing 被删除，保留了最后一位出现的 beijing。

#删除先出现的重复值
df['city'].drop_duplicates(keep='last')
1           sh
2    guangzhou
3     shenzhen
4     shanghai
5      beijing
Name: city, dtype: objec

数值修改及替换
数据清洗中最后一个问题是数值修改或替换，Excel 中使用“查找和替换”功能就可以实现数值的替换。



Python 中使用 replace 函数实现数据替换。数据表中 city 字段上海存在两种写法，分别为 shanghai 和 SH。我们使用 replace 函数对 SH 进行替换。

1#数据替换
2df['city'].replace('sh', 'shanghai')
30      beijing
41     shanghai
52    guangzhou
63     shenzhen
74     shanghai
85      beijing
9Name: city, dtype: object

本篇文章这是系列的第二篇，介绍第 4-6 部分的内容，数据表生成，数据表查看，和数据清洗。



04 数据预处理
第四部分是数据的预处理，对清洗完的数据进行整理以便后期的统计和分析工作。主要包括数据表的合并，排序，数值分列，数据分
组及标记等工作。

数据表合并
首先是对不同的数据表进行合并，我们这里创建一个新的数据表 df1，并将 df 和 df1 两个数据表进行合并。在 Excel 中没有直接完成数据表合并的功能，可以通过 VLOOKUP 函数分步实现。在 python 中可以通过 merge 函数一次性实现。下面建立 df1 数据表，用于和 df 数据表进行合并。

1#创建 df1 数据表
2df1=pd.DataFrame({"id":[1001,1002,1003,1004,1005,1006,1007,1008],
3"gender":['male','female','male','female','male','female','male','female'],
4"pay":['Y','N','Y','Y','N','Y','N','Y',],
5"m-point":[10,12,20,40,40,40,30,20]})

使用 merge 函数对两个数据表进行合并，合并的方式为 inner，将两个数据表中共有的数据匹配到一起生成新的数据表。并命名为 df_inner。

1#数据表匹配合并，inner 模式
2df_inner=pd.merge(df,df1,how='inner')

除了 inner 方式以外，合并的方式还有 left，right 和 outer 方式。这几种方式的差别在我其他的文章中有详细的说明和对比。

1#其他数据表匹配模式
2df_left=pd.merge(df,df1,how='left')
3df_right=pd.merge(df,df1,how='right')
4df_outer=pd.merge(df,df1,how='outer')

设置索引列
完成数据表的合并后，我们对 df_inner 数据表设置索引列，索引列的功能很多，可以进行数据提取，汇总，也可以进行数据筛选等。
设置索引的函数为 set_index。

1#设置索引列
2df_inner.set_index('id')

排序(按索引，按数值)
Excel 中可以通过数据目录下的排序按钮直接对数据表进行排序，比较简单。Python 中需要使用 ort_values 函数和 sort_index 函数完成排序。



在 python 中，既可以按索引对数据表进行排序，也可以看制定列的数值进行排序。首先我们按 age 列中用户的年龄对数据表进行排序。
使用的函数为 sort_values。

1#按特定列的值排序
2df_inner.sort_values(by=['age'])

Sort_index 函数用来将数据表按索引列的值进行排序。

1#按索引列排序
2df_inner.sort_index()

数据分组
Excel 中可以通过 VLOOKUP 函数进行近似匹配来完成对数值的分组，或者使用“数据透视表”来完成分组。相应的 python 中使用 where 函数完成数据分组。

Where 函数用来对数据进行判断和分组，下面的代码中我们对 price 列的值进行判断，将符合条件的分为一组，不符合条件的分为另一组，并使用 group 字段进行标记。

1#如果 price 列的值>3000，group 列显示 high，否则显示 low
2df_inner['group'] = np.where(df_inner['price'] > 3000,'high','low')

除了 where 函数以外，还可以对多个字段的值进行判断后对数据进行分组，下面的代码中对 city 列等于 beijing 并且 price 列大于等于 4000 的数据标记为 1。

1#对复合多个条件的数据进行分组标记
2df_inner.loc[(df_inner['city'] == 'beijing') & (df_inner['price'] >= 4000), 'sign']=1


数据分列
与数据分组相反的是对数值进行分列，Excel 中的数据目录下提供“分列”功能。在 python 中使用 split 函数实现分列。



在数据表中 category 列中的数据包含有两个信息，前面的数字为类别 id，后面的字母为 size 值。中间以连字符进行连接。我们使用 split 函数对这个字段进行拆分，并将拆分后的数据表匹配回原数据表中。

1#对 category 字段的值依次进行分列，并创建数据表，索引值为 df_inner 的索引列，列名称为 category 和 size
2pd.DataFrame((x.split('-') for x in df_inner['category']),index=df_inner.index,columns=['category','size'])


1#将完成分列后的数据表与原 df_inner 数据表进行匹配
2df_inner=pd.merge(df_inner,split,right_index=True, left_index=True)


05 数据提取
第五部分是数据提取，也是数据分析中最常见的一个工作。这部分主要使用三个函数，loc，iloc 和 ix，loc 函数按标签值进行提取，iloc 按位置进行提取，ix 可以同时按标签和位置进行提取。下面介绍每一种函数的使用方法。

按标签提取(loc)
Loc 函数按数据表的索引标签进行提取，下面的代码中提取了索引列为 3 的单条数据。

 1#按索引提取单行的数值
 2df_inner.loc[3]
 3id 1004
 4date 2013-01-05 00:00:00
 5city shenzhen
 6category 110-C
 7age 32
 8price 5433
 9gender female
10m-point 40
11pay Y
12group high
13sign NaN
14category_1 110
15size C
16Name: 3, dtype: object



使用冒号可以限定提取数据的范围，冒号前面为开始的标签值，后面为结束的标签值。下面提取了 0 到 5 的数据行。

1#按索引提取区域行数值
2df_inner.loc[0:5]


Reset_index 函数用于恢复索引，这里我们重新将 date 字段的日期设置为数据表的索引，并按日期进行数据提取。

1#重设索引
2df_inner.reset_index()


1#设置日期为索引
2df_inner=df_inner.set_index('date')


使用冒号限定提取数据的范围，冒号前面为空表示从 0 开始。提取所有 2013 年 1 月 4 日以前的数据。

1#提取 4 日之前的所有数据
2df_inner[:'2013-01-04']


按位置提取(iloc)
使用 iloc 函数按位置对数据表中的数据进行提取，这里冒号前后的数字不再是索引的标签名称，而是数据所在的位置，从 0 开始。

1#使用 iloc 按位置区域提取数据
2df_inner.iloc[:3,:2]

iloc 函数除了可以按区域提取数据，还可以按位置逐条提取，前面方括号中的 0,2,5 表示数据所在行的位置，后面方括号中的数表示所在列的位置。

1#使用 iloc 按位置单独提取数据
2df_inner.iloc[[0,2,5],[4,5]]

按标签和位置提取（ix）
ix 是 loc 和 iloc 的混合，既能按索引标签提取，也能按位置进行数据提取。下面代码中行的位置按索引日期设置，列按位置设置。

1#使用 ix 按索引标签和位置混合提取数据
2df_inner.ix[:'2013-01-03',:4]

按条件提取（区域和条件值）
除了按标签和位置提起数据以外，还可以按具体的条件进行数据。下面使用 loc 和 isin 两个函数配合使用，按指定条件对数据进行提取 。

使用 isin 函数对 city 中的值是否为 beijing 进行判断。

 1#判断 city 列的值是否为 beijing
 2df_inner['city'].isin(['beijing'])
 3
 4date
 52013-01-02 True
 62013-01-05 False
 72013-01-07 True
 82013-01-06 False
 92013-01-03 False
102013-01-04 False
11Name: city, dtype: bool

将 isin 函数嵌套到 loc 的数据提取函数中，将判断结果为 Ture 数据提取出来。这里我们把判断条件改为 city 值是否为 beijing 和 shanghai。如果是就把这条数据提取出来。

1#先判断 city 列里是否包含 beijing 和 shanghai，然后将复合条件的数据提取出来。
2df_inner.loc[df_inner['city'].isin(['beijing','shanghai'])]

数值提取还可以完成类似数据分列的工作，从合并的数值中提取出制定的数值。

 1category=df_inner['category']
 20 100-A
 33 110-C
 45 130-F
 54 210-A
 61 100-B
 72 110-A
 8Name: category, dtype: object
 9
10#提取前三个字符，并生成数据表
11pd.DataFrame(category.str[:3])

06 数据筛选
第六部分为数据筛选，使用与，或，非三个条件配合大于，小于和等于对数据进行筛选，并进行计数和求和。与 excel 中的筛选功能和 countifs 和 sumifs 功能相似。

按条件筛选（与，或，非）
Excel 数据目录下提供了“筛选”功能，用于对数据表按不同的条件进行筛选。Python 中使用 loc 函数配合筛选条件来完成筛选功能。配合 sum 和 count 函数还能实现 excel 中 sumif 和 countif 函数的功能。



使用“与”条件进行筛选，条件是年龄大于 25 岁，并且城市为 beijing。筛选后只有一条数据符合要求。

1#使用“与”条件进行筛选
2df_inner.loc[(df_inner['age'] > 25) & (df_inner['city'] == 'beijing'), ['id','city','age','category','gender']]


使用“或”条件进行筛选，年龄大于 25 岁或城市为 beijing。筛选后有 6 条数据符合要求。

1#使用“或”条件筛选
2df_inner.loc[(df_inner['age'] > 25) | (df_inner['city'] == 'beijing'), ['id','city','age','category','gender']].sort
3(['age'])


在前面的代码后增加 price 字段以及 sum 函数，按筛选后的结果将 price 字段值进行求和，相当于 excel 中 sumifs 的功能。

1#对筛选后的数据按 price 字段进行求和
2df_inner.loc[(df_inner['age'] > 25) | (df_inner['city'] == 'beijing'),
3['id','city','age','category','gender','price']].sort(['age']).price.sum()
4
519796

使用“非”条件进行筛选，城市不等于 beijing。符合条件的数据有 4 条。将筛选结果按 id 列进行排序。

1#使用“非”条件进行筛选
2df_inner.loc[(df_inner['city'] != 'beijing'), ['id','city','age','category','gender']].sort(['id'])

在前面的代码后面增加 city 列，并使用 count 函数进行计数。相当于 excel 中的 countifs 函数的功能。

1#对筛选后的数据按 city 列进行计数
2df_inner.loc[(df_inner['city'] != 'beijing'), ['id','city','age','category','gender']].sort(['id']).city.count()
34

还有一种筛选的方式是用 query 函数。下面是具体的代码和筛选结果。

1#使用 query 函数进行筛选
2df_inner.query('city == ["beijing", "shanghai"]')


在前面的代码后增加 price 字段和 sum 函数。对筛选后的 price 字段进行求和，相当于 excel 中的 sumifs 函数的功能。

1#对筛选后的结果按 price 进行求和
2df_inner.query('city == ["beijing", "shanghai"]').price.sum()
312230

这是第三篇，介绍第 7-9 部分的内容，数据汇总，数据统计，和数据输出。



07 数据汇总
第七部分是对数据进行分类汇总，Excel 中使用分类汇总和数据透视可以按特定维度对数据进行汇总，python 中使用的主要函数是 groupby 和 pivot_table。下面分别介绍这两个函数的使用方法。

分类汇总
Excel 的数据目录下提供了“分类汇总”功能，可以按指定的字段和汇总方式对数据表进行汇总。Python 中通过 Groupby 函数完成相应的操作，并可以支持多级分类汇总。



Groupby 是进行分类汇总的函数，使用方法很简单，制定要分组的列名称就可以，也可以同时制定多个列名称，groupby 按列名称出现的顺序进行分组。同时要制定分组后的汇总方式，常见的是计数和求和两种。

1#对所有列进行计数汇总
2df_inner.groupby('city').count()

可以在 groupby 中设置列名称来对特定的列进行汇总。下面的代码中按城市对 id 字段进行汇总计数。

1#对特定的 ID 列进行计数汇总
2df_inner.groupby('city')['id'].count()
3city
4beijing 2
5guangzhou 1
6shanghai 2
7shenzhen 1
8Name: id, dtype: int64

在前面的基础上增加第二个列名称，分布对 city 和 size 两个字段进行计数汇总。

 1#对两个字段进行汇总计数
 2df_inner.groupby(['city','size'])['id'].count()
 3city size
 4beijing A 1
 5F 1
 6guangzhou A 1
 7shanghai A 1
 8B 1
 9shenzhen C 1
10Name: id, dtype: int64

除了计数和求和外，还可以对汇总后的数据同时按多个维度进行计算，下面的代码中按城市对 price 字段进行汇总，并分别计算 price 的数量，总金额和平均金额。

1#对 city 字段进行汇总并计算 price 的合计和均值。
2df_inner.groupby('city')['price'].agg([len,np.sum, np.mean])

数据透视
Excel 中的插入目录下提供“数据透视表”功能对数据表按特定维度进行汇总。Python 中也提供了数据透视表功能。通过 pivot_table 函数实现同样的效果。



数据透视表也是常用的一种数据分类汇总方式，并且功能上比 groupby 要强大一些。下面的代码中设定 city 为行字段，size 为列字段，price 为值字段。分别计算 price 的数量和金额并且按行与列进行汇总。

1#数据透视表
2pd.pivot_table(df_inner,index=["city"],values=["price"],columns=["size"],aggfunc=[len,np.sum],fill_value=0,margins=True)

08 数据统计
第九部分为数据统计，这里主要介绍数据采样，标准差，协方差和相关系数的使用方法。

数据采样
Excel 的数据分析功能中提供了数据抽样的功能，如下图所示。Python 通过 sample 函数完成数据采样。



Sample 是进行数据采样的函数，设置 n 的数量就可以了。函数自动返回参与的结果。

1#简单的数据采样
2df_inner.sample(n=3)

Weights 参数是采样的权重，通过设置不同的权重可以更改采样的结果，权重高的数据将更有希望被选中。这里手动设置 6 条数据的权重值。将前面 4 个设置为 0，后面两个分别设置为 0.5。

1#手动设置采样权重
2weights = [0, 0, 0, 0, 0.5, 0.5]
3df_inner.sample(n=2, weights=weights)

从采样结果中可以看出，后两条权重高的数据被选中。



Sample 函数中还有一个参数 replace，用来设置采样后是否放回。

1#采样后不放回
2df_inner.sample(n=6, replace=False)

1#采样后放回
2df_inner.sample(n=6, replace=True)

描述统计
Excel 中的数据分析中提供了描述统计的功能。Python 中可以通过 Describe 对数据进行描述统计。



Describe 函数是进行描述统计的函数，自动生成数据的数量，均值，标准差等数据。下面的代码中对数据表进行描述统计，并使用 round 函数设置结果显示的小数位。并对结果数据进行转置。

1#数据表描述性统计
2df_inner.describe().round(2).T


标准差
Python 中的 Std 函数用来接算特定数据列的标准差。

1#标准差
2df_inner['price'].std()
31523.3516556155596

协方差
Excel 中的数据分析功能中提供协方差的计算，python 中通过 cov 函数计算两个字段或数据表中各字段间的协方差。



Cov 函数用来计算两个字段间的协方差，可以只对特定字段进行计算，也可以对整个数据表中各个列之间进行计算。

1#两个字段间的协方差
2df_inner['price'].cov(df_inner['m-point'])
317263.200000000001

相关分析
Excel 的数据分析功能中提供了相关系数的计算功能，python 中则通过 corr 函数完成相关分析的操作，并返回相关系数。



Corr 函数用来计算数据间的相关系数，可以单独对特定数据进行计算，也可以对整个数据表中各个列进行计算。相关系数在-1 到 1 之间，接近 1 为正相关，接近-1 为负相关，0 为不相关。

1#相关性分析
2df_inner['price'].corr(df_inner['m-point'])
30.77466555617085264
4
5#数据表相关性分析
6df_inner.corr()

09 数据输出
第九部分是数据输出，处理和分析完的数据可以输出为 xlsx 格式和 csv 格式。

写入 excel

1#输出到 excel 格式
2df_inner.to_excel('excel_to_python.xlsx', sheet_name='bluewhale_cc')


写入 csv

1#输出到 CSV 格式
2df_inner.to_csv('excel_to_python.csv')

在数据处理的过程中，大部分基础工作是重复和机械的，对于这部分基础工作，我们可以使用自定义函数进行自动化。以下简单介绍对数据表信息获取自动化处理。

 1#创建数据表
 2df = pd.DataFrame({"id":[1001,1002,1003,1004,1005,1006],
 3"date":pd.date_range('20130102', periods=6),
 4"city":['Beijing ', 'SH', ' guangzhou ', 'Shenzhen', 'shanghai', 'BEIJING '],
 5"age":[23,44,54,32,34,32],
 6"category":['100-A','100-B','110-A','110-C','210-A','130-F'],
 7"price":[1200,np.nan,2133,5433,np.nan,4432]},
 8columns =['id','date','city','category','age','price'])
 9
10#创建自定义函数
11def table_info(x):
12    shape=x.shape
13    types=x.dtypes
14    colums=x.columns
15    print("数据维度(行，列):\n",shape)
16    print("数据格式:\n",types)
17    print("列名称:\n",colums)
18
19#调用自定义函数获取 df 数据表信息并输出结果
20table_info(df)
21
22数据维度(行，列):
23(6, 6)
24数据格式:
25id int64
26date datetime64[ns]
27city object
28category object
29age int64
30price float64
31dtype: object
32列名称:
33Index(['id', 'date', 'city', 'category', 'age', 'price'], dtype='object')


Python 在数据分析领域有着非常强大的生态和灵活性，从数据清洗、处理，到分析、可视化，它几乎无所不能。今天，我们一起来聊聊如何用 Python 处理、分析和展示数据，并用几个实际案例让你快速上手。

一、数据分析的流程
数据分析其实是一个完整的闭环，可以分为以下几个步骤：

获取数据：从文件、数据库、API 或网络爬虫中收集数据。
数据清洗：处理缺失值、异常值，转换数据格式。
数据分析：统计、分类、聚类、相关性分析等。
数据可视化：用图表清晰地表达数据特征。
报告与决策：通过可视化和分析结果为业务提供决策支持。
二、Python 数据分析的常用工具
要用 Python 处理数据，离不开以下几个主流工具：

Pandas：数据处理与操作的基础工具，提供 DataFrame 结构，方便清洗和分析数据。
NumPy：高效的数组计算库，用于数学计算和数据处理。
Matplotlib 和 Seaborn：常用的可视化工具，适合绘制各种统计图表。
Plotly 和 Bokeh：交互式可视化工具，适合动态展示数据。
Scikit-learn：常用于机器学习和高级分析的库。
三、案例 1：读取与清洗数据
假设我们有一个包含产品销售数据的 CSV 文件，名为 sales_data.csv，其中包含以下字段：

产品 ID（product_id）
销售日期（sale_date）
销售额（revenue）
城市（city）
我们的目标是：

读取数据；
清洗缺失值；
格式化日期字段。
import pandas as pd

# 读取 CSV 文件
df = pd.read_csv("sales_data.csv")

# 查看数据前几行
print(df.head())

# 检查缺失值
print(df.isnull().sum())

# 填充缺失值
df['revenue'] = df['revenue'].fillna(df['revenue'].mean())

# 转换日期格式
df['sale_date'] = pd.to_datetime(df['sale_date'])

print(df.info())  # 检查数据清洗后的结果

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
四、案例 2：数据分析
在数据清洗后，我们想分析不同城市的销售情况以及总体趋势：

按城市统计总销售额：
# 按城市分组统计销售额
city_revenue = df.groupby('city')['revenue'].sum().reset_index()

print(city_revenue)
1
2
3
4
按月统计销售额趋势：
# 添加月份字段
df['month'] = df['sale_date'].dt.to_period('M')

# 按月份分组统计
monthly_trend = df.groupby('month')['revenue'].sum().reset_index()

print(monthly_trend)
1
2
3
4
5
6
7
五、案例 3：数据可视化
数据分析得出的结论如果不能清晰地展示给他人，很难为决策提供依据。接下来，我们使用 Matplotlib 和 Seaborn 绘制几种常用图表。

柱状图：城市销售额对比
import matplotlib.pyplot as plt

# 绘制柱状图
plt.bar(city_revenue['city'], city_revenue['revenue'], color='skyblue')
plt.title("城市销售额对比")
plt.xlabel("城市")
plt.ylabel("总销售额")
plt.xticks(rotation=45)
plt.show()
1
2
3
4
5
6
7
8
9
折线图：月度销售趋势
# 绘制折线图
plt.plot(monthly_trend['month'].astype(str), monthly_trend['revenue'], marker='o', color='green')
plt.title("月度销售额趋势")
plt.xlabel("月份")
plt.ylabel("总销售额")
plt.grid()
plt.show()
1
2
3
4
5
6
7
分布图：销售额分布
import seaborn as sns

# 使用 Seaborn 绘制分布图
sns.histplot(df['revenue'], kde=True, color='purple')
plt.title("销售额分布图")
plt.xlabel("销售额")
plt.ylabel("频率")
plt.show()
1
2
3
4
5
6
7
8
六、案例 4：交互式可视化
静态图表有时候不足以直观展示数据的动态关系，这时我们可以使用 Plotly 来创建交互式图表。

城市销售额饼图
import plotly.express as px

# 绘制饼图
fig = px.pie(city_revenue, names='city', values='revenue', title="城市销售额占比")
fig.show()
1
2
3
4
5
销售趋势动态折线图
# 动态折线图
fig = px.line(monthly_trend, x='month', y='revenue', title="月度销售趋势", markers=True)
fig.show()
1
2
3
七、综合案例：将分析结果存储到数据库
分析数据后，我们可能需要将结果保存到数据库供后续使用。这里用 SQLite 做示例。

连接 SQLite 数据库并存储结果：
import sqlite3

# 创建数据库连接
conn = sqlite3.connect('sales_analysis.db')

# 存储城市销售数据
city_revenue.to_sql('city_revenue', conn, if_exists='replace', index=False)

# 存储月度趋势数据
monthly_trend.to_sql('monthly_trend', conn, if_exists='replace', index=False)

conn.close()
1
2
3
4
5
6
7
8
9
10
11
12
从数据库读取数据用于后续分析：
# 读取数据库中的数据
conn = sqlite3.connect('sales_analysis.db')
data = pd.read_sql('SELECT * FROM city_revenue', conn)
print(data)
conn.close()
1
2
3
4
5
八、提升数据可视化与分析的效率
数据清洗工具：使用 Pandas 的 apply 方法批量处理字段，提升清洗效率；
自动化分析：结合 Jupyter Notebook 和模板化代码，快速生成分析报告；
更美观的可视化：试试 Seaborn 的主题设置（sns.set_theme()）或 Plotly 的自定义样式；
分布式处理：当数据量很大时，可以使用 Dask 或 PySpark 等工具加速分析。
九、总结
通过 Python，我们可以完成从数据获取、清洗、分析到可视化的全流程操作。掌握这些技能后，你不仅可以高效地完成业务分析，还能通过清晰的图表讲述数据背后的故事

在当今大数据时代，数据挖掘成为解锁隐藏信息、指导决策的关键技能。Python，凭借其强大的库支持和易学性，已成为数据科学家的首选语言。本文将通过一个实战案例，展示如何使用Python进行数据预处理、分析及可视化，让你领略数据背后的秘密。

环境准备
首先，确保你的环境中安装了Python以及以下库：pandas, numpy, matplotlib, 和 seaborn。可以通过pip安装这些库：

pip install pandas numpy matplotlib seaborn
获取数据
我们将使用一个虚构的电商销售数据集作为分析对象。假设你已经有一个名为sales_data.csv的数据文件，包含顾客ID、购买日期、商品类别、销售额等字段。

数据读取与预处理
读取数据
使用Pandas库读取CSV文件：

import pandas as pd

# 读取数据
data = pd.read_csv('sales_data.csv')

# 查看数据前几行
print(data.head())
数据清洗
通常数据中会存在缺失值或异常值，需要进行处理。这里我们简单演示如何检查并填充缺失值：

# 检查缺失值
print(data.isnull().sum())

# 填充缺失的销售额为该商品类别的平均值
data['Sales'] = data.groupby('Product_Category')['Sales'].transform(lambda x: x.fillna(x.mean()))
数据分析
销售额随时间变化
我们分析销售额随着时间的变化趋势：

import matplotlib.pyplot as plt

# 将购买日期转换为日期类型
data['Purchase_Date'] = pd.to_datetime(data['Purchase_Date'])

# 按月分组，计算每月销售额
monthly_sales = data.groupby(data['Purchase_Date'].dt.to_period('M'))['Sales'].sum()

# 绘制销售额随时间变化的折线图
plt.figure(figsize=(10,6))
monthly_sales.plot()
plt.title('Monthly Sales Trend')
plt.xlabel('Month')
plt.ylabel('Sales ($)')
plt.show()
商品类别销售分析
接下来，我们分析不同商品类别的销售情况：

import seaborn as sns

# 绘制各商品类别销售额的条形图
sns.set(style="whitegrid")
category_sales = data.groupby('Product_Category')['Sales'].sum()
category_sales.plot(kind='bar')
plt.title('Sales by Product Category')
plt.xlabel('Product Category')
plt.ylabel('Total Sales ($)')
plt.show()
数据可视化
除了上述分析，我们还可以利用Seaborn库进行更深入的探索性数据分析，比如通过散点图矩阵（Pair Plot）来观察不同变量间的关系：

# 假设数据集中还有'Customer_Age'字段
sns.pairplot(data[['Sales', 'Product_Category', 'Customer_Age']])
plt.show()
结论
通过上述步骤，我们不仅完成了数据的读取、清洗、分析，还借助Python的可视化库直观展示了数据背后的故事。实践证明，Python是进行数据挖掘与分析的强大工具，无论是处理大规模数据集，还是进行复杂的数据可视化，都能轻松应对。掌握这些技能，将帮助你在数据科学领域更进一步。在数据分析和建模的过程中，相当多的时间要用在数据准备上：加载、清理、转换以及重塑。这些工作会占到分析师时间的80%或更多。有时，存储在文件和数据库中的数据的格式不适合某个特定的任务。许多研究者都选择使用通用编程语言（如Python、Perl、R或Java）或UNIX文本处理工具（如sed或awk）对数据格式进行专门处理。幸运的是，pandas和内置的Python标准库提供了一组高级的、灵活的、快速的工具，可以让你轻松地将数据规整为想要的格式。

如果你发现了一种本书或pandas库中没有的数据操作方式，请在邮件列表或GitHub网站上提出。实际上，pandas的许多设计和实现都是由真实应用的需求所驱动的。

在本章中，我会讨论处理缺失数据、重复数据、字符串操作和其它分析数据转换的工具。下一章，我会关注于用多种方法合并、重塑数据集。

# 7.1 处理缺失数据
在许多数据分析工作中，缺失数据是经常发生的。pandas的目标之一就是尽量轻松地处理缺失数据。例如，pandas对象的所有描述性统计默认都不包括缺失数据。

缺失数据在pandas中呈现的方式有些不完美，但对于大多数用户可以保证功能正常。对于数值数据，pandas使用浮点值NaN（Not a Number）表示缺失数据。我们称其为哨兵值，可以方便的检测出来：
```python
In [10]: string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])

In [11]: string_data
Out[11]:
0     aardvark
1    artichoke
2          NaN
3      avocado
dtype: object

In [12]: string_data.isnull()
Out[12]: 
0    False
1    False
2     True
3    False
dtype: bool
```

在pandas中，我们采用了R语言中的惯用法，即将缺失值表示为NA，它表示不可用not available。在统计应用中，NA数据可能是不存在的数据或者虽然存在，但是没有观察到（例如，数据采集中发生了问题）。当进行数据清洗以进行分析时，最好直接对缺失数据进行分析，以判断数据采集的问题或缺失数据可能导致的偏差。

Python内置的None值在对象数组中也可以作为NA：
```python
In [13]: string_data[0] = None

In [14]: string_data.isnull()
Out[14]: 
0     True
1    False
2     True
3    False
dtype: bool
```

pandas项目中还在不断优化内部细节以更好处理缺失数据，像用户API功能，例如pandas.isnull，去除了许多恼人的细节。表7-1列出了一些关于缺失数据处理的函数。

![表7-1 NA处理方法](http://upload-images.jianshu.io/upload_images/7178691-1a0f73e5bb26ea21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 滤除缺失数据
过滤掉缺失数据的办法有很多种。你可以通过pandas.isnull或布尔索引的手工方法，但dropna可能会更实用一些。对于一个Series，dropna返回一个仅含非空数据和索引值的Series：
```python
In [15]: from numpy import nan as NA

In [16]: data = pd.Series([1, NA, 3.5, NA, 7])

In [17]: data.dropna()
Out[17]: 
0    1.0
2    3.5
4    7.0
dtype: float64
```

这等价于：
```python
In [18]: data[data.notnull()]
Out[18]: 
0    1.0
2    3.5
4    7.0
dtype: float64
```

而对于DataFrame对象，事情就有点复杂了。你可能希望丢弃全NA或含有NA的行或列。dropna默认丢弃任何含有缺失值的行：
```python
In [19]: data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA],
   ....:                      [NA, NA, NA], [NA, 6.5, 3.]])

In [20]: cleaned = data.dropna()

In [21]: data
Out[21]: 
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
2  NaN  NaN  NaN
3  NaN  6.5  3.0

In [22]: cleaned
Out[22]: 
     0    1    2
0  1.0  6.5  3.0
```

传入how='all'将只丢弃全为NA的那些行：
```python
In [23]: data.dropna(how='all')
Out[23]: 
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
3  NaN  6.5  3.0
```

用这种方式丢弃列，只需传入axis=1即可：
```python
In [24]: data[4] = NA

In [25]: data
Out[25]: 
     0    1    2   4
0  1.0  6.5  3.0 NaN
1  1.0  NaN  NaN NaN
2  NaN  NaN  NaN NaN
3  NaN  6.5  3.0 NaN

In [26]: data.dropna(axis=1, how='all')
Out[26]: 
     0    1    2
0  1.0  6.5  3.0
1  1.0  NaN  NaN
2  NaN  NaN  NaN
3  NaN  6.5  3.0
```

另一个滤除DataFrame行的问题涉及时间序列数据。假设你只想留下一部分观测数据，可以用thresh参数实现此目的：
```python
In [27]: df = pd.DataFrame(np.random.randn(7, 3))

In [28]: df.iloc[:4, 1] = NA

In [29]: df.iloc[:2, 2] = NA

In [30]: df
Out[30]: 
          0         1         2
0 -0.204708       NaN       NaN
1 -0.555730       NaN       NaN
2  0.092908       NaN  0.769023
3  1.246435       NaN -1.296221
4  0.274992  0.228913  1.352917
5  0.886429 -2.001637 -0.371843
6  1.669025 -0.438570 -0.539741

In [31]: df.dropna()
Out[31]: 
          0         1         2
4  0.274992  0.228913  1.352917
5  0.886429 -2.001637 -0.371843
6  1.669025 -0.438570 -0.539741

In [32]: df.dropna(thresh=2)
Out[32]: 
          0         1         2
2  0.092908       NaN  0.769023
3  1.246435       NaN -1.296221
4  0.274992  0.228913  1.352917
5  0.886429 -2.001637 -0.371843
6  1.669025 -0.438570 -0.539741
```

## 填充缺失数据
你可能不想滤除缺失数据（有可能会丢弃跟它有关的其他数据），而是希望通过其他方式填补那些“空洞”。对于大多数情况而言，fillna方法是最主要的函数。通过一个常数调用fillna就会将缺失值替换为那个常数值：
```python
In [33]: df.fillna(0)
Out[33]: 
          0         1         2
0 -0.204708  0.000000  0.000000
1 -0.555730  0.000000  0.000000
2  0.092908  0.000000  0.769023
3  1.246435  0.000000 -1.296221
4  0.274992  0.228913  1.352917
5  0.886429 -2.001637 -0.371843
6  1.669025 -0.438570 -0.539741
```

若是通过一个字典调用fillna，就可以实现对不同的列填充不同的值：
```python
In [34]: df.fillna({1: 0.5, 2: 0})
Out[34]: 
          0         1         2
0 -0.204708  0.500000  0.000000
1 -0.555730  0.500000  0.000000
2  0.092908  0.500000  0.769023
3  1.246435  0.500000 -1.296221
4  0.274992  0.228913  1.352917
5  0.886429 -2.001637 -0.371843
6  1.669025 -0.438570 -0.539741
```

fillna默认会返回新对象，但也可以对现有对象进行就地修改：
```python
In [35]: _ = df.fillna(0, inplace=True)

In [36]: df
Out[36]: 
          0         1         2
0 -0.204708  0.000000  0.000000
1 -0.555730  0.000000  0.000000
2  0.092908  0.000000  0.769023
3  1.246435  0.000000 -1.296221
4  0.274992  0.228913  1.352917
5  0.886429 -2.001637 -0.371843
6  1.669025 -0.438570 -0.539741
```

对reindexing有效的那些插值方法也可用于fillna：
```python
In [37]: df = pd.DataFrame(np.random.randn(6, 3))

In [38]: df.iloc[2:, 1] = NA

In [39]: df.iloc[4:, 2] = NA

In [40]: df
Out[40]: 
          0         1         2
0  0.476985  3.248944 -1.021228
1 -0.577087  0.124121  0.302614
2  0.523772       NaN  1.343810
3 -0.713544       NaN -2.370232
4 -1.860761       NaN       NaN
5 -1.265934       NaN       NaN

In [41]: df.fillna(method='ffill')
Out[41]: 
          0         1         2
0  0.476985  3.248944 -1.021228
1 -0.577087  0.124121  0.302614
2  0.523772  0.124121  1.343810
3 -0.713544  0.124121 -2.370232
4 -1.860761  0.124121 -2.370232
5 -1.265934  0.124121 -2.370232

In [42]: df.fillna(method='ffill', limit=2)
Out[42]: 
          0         1         2
0  0.476985  3.248944 -1.021228
1 -0.577087  0.124121  0.302614
2  0.523772  0.124121  1.343810
3 -0.713544  0.124121 -2.370232
4 -1.860761       NaN -2.370232
5 -1.265934       NaN -2.370232
```

只要有些创新，你就可以利用fillna实现许多别的功能。比如说，你可以传入Series的平均值或中位数：
```python
In [43]: data = pd.Series([1., NA, 3.5, NA, 7])

In [44]: data.fillna(data.mean())
Out[44]: 
0    1.000000
1    3.833333
2    3.500000
3    3.833333
4    7.000000
dtype: float64
```
表7-2列出了fillna的参考。

![](http://upload-images.jianshu.io/upload_images/7178691-0bf235386a64c3b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![fillna函数参数](http://upload-images.jianshu.io/upload_images/7178691-4edd39e68f4dc530.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 7.2 数据转换
本章到目前为止介绍的都是数据的重排。另一类重要操作则是过滤、清理以及其他的转换工作。

## 移除重复数据

DataFrame中出现重复行有多种原因。下面就是一个例子：
```python
In [45]: data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],
   ....:                      'k2': [1, 1, 2, 3, 3, 4, 4]})

In [46]: data
Out[46]: 
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4
6  two   4
```

DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行（前面出现过的行）：
```python
In [47]: data.duplicated()
Out[47]: 
0    False
1    False
2    False
3    False
4    False
5    False
6     True
dtype: bool
```

还有一个与此相关的drop_duplicates方法，它会返回一个DataFrame，重复的数组会标为False：
```python
In [48]: data.drop_duplicates()
Out[48]: 
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4
```

这两个方法默认会判断全部列，你也可以指定部分列进行重复项判断。假设我们还有一列值，且只希望根据k1列过滤重复项：
```python
In [49]: data['v1'] = range(7)

In [50]: data.drop_duplicates(['k1'])
Out[50]: 
    k1  k2  v1
0  one   1   0
1  two   1   1
```

duplicated和drop_duplicates默认保留的是第一个出现的值组合。传入keep='last'则保留最后一个：
```python
In [51]: data.drop_duplicates(['k1', 'k2'], keep='last')
Out[51]: 
    k1  k2  v1
0  one   1   0
1  two   1   1
2  one   2   2
3  two   3   3
4  one   3   4
6  two   4   6
```

## 利用函数或映射进行数据转换
对于许多数据集，你可能希望根据数组、Series或DataFrame列中的值来实现转换工作。我们来看看下面这组有关肉类的数据：
```python
In [52]: data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',
   ....:                               'Pastrami', 'corned beef', 'Bacon',
   ....:                               'pastrami', 'honey ham', 'nova lox'],
   ....:                      'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})

In [53]: data
Out[53]: 
          food  ounces
0        bacon     4.0
1  pulled pork     3.0
2        bacon    12.0
3     Pastrami     6.0
4  corned beef     7.5
5        Bacon     8.0
6     pastrami     3.0
7    honey ham     5.0
8     nova lox     6.0
```

假设你想要添加一列表示该肉类食物来源的动物类型。我们先编写一个不同肉类到动物的映射：
```python
meat_to_animal = {
  'bacon': 'pig',
  'pulled pork': 'pig',
  'pastrami': 'cow',
  'corned beef': 'cow',
  'honey ham': 'pig',
  'nova lox': 'salmon'
}
```

Series的map方法可以接受一个函数或含有映射关系的字典型对象，但是这里有一个小问题，即有些肉类的首字母大写了，而另一些则没有。因此，我们还需要使用Series的str.lower方法，将各个值转换为小写：
```python
In [55]: lowercased = data['food'].str.lower()

In [56]: lowercased
Out[56]: 
0          bacon
1    pulled pork
2          bacon
3       pastrami
4    corned beef
5          bacon
6       pastrami
7      honey ham
8       nova lox
Name: food, dtype: object

In [57]: data['animal'] = lowercased.map(meat_to_animal)

In [58]: data
Out[58]: 
          food  ounces  animal
0        bacon     4.0     pig
1  pulled pork     3.0     pig
2        bacon    12.0     pig
3     Pastrami     6.0     cow
4  corned beef     7.5     cow
5        Bacon     8.0     pig
6     pastrami     3.0     cow
7    honey ham     5.0     pig
8     nova lox     6.0  salmon
```

我们也可以传入一个能够完成全部这些工作的函数：
```python
In [59]: data['food'].map(lambda x: meat_to_animal[x.lower()])
Out[59]: 
0       pig
1       pig
2       pig
3       cow
4       cow
5       pig
6       cow
7       pig
8    salmon
Name: food, dtype: object
```

使用map是一种实现元素级转换以及其他数据清理工作的便捷方式。

## 替换值
利用fillna方法填充缺失数据可以看做值替换的一种特殊情况。前面已经看到，map可用于修改对象的数据子集，而replace则提供了一种实现该功能的更简单、更灵活的方式。我们来看看下面这个Series：
```python
In [60]: data = pd.Series([1., -999., 2., -999., -1000., 3.])

In [61]: data
Out[61]: 
0       1.0
1    -999.0
2       2.0
3    -999.0
4   -1000.0
5       3.0
```

-999这个值可能是一个表示缺失数据的标记值。要将其替换为pandas能够理解的NA值，我们可以利用replace来产生一个新的Series（除非传入inplace=True）：
```python
In [62]: data.replace(-999, np.nan)
Out[62]: 
0       1.0
1       NaN
2       2.0
3       NaN
4   -1000.0
5       3.0
dtype: float64
```

如果你希望一次性替换多个值，可以传入一个由待替换值组成的列表以及一个替换值：：
```python
In [63]: data.replace([-999, -1000], np.nan)
Out[63]: 
0    1.0
1    NaN
2    2.0
3    NaN
4    NaN
5    3.0
dtype: float64
```

要让每个值有不同的替换值，可以传递一个替换列表：
```python
In [64]: data.replace([-999, -1000], [np.nan, 0])
Out[64]: 
0    1.0
1    NaN
2    2.0
3    NaN
4    0.0
5    3.0
dtype: float64
```

传入的参数也可以是字典：
```python
In [65]: data.replace({-999: np.nan, -1000: 0})
Out[65]: 
0    1.0
1    NaN
2    2.0
3    NaN
4    0.0
5    3.0
dtype: float64
```

>笔记：data.replace方法与data.str.replace不同，后者做的是字符串的元素级替换。我们会在后面学习Series的字符串方法。

## 重命名轴索引
跟Series中的值一样，轴标签也可以通过函数或映射进行转换，从而得到一个新的不同标签的对象。轴还可以被就地修改，而无需新建一个数据结构。接下来看看下面这个简单的例子：
```python
In [66]: data = pd.DataFrame(np.arange(12).reshape((3, 4)),
   ....:                     index=['Ohio', 'Colorado', 'New York'],
   ....:                     columns=['one', 'two', 'three', 'four'])
```

跟Series一样，轴索引也有一个map方法：
```python
In [67]: transform = lambda x: x[:4].upper()

In [68]: data.index.map(transform)
Out[68]: Index(['OHIO', 'COLO', 'NEW '], dtype='object')
```

你可以将其赋值给index，这样就可以对DataFrame进行就地修改：
```python
In [69]: data.index = data.index.map(transform)

In [70]: data
Out[70]:
one  two  three  four
OHIO    0    1      2     3
COLO    4    5      6     7
NEW     8    9     10    11
```

如果想要创建数据集的转换版（而不是修改原始数据），比较实用的方法是rename：
```python
In [71]: data.rename(index=str.title, columns=str.upper)
Out[71]: 
      ONE  TWO  THREE  FOUR
Ohio    0    1      2     3
Colo    4    5      6     7
New     8    9     10    11
```

特别说明一下，rename可以结合字典型对象实现对部分轴标签的更新：
```python
In [72]: data.rename(index={'OHIO': 'INDIANA'},
   ....:             columns={'three': 'peekaboo'})
Out[72]:
one  two  peekaboo  four
INDIANA    0    1         2     3
COLO       4    5         6     7
NEW        8    9        10    11
```

rename可以实现复制DataFrame并对其索引和列标签进行赋值。如果希望就地修改某个数据集，传入inplace=True即可：
```python
In [73]: data.rename(index={'OHIO': 'INDIANA'}, inplace=True)

In [74]: data
Out[74]: 
         one  two  three  four
INDIANA    0    1      2     3
COLO       4    5      6     7
NEW        8    9     10    11
```

## 离散化和面元划分
为了便于分析，连续数据常常被离散化或拆分为“面元”（bin）。假设有一组人员数据，而你希望将它们划分为不同的年龄组：
```python
In [75]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
```

接下来将这些数据划分为“18到25”、“26到35”、“35到60”以及“60以上”几个面元。要实现该功能，你需要使用pandas的cut函数：
```python
In [76]: bins = [18, 25, 35, 60, 100]

In [77]: cats = pd.cut(ages, bins)

In [78]: cats
Out[78]: 
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35,60], (35, 60], (25, 35]]
Length: 12
Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]
```

pandas返回的是一个特殊的Categorical对象。结果展示了pandas.cut划分的面元。你可以将其看做一组表示面元名称的字符串。它的底层含有一个表示不同分类名称的类型数组，以及一个codes属性中的年龄数据的标签：
```python
In [79]: cats.codes
Out[79]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)

In [80]: cats.categories
Out[80]: 
IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]]
              closed='right',
              dtype='interval[int64]')

In [81]: pd.value_counts(cats)
Out[81]: 
(18, 25]     5
(35, 60]     3
(25, 35]     3
(60, 100]    1
dtype: int64
```

pd.value_counts(cats)是pandas.cut结果的面元计数。

跟“区间”的数学符号一样，圆括号表示开端，而方括号则表示闭端（包括）。哪边是闭端可以通过right=False进行修改：
```python
In [82]: pd.cut(ages, [18, 26, 36, 61, 100], right=False)
Out[82]: 
[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36,
 61), [36, 61), [26, 36)]
Length: 12
Categories (4, interval[int64]): [[18, 26) < [26, 36) < [36, 61) < [61, 100)]
```

你可 以通过传递一个列表或数组到labels，设置自己的面元名称：
```python
In [83]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']

In [84]: pd.cut(ages, bins, labels=group_names)
Out[84]: 
[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, Mid
dleAged, YoungAdult]
Length: 12
Categories (4, object): [Youth < YoungAdult < MiddleAged < Senior]
```

如果向cut传入的是面元的数量而不是确切的面元边界，则它会根据数据的最小值和最大值计算等长面元。下面这个例子中，我们将一些均匀分布的数据分成四组：
```python
In [85]: data = np.random.rand(20)

In [86]: pd.cut(data, 4, precision=2)
Out[86]: 
[(0.34, 0.55], (0.34, 0.55], (0.76, 0.97], (0.76, 0.97], (0.34, 0.55], ..., (0.34
, 0.55], (0.34, 0.55], (0.55, 0.76], (0.34, 0.55], (0.12, 0.34]]
Length: 20
Categories (4, interval[float64]): [(0.12, 0.34] < (0.34, 0.55] < (0.55, 0.76] < 
(0.76, 0.97]]
```

选项precision=2，限定小数只有两位。

qcut是一个非常类似于cut的函数，它可以根据样本分位数对数据进行面元划分。根据数据的分布情况，cut可能无法使各个面元中含有相同数量的数据点。而qcut由于使用的是样本分位数，因此可以得到大小基本相等的面元：
```python
In [87]: data = np.random.randn(1000)  # Normally distributed

In [88]: cats = pd.qcut(data, 4)  # Cut into quartiles

In [89]: cats
Out[89]: 
[(-0.0265, 0.62], (0.62, 3.928], (-0.68, -0.0265], (0.62, 3.928], (-0.0265, 0.62]
, ..., (-0.68, -0.0265], (-0.68, -0.0265], (-2.95, -0.68], (0.62, 3.928], (-0.68,
 -0.0265]]
Length: 1000
Categories (4, interval[float64]): [(-2.95, -0.68] < (-0.68, -0.0265] < (-0.0265,
 0.62] <
                                    (0.62, 3.928]]

In [90]: pd.value_counts(cats)
Out[90]:
(0.62, 3.928]       250
(-0.0265, 0.62]     250
(-0.68, -0.0265]    250
(-2.95, -0.68]      250
dtype: int64
```

与cut类似，你也可以传递自定义的分位数（0到1之间的数值，包含端点）：
```python
In [91]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])
Out[91]: 
[(-0.0265, 1.286], (-0.0265, 1.286], (-1.187, -0.0265], (-0.0265, 1.286], (-0.026
5, 1.286], ..., (-1.187, -0.0265], (-1.187, -0.0265], (-2.95, -1.187], (-0.0265, 
1.286], (-1.187, -0.0265]]
Length: 1000
Categories (4, interval[float64]): [(-2.95, -1.187] < (-1.187, -0.0265] < (-0.026
5, 1.286] <
                                    (1.286, 3.928]]
```

本章稍后在讲解聚合和分组运算时会再次用到cut和qcut，因为这两个离散化函数对分位和分组分析非常重要。

## 检测和过滤异常值
过滤或变换异常值（outlier）在很大程度上就是运用数组运算。来看一个含有正态分布数据的DataFrame：
```python
In [92]: data = pd.DataFrame(np.random.randn(1000, 4))

In [93]: data.describe()
Out[93]: 
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean      0.049091     0.026112    -0.002544    -0.051827
std       0.996947     1.007458     0.995232     0.998311
min      -3.645860    -3.184377    -3.745356    -3.428254
25%      -0.599807    -0.612162    -0.687373    -0.747478
50%       0.047101    -0.013609    -0.022158    -0.088274
75%       0.756646     0.695298     0.699046     0.623331
max       2.653656     3.525865     2.735527     3.366626
```

假设你想要找出某列中绝对值大小超过3的值：
```python
In [94]: col = data[2]

In [95]: col[np.abs(col) > 3]
Out[95]: 
41    -3.399312
136   -3.745356
Name: 2, dtype: float64
```

要选出全部含有“超过3或－3的值”的行，你可以在布尔型DataFrame中使用any方法：
```python
In [96]: data[(np.abs(data) > 3).any(1)]
Out[96]: 
            0         1         2         3
41   0.457246 -0.025907 -3.399312 -0.974657
60   1.951312  3.260383  0.963301  1.201206
136  0.508391 -0.196713 -3.745356 -1.520113
235 -0.242459 -3.056990  1.918403 -0.578828
258  0.682841  0.326045  0.425384 -3.428254
322  1.179227 -3.184377  1.369891 -1.074833
544 -3.548824  1.553205 -2.186301  1.277104
635 -0.578093  0.193299  1.397822  3.366626
782 -0.207434  3.525865  0.283070  0.544635
803 -3.645860  0.255475 -0.549574 -1.907459
```

根据这些条件，就可以对值进行设置。下面的代码可以将值限制在区间－3到3以内：
```python
In [97]: data[np.abs(data) > 3] = np.sign(data) * 3

In [98]: data.describe()
Out[98]: 
                 0            1            2            3
count  1000.000000  1000.000000  1000.000000  1000.000000
mean      0.050286     0.025567    -0.001399    -0.051765
std       0.992920     1.004214     0.991414     0.995761
min      -3.000000    -3.000000    -3.000000    -3.000000
25%      -0.599807    -0.612162    -0.687373    -0.747478
50%       0.047101    -0.013609    -0.022158    -0.088274
75%       0.756646     0.695298     0.699046     0.623331
max       2.653656     3.000000     2.735527     3.000000
```

根据数据的值是正还是负，np.sign(data)可以生成1和-1：
```python
In [99]: np.sign(data).head()
Out[99]: 
     0    1    2    3
0 -1.0  1.0 -1.0  1.0
1  1.0 -1.0  1.0 -1.0
2  1.0  1.0  1.0 -1.0
3 -1.0 -1.0  1.0 -1.0
4 -1.0  1.0 -1.0 -1.0
```

## 排列和随机采样
利用numpy.random.permutation函数可以轻松实现对Series或DataFrame的列的排列工作（permuting，随机重排序）。通过需要排列的轴的长度调用permutation，可产生一个表示新顺序的整数数组：
```python
In [100]: df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))

In [101]: sampler = np.random.permutation(5)

In [102]: sampler
Out[102]: array([3, 1, 4, 2, 0])
```

然后就可以在基于iloc的索引操作或take函数中使用该数组了：
```python
In [103]: df
Out[103]: 
    0   1   2   3
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19

In [104]: df.take(sampler)
Out[104]: 
    0   1   2   3
3  12  13  14  15
1   4   5   6   7
4  16  17  18  19
2   8   9  10  11
0   0   1   2   3
```

如果不想用替换的方式选取随机子集，可以在Series和DataFrame上使用sample方法：
```python
In [105]: df.sample(n=3)
Out[105]: 
    0   1   2   3
3  12  13  14  15
4  16  17  18  19
2   8   9  10  11
```

要通过替换的方式产生样本（允许重复选择），可以传递replace=True到sample：
```python
In [106]: choices = pd.Series([5, 7, -1, 6, 4])

In [107]: draws = choices.sample(n=10, replace=True)

In [108]: draws
Out[108]: 
4    4
1    7
4    4
2   -1
0    5
3    6
1    7
4    4
0    5
4    4
dtype: int64
```

## 计算指标/哑变量
另一种常用于统计建模或机器学习的转换方式是：将分类变量（categorical variable）转换为“哑变量”或“指标矩阵”。

如果DataFrame的某一列中含有k个不同的值，则可以派生出一个k列矩阵或DataFrame（其值全为1和0）。pandas有一个get_dummies函数可以实现该功能（其实自己动手做一个也不难）。使用之前的一个DataFrame例子：
```python
In [109]: df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],
   .....:                    'data1': range(6)})

In [110]: pd.get_dummies(df['key'])
Out[110]: 
   a  b  c
0  0  1  0
1  0  1  0
2  1  0  0
3  0  0  1
4  1  0  0
5  0  1  0
```

有时候，你可能想给指标DataFrame的列加上一个前缀，以便能够跟其他数据进行合并。get_dummies的prefix参数可以实现该功能：
```python
In [111]: dummies = pd.get_dummies(df['key'], prefix='key')

In [112]: df_with_dummy = df[['data1']].join(dummies)

In [113]: df_with_dummy
Out[113]: 
   data1  key_a  key_b  key_c
0      0      0      1      0
1      1      0      1      0
2      2      1      0      0
3      3      0      0      1
4      4      1      0      0
5      5      0      1      0
```

如果DataFrame中的某行同属于多个分类，则事情就会有点复杂。看一下MovieLens 1M数据集，14章会更深入地研究它：
```python
In [114]: mnames = ['movie_id', 'title', 'genres']

In [115]: movies = pd.read_table('datasets/movielens/movies.dat', sep='::',
   .....:                        header=None, names=mnames)

In [116]: movies[:10]
Out[116]: 
   movie_id                               title                        genres
0         1                    Toy Story (1995)   Animation|Children's|Comedy
1         2                      Jumanji (1995)  Adventure|Children's|Fantasy
2         3             Grumpier Old Men (1995)                Comedy|Romance
3         4            Waiting to Exhale (1995)                  Comedy|Drama
4         5  Father of the Bride Part II (1995)                        Comedy
5         6                         Heat (1995)         Action|Crime|Thriller
6         7                      Sabrina (1995)                Comedy|Romance
7         8                 Tom and Huck (1995)          Adventure|Children's
8         9                 Sudden Death (1995)
Action
9        10                    GoldenEye (1995)     Action|Adventure|Thriller
```

要为每个genre添加指标变量就需要做一些数据规整操作。首先，我们从数据集中抽取出不同的genre值：
```python
In [117]: all_genres = []

In [118]: for x in movies.genres:
   .....:     all_genres.extend(x.split('|'))

In [119]: genres = pd.unique(all_genres)
```

现在有：
```python
In [120]: genres
Out[120]: 
array(['Animation', "Children's", 'Comedy', 'Adventure', 'Fantasy',
       'Romance', 'Drama', 'Action', 'Crime', 'Thriller','Horror',
       'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',
       'Western'], dtype=object)
```

构建指标DataFrame的方法之一是从一个全零DataFrame开始：
```python
In [121]: zero_matrix = np.zeros((len(movies), len(genres)))

In [122]: dummies = pd.DataFrame(zero_matrix, columns=genres)
```

现在，迭代每一部电影，并将dummies各行的条目设为1。要这么做，我们使用dummies.columns来计算每个类型的列索引：
```python
In [123]: gen = movies.genres[0]

In [124]: gen.split('|')
Out[124]: ['Animation', "Children's", 'Comedy']

In [125]: dummies.columns.get_indexer(gen.split('|'))
Out[125]: array([0, 1, 2])
```

然后，根据索引，使用.iloc设定值：
```python
In [126]: for i, gen in enumerate(movies.genres):
   .....:     indices = dummies.columns.get_indexer(gen.split('|'))
   .....:     dummies.iloc[i, indices] = 1
   .....:
```

然后，和以前一样，再将其与movies合并起来：
```python
In [127]: movies_windic = movies.join(dummies.add_prefix('Genre_'))

In [128]: movies_windic.iloc[0]
Out[128]: 
movie_id                                       1
title                           Toy Story (1995)
genres               Animation|Children's|Comedy
Genre_Animation                                1
Genre_Children's                               1
Genre_Comedy                                   1
Genre_Adventure                                0
Genre_Fantasy                                  0
Genre_Romance                                  0
Genre_Drama                                    0
                                ...             
Genre_Crime                                    0
Genre_Thriller                                 0
Genre_Horror                                   0
Genre_Sci-Fi                                   0
Genre_Documentary                              0
Genre_War                                      0
Genre_Musical                                  0
Genre_Mystery                                  0
Genre_Film-Noir                                0
Genre_Western                                  0
Name: 0, Length: 21, dtype: object
```

>笔记：对于很大的数据，用这种方式构建多成员指标变量就会变得非常慢。最好使用更低级的函数，将其写入NumPy数组，然后结果包装在DataFrame中。

一个对统计应用有用的秘诀是：结合get_dummies和诸如cut之类的离散化函数：
```python
In [129]: np.random.seed(12345)

In [130]: values = np.random.rand(10)

In [131]: values
Out[131]: 
array([ 0.9296,  0.3164,  0.1839,  0.2046,  0.5677,  0.5955,  0.9645,
        0.6532,  0.7489,  0.6536])

In [132]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]

In [133]: pd.get_dummies(pd.cut(values, bins))
Out[133]: 
   (0.0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1.0]
0           0           0           0           0           1
1           0           1           0           0           0
2           1           0           0           0           0
3           0           1           0           0           0
4           0           0           1           0           0
5           0           0           1           0           0
6           0           0           0           0           1
7           0           0           0           1           0
8           0           0           0           1           0
9           0           0           0           1           0
```

我们用numpy.random.seed，使这个例子具有确定性。本书后面会介绍pandas.get_dummies。

# 7.3 字符串操作

Python能够成为流行的数据处理语言，部分原因是其简单易用的字符串和文本处理功能。大部分文本运算都直接做成了字符串对象的内置方法。对于更为复杂的模式匹配和文本操作，则可能需要用到正则表达式。pandas对此进行了加强，它使你能够对整组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。

## 字符串对象方法

对于许多字符串处理和脚本应用，内置的字符串方法已经能够满足要求了。例如，以逗号分隔的字符串可以用split拆分成数段：
```python
In [134]: val = 'a,b,  guido'
In [135]: val.split(',')
Out[135]: ['a', 'b', '  guido']
```

split常常与strip一起使用，以去除空白符（包括换行符）：
```python
In [136]: pieces = [x.strip() for x in val.split(',')]

In [137]: pieces
Out[137]: ['a', 'b', 'guido']
```

利用加法，可以将这些子字符串以双冒号分隔符的形式连接起来：
```python
In [138]: first, second, third = pieces

In [139]: first + '::' + second + '::' + third
Out[139]: 'a::b::guido'
```

但这种方式并不是很实用。一种更快更符合Python风格的方式是，向字符串"::"的join方法传入一个列表或元组：
```python
In [140]: '::'.join(pieces)
Out[140]: 'a::b::guido'
```

其它方法关注的是子串定位。检测子串的最佳方式是利用Python的in关键字，还可以使用index和find：
```python
In [141]: 'guido' in val
Out[141]: True

In [142]: val.index(',')
Out[142]: 1

In [143]: val.find(':')
Out[143]: -1
```

注意find和index的区别：如果找不到字符串，index将会引发一个异常（而不是返回－1）：
```python
In [144]: val.index(':')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-144-280f8b2856ce> in <module>()
----> 1 val.index(':')
ValueError: substring not found
```

与此相关，count可以返回指定子串的出现次数：
```python
In [145]: val.count(',')
Out[145]: 2
```

replace用于将指定模式替换为另一个模式。通过传入空字符串，它也常常用于删除模式：
```python
In [146]: val.replace(',', '::')
Out[146]: 'a::b::  guido'

In [147]: val.replace(',', '')
Out[147]: 'ab  guido'
```

表7-3列出了Python内置的字符串方法。

这些运算大部分都能使用正则表达式实现（马上就会看到）。

![](http://upload-images.jianshu.io/upload_images/7178691-087fe67bf6db0701.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/7178691-d1f0d4ed3e895016.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

casefold      将字符转换为小写，并将任何特定区域的变量字符组合转换成一个通用的可比较形式。

## 正则表达式

正则表达式提供了一种灵活的在文本中搜索或匹配（通常比前者复杂）字符串模式的方式。正则表达式，常称作regex，是根据正则表达式语言编写的字符串。Python内置的re模块负责对字符串应用正则表达式。我将通过一些例子说明其使用方法。

>笔记：正则表达式的编写技巧可以自成一章，超出了本书的范围。从网上和其它书可以找到许多非常不错的教程和参考资料。

re模块的函数可以分为三个大类：模式匹配、替换以及拆分。当然，它们之间是相辅相成的。一个regex描述了需要在文本中定位的一个模式，它可以用于许多目的。我们先来看一个简单的例子：假设我想要拆分一个字符串，分隔符为数量不定的一组空白符（制表符、空格、换行符等）。描述一个或多个空白符的regex是\s+：
```python
In [148]: import re

In [149]: text = "foo    bar\t baz  \tqux"

In [150]: re.split('\s+', text)
Out[150]: ['foo', 'bar', 'baz', 'qux']
```

调用re.split('\s+',text)时，正则表达式会先被编译，然后再在text上调用其split方法。你可以用re.compile自己编译regex以得到一个可重用的regex对象：
```python
In [151]: regex = re.compile('\s+')

In [152]: regex.split(text)
Out[152]: ['foo', 'bar', 'baz', 'qux']
```

如果只希望得到匹配regex的所有模式，则可以使用findall方法：
```python
In [153]: regex.findall(text)
Out[153]: ['    ', '\t ', '  \t']
```

>笔记：如果想避免正则表达式中不需要的转义（\），则可以使用原始字符串字面量如r'C:\x'（也可以编写其等价式'C:\\x'）。

如果打算对许多字符串应用同一条正则表达式，强烈建议通过re.compile创建regex对象。这样将可以节省大量的CPU时间。

match和search跟findall功能类似。findall返回的是字符串中所有的匹配项，而search则只返回第一个匹配项。match更加严格，它只匹配字符串的首部。来看一个小例子，假设我们有一段文本以及一条能够识别大部分电子邮件地址的正则表达式：
```python
text = """Dave dave@google.com
Steve steve@gmail.com
Rob rob@gmail.com
Ryan ryan@yahoo.com
"""
pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'

# re.IGNORECASE makes the regex case-insensitive
regex = re.compile(pattern, flags=re.IGNORECASE)
```

对text使用findall将得到一组电子邮件地址：
```python
In [155]: regex.findall(text)
Out[155]: 
['dave@google.com',
 'steve@gmail.com',
 'rob@gmail.com',
 'ryan@yahoo.com']
```

search返回的是文本中第一个电子邮件地址（以特殊的匹配项对象形式返回）。对于上面那个regex，匹配项对象只能告诉我们模式在原字符串中的起始和结束位置：
```python
In [156]: m = regex.search(text)

In [157]: m
Out[157]: <_sre.SRE_Match object; span=(5, 20), match='dave@google.com'>

In [158]: text[m.start():m.end()]
Out[158]: 'dave@google.com'
```

regex.match则将返回None，因为它只匹配出现在字符串开头的模式：
```python
In [159]: print(regex.match(text))
None
```

相关的，sub方法可以将匹配到的模式替换为指定字符串，并返回所得到的新字符串：
```python
In [160]: print(regex.sub('REDACTED', text))
Dave REDACTED
Steve REDACTED
Rob REDACTED
Ryan REDACTED
```

假设你不仅想要找出电子邮件地址，还想将各个地址分成3个部分：用户名、域名以及域后缀。要实现此功能，只需将待分段的模式的各部分用圆括号包起来即可：
```python
In [161]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'

In [162]: regex = re.compile(pattern, flags=re.IGNORECASE)
```

由这种修改过的正则表达式所产生的匹配项对象，可以通过其groups方法返回一个由模式各段组成的元组：
```python
In [163]: m = regex.match('wesm@bright.net')

In [164]: m.groups()
Out[164]: ('wesm', 'bright', 'net')
```

对于带有分组功能的模式，findall会返回一个元组列表：
```python
In [165]: regex.findall(text)
Out[165]:
[('dave', 'google', 'com'),
 ('steve', 'gmail', 'com'),
 ('rob', 'gmail', 'com'),
 ('ryan', 'yahoo', 'com')]
```

sub还能通过诸如\1、\2之类的特殊符号访问各匹配项中的分组。符号\1对应第一个匹配的组，\2对应第二个匹配的组，以此类推：
```python
In [166]: print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text))
Dave Username: dave, Domain: google, Suffix: com
Steve Username: steve, Domain: gmail, Suffix: com
Rob Username: rob, Domain: gmail, Suffix: com
Ryan Username: ryan, Domain: yahoo, Suffix: com
```

Python中还有许多的正则表达式，但大部分都超出了本书的范围。表7-4是一个简要概括。

![](http://upload-images.jianshu.io/upload_images/7178691-efbb80a793759fc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## pandas的矢量化字符串函数

清理待分析的散乱数据时，常常需要做一些字符串规整化工作。更为复杂的情况是，含有字符串的列有时还含有缺失数据：
```python
In [167]: data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',
   .....:         'Rob': 'rob@gmail.com', 'Wes': np.nan}

In [168]: data = pd.Series(data)

In [169]: data
Out[169]: 
Dave     dave@google.com
Rob        rob@gmail.com
Steve    steve@gmail.com
Wes                  NaN
dtype: object

In [170]: data.isnull()
Out[170]: 
Dave     False
Rob      False
Steve    False
Wes       True
dtype: bool
```

通过data.map，所有字符串和正则表达式方法都能被应用于（传入lambda表达式或其他函数）各个值，但是如果存在NA（null）就会报错。为了解决这个问题，Series有一些能够跳过NA值的面向数组方法，进行字符串操作。通过Series的str属性即可访问这些方法。例如，我们可以通过str.contains检查各个电子邮件地址是否含有"gmail"：
```python
In [171]: data.str.contains('gmail')
Out[171]: 
Dave     False
Rob       True
Steve     True
Wes        NaN
dtype: object
```

也可以使用正则表达式，还可以加上任意re选项（如IGNORECASE）：
```python
In [172]: pattern
Out[172]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})'

In [173]: data.str.findall(pattern, flags=re.IGNORECASE)
Out[173]: 
Dave     [(dave, google, com)]
Rob        [(rob, gmail, com)]
Steve    [(steve, gmail, com)]
Wes                        NaN
dtype: object
```

有两个办法可以实现矢量化的元素获取操作：要么使用str.get，要么在str属性上使用索引：
```python
In [174]: matches = data.str.match(pattern, flags=re.IGNORECASE)

In [175]: matches
Out[175]: 
Dave     True
Rob      True
Steve    True
Wes       NaN
dtype: object
```

要访问嵌入列表中的元素，我们可以传递索引到这两个函数中：
```python
In [176]: matches.str.get(1)
Out[176]: 
Dave    NaN
Rob     NaN
Steve   NaN
Wes     NaN
dtype: float64

In [177]: matches.str[0]
Out[177]: 
Dave    NaN
Rob     NaN
Steve   NaN
Wes     NaN
dtype: float64
```

你可以利用这种方法对字符串进行截取：
```python
In [178]: data.str[:5]
Out[178]: 
Dave     dave@
Rob      rob@g
Steve    steve
Wes        NaN
dtype: object
```

表7-5介绍了更多的pandas字符串方法。

![表7-5 部分矢量化字符串方法](http://upload-images.jianshu.io/upload_images/7178691-a634364ed6d5d5c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


 # 7.4 总结

高效的数据准备可以让你将更多的时间用于数据分析，花较少的时间用于准备工作，这样就可以极大地提高生产力。我们在本章中学习了许多工具，但覆盖并不全面。下一章，我们会学习pandas的聚合与分组。
信息可视化（也叫绘图）是数据分析中最重要的工作之一。它可能是探索过程的一部分，例如，帮助我们找出异常值、必要的数据转换、得出有关模型的idea等。另外，做一个可交互的数据可视化也许是工作的最终目标。Python有许多库进行静态或动态的数据可视化，但我这里重要关注于matplotlib（http://matplotlib.org/）和基于它的库。

matplotlib是一个用于创建出版质量图表的桌面绘图包（主要是2D方面）。该项目是由John Hunter于2002年启动的，其目的是为Python构建一个MATLAB式的绘图接口。matplotlib和IPython社区进行合作，简化了从IPython shell（包括现在的Jupyter notebook）进行交互式绘图。matplotlib支持各种操作系统上许多不同的GUI后端，而且还能将图片导出为各种常见的矢量（vector）和光栅（raster）图：PDF、SVG、JPG、PNG、BMP、GIF等。除了几张，本书中的大部分图都是用它生成的。

随着时间的发展，matplotlib衍生出了多个数据可视化的工具集，它们使用matplotlib作为底层。其中之一是seaborn（http://seaborn.pydata.org/），本章后面会学习它。

学习本章代码案例的最简单方法是在Jupyter notebook进行交互式绘图。在Jupyter notebook中执行下面的语句：
```python
%matplotlib notebook
```

# 9.1 matplotlib API入门

 matplotlib的通常引入约定是：

```python
In [11]: import matplotlib.pyplot as plt
```

在Jupyter中运行%matplotlib notebook（或在IPython中运行%matplotlib），就可以创建一个简单的图形。如果一切设置正确，会看到图9-1：

```python
In [12]: import numpy as np

In [13]: data = np.arange(10)

In [14]: data
Out[14]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

In [15]: plt.plot(data)
```

![图9-1 简单的线图](http://upload-images.jianshu.io/upload_images/7178691-7032e333a6ecdd37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

虽然seaborn这样的库和pandas的内置绘图函数能够处理许多普通的绘图任务，但如果需要自定义一些高级功能的话就必须学习matplotlib API。

>笔记：虽然本书没有详细地讨论matplotlib的各种功能，但足以将你引入门。matplotlib的示例库和文档是学习高级特性的最好资源。

## Figure和Subplot

matplotlib的图像都位于Figure对象中。你可以用plt.figure创建一个新的Figure：

```python
In [16]: fig = plt.figure()
```

如果用的是IPython，这时会弹出一个空窗口，但在Jupyter中，必须再输入更多命令才能看到。plt.figure有一些选项，特别是figsize，它用于确保当图片保存到磁盘时具有一定的大小和纵横比。

不能通过空Figure绘图。必须用add_subplot创建一个或多个subplot才行：

```python
In [17]: ax1 = fig.add_subplot(2, 2, 1)
```

这条代码的意思是：图像应该是2×2的（即最多4张图），且当前选中的是4个subplot中的第一个（编号从1开始）。如果再把后面两个subplot也创建出来，最终得到的图像如图9-2所示：

```python
In [18]: ax2 = fig.add_subplot(2, 2, 2)

In [19]: ax3 = fig.add_subplot(2, 2, 3)
```

![图9-2 带有三个subplot的Figure](http://upload-images.jianshu.io/upload_images/7178691-b8cff158e64eae74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>提示：使用Jupyter notebook有一点不同，即每个小窗重新执行后，图形会被重置。因此，对于复杂的图形，，你必须将所有的绘图命令存在一个小窗里。

这里，我们运行同一个小窗里的所有命令：

```python 
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
```

如果这时执行一条绘图命令（如plt.plot([1.5, 3.5, -2, 1.6])），matplotlib就会在最后一个用过的subplot（如果没有则创建一个）上进行绘制，隐藏创建figure和subplot的过程。因此，如果我们执行下列命令，你就会得到如图9-3所示的结果：

```python
In [20]: plt.plot(np.random.randn(50).cumsum(), 'k--')
```

![图9-3 绘制一次之后的图像](http://upload-images.jianshu.io/upload_images/7178691-7bcbd5e56fdbbd92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

"k--"是一个线型选项，用于告诉matplotlib绘制黑色虚线图。上面那些由fig.add_subplot所返回的对象是AxesSubplot对象，直接调用它们的实例方法就可以在其它空着的格子里面画图了，如图9-4所示：

```python
In [21]: ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)

In [22]: ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
```

![图9-4 继续绘制两次之后的图像](http://upload-images.jianshu.io/upload_images/7178691-2297bcaf355db24c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

你可以在matplotlib的文档中找到各种图表类型。

创建包含subplot网格的figure是一个非常常见的任务，matplotlib有一个更为方便的方法plt.subplots，它可以创建一个新的Figure，并返回一个含有已创建的subplot对象的NumPy数组：

```python
In [24]: fig, axes = plt.subplots(2, 3)

In [25]: axes
Out[25]: 
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fb626374048>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb62625db00>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb6262f6c88>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fb6261a36a0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb626181860>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb6260fd4e0>]], dtype
=object)
```

这是非常实用的，因为可以轻松地对axes数组进行索引，就好像是一个二维数组一样，例如axes[0,1]。你还可以通过sharex和sharey指定subplot应该具有相同的X轴或Y轴。在比较相同范围的数据时，这也是非常实用的，否则，matplotlib会自动缩放各图表的界限。有关该方法的更多信息，请参见表9-1。

![表9-1 pyplot.subplots的选项](http://upload-images.jianshu.io/upload_images/7178691-88bb55faca7d01ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 调整subplot周围的间距

默认情况下，matplotlib会在subplot外围留下一定的边距，并在subplot之间留下一定的间距。间距跟图像的高度和宽度有关，因此，如果你调整了图像大小（不管是编程还是手工），间距也会自动调整。利用Figure的subplots_adjust方法可以轻而易举地修改间距，此外，它也是个顶级函数：

```python
subplots_adjust(left=None, bottom=None, right=None, top=None,
                wspace=None, hspace=None)
```

wspace和hspace用于控制宽度和高度的百分比，可以用作subplot之间的间距。下面是一个简单的例子，其中我将间距收缩到了0（如图9-5所示）：

```python
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
plt.subplots_adjust(wspace=0, hspace=0)
```

![图9-5 各subplot之间没有间距](http://upload-images.jianshu.io/upload_images/7178691-80be7ffc3dec88a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

不难看出，其中的轴标签重叠了。matplotlib不会检查标签是否重叠，所以对于这种情况，你只能自己设定刻度位置和刻度标签。后面几节将会详细介绍该内容。

## 颜色、标记和线型

matplotlib的plot函数接受一组X和Y坐标，还可以接受一个表示颜色和线型的字符串缩写。例如，要根据x和y绘制绿色虚线，你可以执行如下代码：

```python
ax.plot(x, y, 'g--')
```

这种在一个字符串中指定颜色和线型的方式非常方便。在实际中，如果你是用代码绘图，你可能不想通过处理字符串来获得想要的格式。通过下面这种更为明确的方式也能得到同样的效果：

```python
ax.plot(x, y, linestyle='--', color='g')
```

常用的颜色可以使用颜色缩写，你也可以指定颜色码（例如，'#CECECE'）。你可以通过查看plot的文档字符串查看所有线型的合集（在IPython和Jupyter中使用plot?）。

线图可以使用标记强调数据点。因为matplotlib可以创建连续线图，在点之间进行插值，因此有时可能不太容易看出真实数据点的位置。标记也可以放到格式字符串中，但标记类型和线型必须放在颜色后面（见图9-6）：

```python
In [30]: from numpy.random import randn

In [31]: plt.plot(randn(30).cumsum(), 'ko--')
```

![图9-6 带有标记的线型图示例](http://upload-images.jianshu.io/upload_images/7178691-404d816f3e1d6621.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

还可以将其写成更为明确的形式：

```python
plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')
```

在线型图中，非实际数据点默认是按线性方式插值的。可以通过drawstyle选项修改（见图9-7）：

```python
In [33]: data = np.random.randn(30).cumsum()

In [34]: plt.plot(data, 'k--', label='Default')
Out[34]: [<matplotlib.lines.Line2D at 0x7fb624d86160>]

In [35]: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
Out[35]: [<matplotlib.lines.Line2D at 0x7fb624d869e8>]

In [36]: plt.legend(loc='best')
```

![图9-7 不同drawstyle选项的线型图](http://upload-images.jianshu.io/upload_images/7178691-3ec7642e1a592f08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

你可能注意到运行上面代码时有输出<matplotlib.lines.Line2D at ...>。matplotlib会返回引用了新添加的子组件的对象。大多数时候，你可以放心地忽略这些输出。这里，因为我们传递了label参数到plot，我们可以创建一个plot图例，指明每条使用plt.legend的线。

>笔记：你必须调用plt.legend（或使用ax.legend，如果引用了轴的话）来创建图例，无论你绘图时是否传递label标签选项。

## 刻度、标签和图例

对于大多数的图表装饰项，其主要实现方式有二：使用过程型的pyplot接口（例如，matplotlib.pyplot）以及更为面向对象的原生matplotlib API。

pyplot接口的设计目的就是交互式使用，含有诸如xlim、xticks和xticklabels之类的方法。它们分别控制图表的范围、刻度位置、刻度标签等。其使用方式有以下两种：

- 调用时不带参数，则返回当前的参数值（例如，plt.xlim()返回当前的X轴绘图范围）。
- 调用时带参数，则设置参数值（例如，plt.xlim([0,10])会将X轴的范围设置为0到10）。

所有这些方法都是对当前或最近创建的AxesSubplot起作用的。它们各自对应subplot对象上的两个方法，以xlim为例，就是ax.get_xlim和ax.set_xlim。我更喜欢使用subplot的实例方法（因为我喜欢明确的事情，而且在处理多个subplot时这样也更清楚一些）。当然你完全可以选择自己觉得方便的那个。

## 设置标题、轴标签、刻度以及刻度标签

为了说明自定义轴，我将创建一个简单的图像并绘制一段随机漫步（如图9-8所示）：

```python
In [37]: fig = plt.figure()

In [38]: ax = fig.add_subplot(1, 1, 1)

In [39]: ax.plot(np.random.randn(1000).cumsum())
```

![图9-8 用于演示xticks的简单线型图（带有标签）](http://upload-images.jianshu.io/upload_images/7178691-caf9300dacb61fa4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

要改变x轴刻度，最简单的办法是使用set_xticks和set_xticklabels。前者告诉matplotlib要将刻度放在数据范围中的哪些位置，默认情况下，这些位置也就是刻度标签。但我们可以通过set_xticklabels将任何其他的值用作标签：

```python
In [40]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])

In [41]: labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'],
   ....:                             rotation=30, fontsize='small')
```

rotation选项设定x刻度标签倾斜30度。最后，再用set_xlabel为X轴设置一个名称，并用set_title设置一个标题（见图9-9的结果）：

```python
In [42]: ax.set_title('My first matplotlib plot')
Out[42]: <matplotlib.text.Text at 0x7fb624d055f8>

In [43]: ax.set_xlabel('Stages')
```

![图9-9 用于演示xticks的简单线型图](http://upload-images.jianshu.io/upload_images/7178691-741f968323bd818f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Y轴的修改方式与此类似，只需将上述代码中的x替换为y即可。轴的类有集合方法，可以批量设定绘图选项。前面的例子，也可以写为：

```python
props = {
    'title': 'My first matplotlib plot',
    'xlabel': 'Stages'
}
ax.set(**props)
```

## 添加图例

图例（legend）是另一种用于标识图表元素的重要工具。添加图例的方式有多种。最简单的是在添加subplot的时候传入label参数：

```python
In [44]: from numpy.random import randn

In [45]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)

In [46]: ax.plot(randn(1000).cumsum(), 'k', label='one')
Out[46]: [<matplotlib.lines.Line2D at 0x7fb624bdf860>]

In [47]: ax.plot(randn(1000).cumsum(), 'k--', label='two')
Out[47]: [<matplotlib.lines.Line2D at 0x7fb624be90f0>]

In [48]: ax.plot(randn(1000).cumsum(), 'k.', label='three')
Out[48]: [<matplotlib.lines.Line2D at 0x7fb624be9160>]
```

在此之后，你可以调用ax.legend()或plt.legend()来自动创建图例（结果见图9-10）：

```python
In [49]: ax.legend(loc='best')
```

![图9-10 带有三条线以及图例的简单线型图](http://upload-images.jianshu.io/upload_images/7178691-651ff89750c0a89b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

legend方法有几个其它的loc位置参数选项。请查看文档字符串（使用ax.legend?）。

loc告诉matplotlib要将图例放在哪。如果你不是吹毛求疵的话，"best"是不错的选择，因为它会选择最不碍事的位置。要从图例中去除一个或多个元素，不传入label或传入label='_nolegend_'即可。（中文第一版这里把best错写成了beat）

## 注解以及在Subplot上绘图

除标准的绘图类型，你可能还希望绘制一些子集的注解，可能是文本、箭头或其他图形等。注解和文字可以通过text、arrow和annotate函数进行添加。text可以将文本绘制在图表的指定坐标(x,y)，还可以加上一些自定义格式：

```python
ax.text(x, y, 'Hello world!',
        family='monospace', fontsize=10)
```

注解中可以既含有文本也含有箭头。例如，我们根据最近的标准普尔500指数价格（来自Yahoo!Finance）绘制一张曲线图，并标出2008年到2009年金融危机期间的一些重要日期。你可以在Jupyter notebook的一个小窗中试验这段代码（图9-11是结果）：

```python
from datetime import datetime

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

data = pd.read_csv('examples/spx.csv', index_col=0, parse_dates=True)
spx = data['SPX']

spx.plot(ax=ax, style='k-')

crisis_data = [
    (datetime(2007, 10, 11), 'Peak of bull market'),
    (datetime(2008, 3, 12), 'Bear Stearns Fails'),
    (datetime(2008, 9, 15), 'Lehman Bankruptcy')
]

for date, label in crisis_data:
    ax.annotate(label, xy=(date, spx.asof(date) + 75),
                xytext=(date, spx.asof(date) + 225),
                arrowprops=dict(facecolor='black', headwidth=4, width=2,
                                headlength=4),
                horizontalalignment='left', verticalalignment='top')

# Zoom in on 2007-2010
ax.set_xlim(['1/1/2007', '1/1/2011'])
ax.set_ylim([600, 1800])

ax.set_title('Important dates in the 2008-2009 financial crisis')
```

![图9-11 2008-2009年金融危机期间的重要日期](http://upload-images.jianshu.io/upload_images/7178691-3127eaa51f5e4c2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这张图中有几个重要的点要强调：ax.annotate方法可以在指定的x和y坐标轴绘制标签。我们使用set_xlim和set_ylim人工设定起始和结束边界，而不使用matplotlib的默认方法。最后，用ax.set_title添加图标标题。

更多有关注解的示例，请访问matplotlib的在线示例库。

图形的绘制要麻烦一些。matplotlib有一些表示常见图形的对象。这些对象被称为块（patch）。其中有些（如Rectangle和Circle），可以在matplotlib.pyplot中找到，但完整集合位于matplotlib.patches。

要在图表中添加一个图形，你需要创建一个块对象shp，然后通过ax.add_patch(shp)将其添加到subplot中（如图9-12所示）：

```python
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],
                   color='g', alpha=0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)
```

![图9-12 由三个块图形组成的图](http://upload-images.jianshu.io/upload_images/7178691-1f8a3d7a3a02d7d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


如果查看许多常见图表对象的具体实现代码，你就会发现它们其实就是由块patch组装而成的。

## 将图表保存到文件

利用plt.savefig可以将当前图表保存到文件。该方法相当于Figure对象的实例方法savefig。例如，要将图表保存为SVG文件，你只需输入：

```python
plt.savefig('figpath.svg')
```

文件类型是通过文件扩展名推断出来的。因此，如果你使用的是.pdf，就会得到一个PDF文件。我在发布图片时最常用到两个重要的选项是dpi（控制“每英寸点数”分辨率）和bbox_inches（可以剪除当前图表周围的空白部分）。要得到一张带有最小白边且分辨率为400DPI的PNG图片，你可以：

```python
plt.savefig('figpath.png', dpi=400, bbox_inches='tight')
```

savefig并非一定要写入磁盘，也可以写入任何文件型的对象，比如BytesIO：

```python
from io import BytesIO
buffer = BytesIO()
plt.savefig(buffer)
plot_data = buffer.getvalue()
```

表9-2列出了savefig的其它选项。

![表9-2 Figure.savefig的选项](http://upload-images.jianshu.io/upload_images/7178691-4bee796bf7262423.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## matplotlib配置

matplotlib自带一些配色方案，以及为生成出版质量的图片而设定的默认配置信息。幸运的是，几乎所有默认行为都能通过一组全局参数进行自定义，它们可以管理图像大小、subplot边距、配色方案、字体大小、网格类型等。一种Python编程方式配置系统的方法是使用rc方法。例如，要将全局的图像默认大小设置为10×10，你可以执行：

```python
plt.rc('figure', figsize=(10, 10))
```

rc的第一个参数是希望自定义的对象，如'figure'、'axes'、'xtick'、'ytick'、'grid'、'legend'等。其后可以跟上一系列的关键字参数。一个简单的办法是将这些选项写成一个字典：

```python
font_options = {'family' : 'monospace',
                'weight' : 'bold',
                'size'   : 'small'}
plt.rc('font', **font_options)
```

要了解全部的自定义选项，请查阅matplotlib的配置文件matplotlibrc（位于matplotlib/mpl-data目录中）。如果对该文件进行了自定义，并将其放在你自己的.matplotlibrc目录中，则每次使用matplotlib时就会加载该文件。

下一节，我们会看到，seaborn包有若干内置的绘图主题或类型，它们使用了matplotlib的内部配置。

# 9.2 使用pandas和seaborn绘图

matplotlib实际上是一种比较低级的工具。要绘制一张图表，你组装一些基本组件就行：数据展示（即图表类型：线型图、柱状图、盒形图、散布图、等值线图等）、图例、标题、刻度标签以及其他注解型信息。

在pandas中，我们有多列数据，还有行和列标签。pandas自身就有内置的方法，用于简化从DataFrame和Series绘制图形。另一个库seaborn（https://seaborn.pydata.org/），由Michael Waskom创建的静态图形库。Seaborn简化了许多常见可视类型的创建。

>提示：引入seaborn会修改matplotlib默认的颜色方案和绘图类型，以提高可读性和美观度。即使你不使用seaborn API，你可能也会引入seaborn，作为提高美观度和绘制常见matplotlib图形的简化方法。

## 线型图

Series和DataFrame都有一个用于生成各类图表的plot方法。默认情况下，它们所生成的是线型图（如图9-13所示）：

```python
In [60]: s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))

In [61]: s.plot()
```

![图9-13 简单的Series图表示例](http://upload-images.jianshu.io/upload_images/7178691-f28e5ab2ac94c7a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

该Series对象的索引会被传给matplotlib，并用以绘制X轴。可以通过use_index=False禁用该功能。X轴的刻度和界限可以通过xticks和xlim选项进行调节，Y轴就用yticks和ylim。plot参数的完整列表请参见表9-3。我只会讲解其中几个，剩下的就留给读者自己去研究了。


![](http://upload-images.jianshu.io/upload_images/7178691-6d9fbf863c09370a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![表9-3 Series.plot方法的参数](http://upload-images.jianshu.io/upload_images/7178691-44e50562aeb5eb49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

pandas的大部分绘图方法都有一个可选的ax参数，它可以是一个matplotlib的subplot对象。这使你能够在网格布局中更为灵活地处理subplot的位置。

DataFrame的plot方法会在一个subplot中为各列绘制一条线，并自动创建图例（如图9-14所示）：
```python
In [62]: df = pd.DataFrame(np.random.randn(10, 4).cumsum(0),
   ....:                   columns=['A', 'B', 'C', 'D'],
   ....:                   index=np.arange(0, 100, 10))

In [63]: df.plot()
```

![图9-14 简单的DataFrame绘图](http://upload-images.jianshu.io/upload_images/7178691-a1234d5e5ee41a40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

plot属性包含一批不同绘图类型的方法。例如，df.plot()等价于df.plot.line()。后面会学习这些方法。

>笔记：plot的其他关键字参数会被传给相应的matplotlib绘图函数，所以要更深入地自定义图表，就必须学习更多有关matplotlib API的知识。

DataFrame还有一些用于对列进行灵活处理的选项，例如，是要将所有列都绘制到一个subplot中还是创建各自的subplot。详细信息请参见表9-4。

![表9-4 专用于DataFrame的plot参数](http://upload-images.jianshu.io/upload_images/7178691-96651ecaa90f1c68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>注意： 有关时间序列的绘图，请见第11章。

## 柱状图

plot.bar()和plot.barh()分别绘制水平和垂直的柱状图。这时，Series和DataFrame的索引将会被用作X（bar）或Y（barh）刻度（如图9-15所示）：

```python
In [64]: fig, axes = plt.subplots(2, 1)

In [65]: data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

In [66]: data.plot.bar(ax=axes[0], color='k', alpha=0.7)
Out[66]: <matplotlib.axes._subplots.AxesSubplot at 0x7fb62493d470>

In [67]: data.plot.barh(ax=axes[1], color='k', alpha=0.7)
```

![图9-15 水平和垂直的柱状图](http://upload-images.jianshu.io/upload_images/7178691-cd54c7ccfa3f0687.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

color='k'和alpha=0.7设定了图形的颜色为黑色，并使用部分的填充透明度。对于DataFrame，柱状图会将每一行的值分为一组，并排显示，如图9-16所示：

```python
In [69]: df = pd.DataFrame(np.random.rand(6, 4),
   ....:                   index=['one', 'two', 'three', 'four', 'five', 'six'],
   ....:                   columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))

In [70]: df
Out[70]: 
Genus         A         B         C         D
one    0.370670  0.602792  0.229159  0.486744
two    0.420082  0.571653  0.049024  0.880592
three  0.814568  0.277160  0.880316  0.431326
four   0.374020  0.899420  0.460304  0.100843
five   0.433270  0.125107  0.494675  0.961825
six    0.601648  0.478576  0.205690  0.560547

In [71]: df.plot.bar()
```

![图9-16 DataFrame的柱状图](http://upload-images.jianshu.io/upload_images/7178691-bfc141acb37d99b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注意，DataFrame各列的名称"Genus"被用作了图例的标题。

设置stacked=True即可为DataFrame生成堆积柱状图，这样每行的值就会被堆积在一起（如图9-17所示）：

```python
In [73]: df.plot.barh(stacked=True, alpha=0.5)
```

![图9-17 DataFrame的堆积柱状图](http://upload-images.jianshu.io/upload_images/7178691-c19e4246eb897978.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>笔记：柱状图有一个非常不错的用法：利用value_counts图形化显示Series中各值的出现频率，比如s.value_counts().plot.bar()。

再以本书前面用过的那个有关小费的数据集为例，假设我们想要做一张堆积柱状图以展示每天各种聚会规模的数据点的百分比。我用read_csv将数据加载进来，然后根据日期和聚会规模创建一张交叉表：

```python
In [75]: tips = pd.read_csv('examples/tips.csv')

In [76]: party_counts = pd.crosstab(tips['day'], tips['size'])

In [77]: party_counts
Out[77]: 
size  1   2   3   4  5  6
day                      
Fri   1  16   1   1  0  0
Sat   2  53  18  13  1  0
Sun   0  39  15  18  3  1
Thur  1  48   4   5  1  3

# Not many 1- and 6-person parties
In [78]: party_counts = party_counts.loc[:, 2:5]
```

然后进行规格化，使得各行的和为1，并生成图表（如图9-18所示）：

```python
# Normalize to sum to 1
In [79]: party_pcts = party_counts.div(party_counts.sum(1), axis=0)

In [80]: party_pcts
Out[80]: 
size         2         3         4         5
day                                         
Fri   0.888889  0.055556  0.055556  0.000000
Sat   0.623529  0.211765  0.152941  0.011765
Sun   0.520000  0.200000  0.240000  0.040000
Thur  0.827586  0.068966  0.086207  0.017241

In [81]: party_pcts.plot.bar()
```

![图9-18 每天各种聚会规模的比例](http://upload-images.jianshu.io/upload_images/7178691-2918f67936823834.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

于是，通过该数据集就可以看出，聚会规模在周末会变大。

对于在绘制一个图形之前，需要进行合计的数据，使用seaborn可以减少工作量。用seaborn来看每天的小费比例（图9-19是结果）：

```python
In [83]: import seaborn as sns

In [84]: tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])

In [85]: tips.head()
Out[85]: 
   total_bill   tip smoker  day    time  size   tip_pct
0       16.99  1.01     No  Sun  Dinner     2  0.063204
1       10.34  1.66     No  Sun  Dinner     3  0.191244
2       21.01  3.50     No  Sun  Dinner     3  0.199886
3       23.68  3.31     No  Sun  Dinner     2  0.162494
4       24.59  3.61     No  Sun  Dinner     4  0.172069

In [86]: sns.barplot(x='tip_pct', y='day', data=tips, orient='h')
```

![图9-19 小费的每日比例，带有误差条](http://upload-images.jianshu.io/upload_images/7178691-c33e8b3add99904b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

seaborn的绘制函数使用data参数，它可能是pandas的DataFrame。其它的参数是关于列的名字。因为一天的每个值有多次观察，柱状图的值是tip_pct的平均值。绘制在柱状图上的黑线代表95%置信区间（可以通过可选参数配置）。

seaborn.barplot有颜色选项，使我们能够通过一个额外的值设置（见图9-20）：

```python
In [88]: sns.barplot(x='tip_pct', y='day', hue='time', data=tips, orient='h')
```

![图9-20 根据天和时间的小费比例](http://upload-images.jianshu.io/upload_images/7178691-06abe2f070222115.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注意，seaborn已经自动修改了图形的美观度：默认调色板，图形背景和网格线的颜色。你可以用seaborn.set在不同的图形外观之间切换：

```python
In [90]: sns.set(style="whitegrid")
```

## 直方图和密度图

直方图（histogram）是一种可以对值频率进行离散化显示的柱状图。数据点被拆分到离散的、间隔均匀的面元中，绘制的是各面元中数据点的数量。再以前面那个小费数据为例，通过在Series使用plot.hist方法，我们可以生成一张“小费占消费总额百分比”的直方图（如图9-21所示）：
```python
In [92]: tips['tip_pct'].plot.hist(bins=50)
```

![图9-21 小费百分比的直方图](http://upload-images.jianshu.io/upload_images/7178691-255279376f7649a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

与此相关的一种图表类型是密度图，它是通过计算“可能会产生观测数据的连续概率分布的估计”而产生的。一般的过程是将该分布近似为一组核（即诸如正态分布之类的较为简单的分布）。因此，密度图也被称作KDE（Kernel Density Estimate，核密度估计）图。使用plot.kde和标准混合正态分布估计即可生成一张密度图（见图9-22）：
```python
In [94]: tips['tip_pct'].plot.density()
```

![图9-22  小费百分比的密度图](http://upload-images.jianshu.io/upload_images/7178691-ee929d033159516a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

seaborn的distplot方法绘制直方图和密度图更加简单，还可以同时画出直方图和连续密度估计图。作为例子，考虑一个双峰分布，由两个不同的标准正态分布组成（见图9-23）：

```python
In [96]: comp1 = np.random.normal(0, 1, size=200)

In [97]: comp2 = np.random.normal(10, 2, size=200)

In [98]: values = pd.Series(np.concatenate([comp1, comp2]))

In [99]: sns.distplot(values, bins=100, color='k')
```

![图9-23 标准混合密度估计的标准直方图](http://upload-images.jianshu.io/upload_images/7178691-975f04d750c4efe2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 散布图或点图

点图或散布图是观察两个一维数据序列之间的关系的有效手段。在下面这个例子中，我加载了来自statsmodels项目的macrodata数据集，选择了几个变量，然后计算对数差：

```python
In [100]: macro = pd.read_csv('examples/macrodata.csv')

In [101]: data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]

In [102]: trans_data = np.log(data).diff().dropna()

In [103]: trans_data[-5:]
Out[103]: 
          cpi        m1  tbilrate     unemp
198 -0.007904  0.045361 -0.396881  0.105361
199 -0.021979  0.066753 -2.277267  0.139762
200  0.002340  0.010286  0.606136  0.160343
201  0.008419  0.037461 -0.200671  0.127339
202  0.008894  0.012202 -0.405465  0.042560
```

然后可以使用seaborn的regplot方法，它可以做一个散布图，并加上一条线性回归的线（见图9-24）：

```python
In [105]: sns.regplot('m1', 'unemp', data=trans_data)
Out[105]: <matplotlib.axes._subplots.AxesSubplot at 0x7fb613720be0>

In [106]: plt.title('Changes in log %s versus log %s' % ('m1', 'unemp'))
```

![图9-24 seaborn的回归/散布图](http://upload-images.jianshu.io/upload_images/7178691-2133d20739478a80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在探索式数据分析工作中，同时观察一组变量的散布图是很有意义的，这也被称为散布图矩阵（scatter plot matrix）。纯手工创建这样的图表很费工夫，所以seaborn提供了一个便捷的pairplot函数，它支持在对角线上放置每个变量的直方图或密度估计（见图9-25）：

```python
In [107]: sns.pairplot(trans_data, diag_kind='kde', plot_kws={'alpha': 0.2})
```

![图9-25 statsmodels macro data的散布图矩阵](http://upload-images.jianshu.io/upload_images/7178691-20aa530a44e06f61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

你可能注意到了plot_kws参数。它可以让我们传递配置选项到非对角线元素上的图形使用。对于更详细的配置选项，可以查阅seaborn.pairplot文档字符串。


##分面网格（facet grid）和类型数据
要是数据集有额外的分组维度呢？有多个分类变量的数据可视化的一种方法是使用小面网格。seaborn有一个有用的内置函数factorplot，可以简化制作多种分面图（见图9-26）：
```python
 In [108]: sns.factorplot(x='day', y='tip_pct', hue='time', col='smoker',
   .....:                kind='bar', data=tips[tips.tip_pct < 1])
```

![图9-26 按照天/时间/吸烟者的小费百分比](http://upload-images.jianshu.io/upload_images/7178691-737ba19a0cbdd46f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

除了在分面中用不同的颜色按时间分组，我们还可以通过给每个时间值添加一行来扩展分面网格：

```python
In [109]: sns.factorplot(x='day', y='tip_pct', row='time',
   .....:                col='smoker',
   .....:                kind='bar', data=tips[tips.tip_pct < 1])
```

![图9-27 按天的tip_pct，通过time/smoker分面](http://upload-images.jianshu.io/upload_images/7178691-4e52192441c609f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

factorplot支持其它的绘图类型，你可能会用到。例如，盒图（它可以显示中位数，四分位数，和异常值）就是一个有用的可视化类型（见图9-28）：

```python
In [110]: sns.factorplot(x='tip_pct', y='day', kind='box',
   .....:                data=tips[tips.tip_pct < 0.5])
```

![图9-28 按天的tip_pct的盒图](http://upload-images.jianshu.io/upload_images/7178691-356fb27a7c658920.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

使用更通用的seaborn.FacetGrid类，你可以创建自己的分面网格。请查阅seaborn的文档（https://seaborn.pydata.org/）。

# 9.3 其它的Python可视化工具
与其它开源库类似，Python创建图形的方式非常多（根本罗列不完）。自从2010年，许多开发工作都集中在创建交互式图形以便在Web上发布。利用工具如Boken（https://bokeh.pydata.org/en/latest/）和Plotly（https://github.com/plotly/plotly.py），现在可以创建动态交互图形，用于网页浏览器。

对于创建用于打印或网页的静态图形，我建议默认使用matplotlib和附加的库，比如pandas和seaborn。对于其它数据可视化要求，学习其它的可用工具可能是有用的。我鼓励你探索绘图的生态系统，因为它将持续发展。

# 9.4 总结

本章的目的是熟悉一些基本的数据可视化操作，使用pandas，matplotlib，和seaborn。如果视觉显示数据分析的结果对你的工作很重要，我鼓励你寻求更多的资源来了解更高效的数据可视化。这是一个活跃的研究领域，你可以通过在线和纸质的形式学习许多优秀的资源。

下一章，我们将重点放在pandas的数据聚合和分组操作上。
在许多应用中，数据可能分散在许多文件或数据库中，存储的形式也不利于分析。本章关注可以聚合、合并、重塑数据的方法。

首先，我会介绍pandas的层次化索引，它广泛用于以上操作。然后，我深入介绍了一些特殊的数据操作。在第14章，你可以看到这些工具的多种应用。

# 8.1 层次化索引

层次化索引（hierarchical indexing）是pandas的一项重要功能，它使你能在一个轴上拥有多个（两个以上）索引级别。抽象点说，它使你能以低维度形式处理高维度数据。我们先来看一个简单的例子：创建一个Series，并用一个由列表或数组组成的列表作为索引：
```python
In [9]: data = pd.Series(np.random.randn(9),
   ...:                  index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],
   ...:                         [1, 2, 3, 1, 3, 1, 2, 2, 3]])

In [10]: data
Out[10]: 
a  1   -0.204708
   2    0.478943
   3   -0.519439
b  1   -0.555730
   3    1.965781
c  1    1.393406
   2    0.092908
d  2    0.281746
   3    0.769023
dtype: float64
```

看到的结果是经过美化的带有MultiIndex索引的Series的格式。索引之间的“间隔”表示“直接使用上面的标签”：
```python
In [11]: data.index
Out[11]: 
MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],
           labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1, 1, 2]])
```

对于一个层次化索引的对象，可以使用所谓的部分索引，使用它选取数据子集的操作更简单：
```python
In [12]: data['b']
Out[12]: 
1   -0.555730
3    1.965781
dtype: float64

In [13]: data['b':'c']
Out[13]: 
b  1   -0.555730
   3    1.965781
c  1    1.393406
   2    0.092908
dtype: float64

In [14]: data.loc[['b', 'd']]
Out[14]: 
b  1   -0.555730
   3    1.965781
d  2    0.281746
   3    0.769023
dtype: float64
```

有时甚至还可以在“内层”中进行选取：
```python
In [15]: data.loc[:, 2]
Out[15]: 
a    0.478943
c    0.092908
d    0.281746
dtype: float64
```

层次化索引在数据重塑和基于分组的操作（如透视表生成）中扮演着重要的角色。例如，可以通过unstack方法将这段数据重新安排到一个DataFrame中：
```python
In [16]: data.unstack()
Out[16]: 
          1         2         3
a -0.204708  0.478943 -0.519439
b -0.555730       NaN  1.965781
c  1.393406  0.092908       NaN
d       NaN  0.281746  0.769023
```

unstack的逆运算是stack：
```python
In [17]: data.unstack().stack()
Out[17]: 
a  1   -0.204708
   2    0.478943
   3   -0.519439
b  1   -0.555730
   3    1.965781
c  1    1.393406
   2    0.092908
d  2    0.281746
   3    0.769023
dtype: float64
```

stack和unstack将在本章后面详细讲解。

对于一个DataFrame，每条轴都可以有分层索引：
```python
In [18]: frame = pd.DataFrame(np.arange(12).reshape((4, 3)),
   ....:                      index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],
   ....:                      columns=[['Ohio', 'Ohio', 'Colorado'],
   ....:                               ['Green', 'Red', 'Green']])

In [19]: frame
Out[19]: 
     Ohio     Colorado
    Green Red    Green
a 1     0   1        2
  2     3   4        5
b 1     6   7        8
  2     9  10       11
```

各层都可以有名字（可以是字符串，也可以是别的Python对象）。如果指定了名称，它们就会显示在控制台输出中：
```python
In [20]: frame.index.names = ['key1', 'key2']

In [21]: frame.columns.names = ['state', 'color']

In [22]: frame
Out[22]: 
state      Ohio     Colorado
color     Green Red    Green
key1 key2                   
a    1        0   1        2
     2        3   4        5
b    1        6   7        8
     2        9  10       11
```

>注意：小心区分索引名state、color与行标签。

有了部分列索引，因此可以轻松选取列分组：
```python
In [23]: frame['Ohio']
Out[23]: 
color      Green  Red
key1 key2            
a    1         0    1
     2         3    4
b    1         6    7
     2         9   10
```

可以单独创建MultiIndex然后复用。上面那个DataFrame中的（带有分级名称）列可以这样创建：
```python
MultiIndex.from_arrays([['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']],
                       names=['state', 'color'])
```

## 重排与分级排序

有时，你需要重新调整某条轴上各级别的顺序，或根据指定级别上的值对数据进行排序。swaplevel接受两个级别编号或名称，并返回一个互换了级别的新对象（但数据不会发生变化）：
```python
In [24]: frame.swaplevel('key1', 'key2')
Out[24]: 
state      Ohio     Colorado
color     Green Red    Green
key2 key1                   
1    a        0   1        2
2    a        3   4        5
1    b        6   7        8
2    b        9  10       11
```

而sort_index则根据单个级别中的值对数据进行排序。交换级别时，常常也会用到sort_index，这样最终结果就是按照指定顺序进行字母排序了：
```python
In [25]: frame.sort_index(level=1)
Out[25]: 
state      Ohio     Colorado
color     Green Red    Green
key1 key2                   
a    1        0   1        2
b    1        6   7        8
a    2        3   4        5
b    2        9  10       11

In [26]: frame.swaplevel(0, 1).sort_index(level=0)
Out[26]: 
state      Ohio     Colorado
color     Green Red    Green
key2 key1                   
1    a        0   1        2
     b        6   7        8
2    a        3   4        5
     b        9  10       11
```

## 根据级别汇总统计

许多对DataFrame和Series的描述和汇总统计都有一个level选项，它用于指定在某条轴上求和的级别。再以上面那个DataFrame为例，我们可以根据行或列上的级别来进行求和：
```python
In [27]: frame.sum(level='key2')
Out[27]: 
state  Ohio     Colorado
color Green Red    Green
key2                    
1         6   8       10
2        12  14       16

In [28]: frame.sum(level='color', axis=1)
Out[28]: 
color      Green  Red
key1 key2            
a    1         2    1
     2         8    4
b    1        14    7
     2        20   10
```

这其实是利用了pandas的groupby功能，本书稍后将对其进行详细讲解。

## 使用DataFrame的列进行索引

人们经常想要将DataFrame的一个或多个列当做行索引来用，或者可能希望将行索引变成DataFrame的列。以下面这个DataFrame为例：
```python
In [29]: frame = pd.DataFrame({'a': range(7), 'b': range(7, 0, -1),
   ....:                       'c': ['one', 'one', 'one', 'two', 'two',
   ....:                             'two', 'two'],
   ....:                       'd': [0, 1, 2, 0, 1, 2, 3]})

In [30]: frame
Out[30]: 
   a  b    c  d
0  0  7  one  0
1  1  6  one  1
2  2  5  one  2
3  3  4  two  0
4  4  3  two  1
5  5  2  two  2
6  6  1  two  3
```

DataFrame的set_index函数会将其一个或多个列转换为行索引，并创建一个新的DataFrame：
```python
In [31]: frame2 = frame.set_index(['c', 'd'])

In [32]: frame2
Out[32]: 
       a  b
c   d      
one 0  0  7
    1  1  6
    2  2  5
two 0  3  4
    1  4  3
    2  5  2
    3  6  1
```

默认情况下，那些列会从DataFrame中移除，但也可以将其保留下来：
```python
In [33]: frame.set_index(['c', 'd'], drop=False)
Out[33]: 
       a  b    c  d
c   d              
one 0  0  7  one  0
    1  1  6  one  1
    2  2  5  one  2
two 0  3  4  two  0
    1  4  3  two  1
    2  5  2  two  2
    3  6  1  two  3
```

reset_index的功能跟set_index刚好相反，层次化索引的级别会被转移到列里面：
```python
In [34]: frame2.reset_index()
Out[34]:
c  d  a  b
0  one  0  0  7
1  one  1  1  6
2  one  2  2  5
3  two  0  3  4
4  two  1  4  3
5  two  2  5  2
6  two  3  6  1
```

# 8.2 合并数据集

pandas对象中的数据可以通过一些方式进行合并：

- pandas.merge可根据一个或多个键将不同DataFrame中的行连接起来。SQL或其他关系型数据库的用户对此应该会比较熟悉，因为它实现的就是数据库的join操作。
- pandas.concat可以沿着一条轴将多个对象堆叠到一起。
- 实例方法combine_first可以将重复数据拼接在一起，用一个对象中的值填充另一个对象中的缺失值。

我将分别对它们进行讲解，并给出一些例子。本书剩余部分的示例中将经常用到它们。

##数据库风格的DataFrame合并

数据集的合并（merge）或连接（join）运算是通过一个或多个键将行连接起来的。这些运算是关系型数据库（基于SQL）的核心。pandas的merge函数是对数据应用这些算法的主要切入点。

以一个简单的例子开始：
```python
In [35]: df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
   ....:                     'data1': range(7)})

In [36]: df2 = pd.DataFrame({'key': ['a', 'b', 'd'],
   ....:                     'data2': range(3)})

In [37]: df1
Out[37]: 
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   a
6      6   b

In [38]: df2
Out[38]: 
   data2 key
0      0   a
1      1   b
2      2   d
```

这是一种多对一的合并。df1中的数据有多个被标记为a和b的行，而df2中key列的每个值则仅对应一行。对这些对象调用merge即可得到：
```python
In [39]: pd.merge(df1, df2)
Out[39]: 
   data1 key  data2
0      0   b      1
1      1   b      1
2      6   b      1
3      2   a      0
4      4   a      0
5      5   a      0
```

注意，我并没有指明要用哪个列进行连接。如果没有指定，merge就会将重叠列的列名当做键。不过，最好明确指定一下：
```python
In [40]: pd.merge(df1, df2, on='key')
Out[40]: 
   data1 key  data2
0      0   b      1
1      1   b      1
2      6   b      1
3      2   a      0
4      4   a      0
5      5   a      0
```

如果两个对象的列名不同，也可以分别进行指定：
```python
In [41]: df3 = pd.DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],
   ....:                     'data1': range(7)})

In [42]: df4 = pd.DataFrame({'rkey': ['a', 'b', 'd'],
   ....:                     'data2': range(3)})

In [43]: pd.merge(df3, df4, left_on='lkey', right_on='rkey')
Out[43]: 
   data1 lkey  data2 rkey
0      0    b      1    b
1      1    b      1    b
2      6    b      1    b
3      2    a      0    a
4      4    a      0    a
5      5    a      0    a
```

可能你已经注意到了，结果里面c和d以及与之相关的数据消失了。默认情况下，merge做的是“内连接”；结果中的键是交集。其他方式还有"left"、"right"以及"outer"。外连接求取的是键的并集，组合了左连接和右连接的效果：
```python
In [44]: pd.merge(df1, df2, how='outer')
Out[44]: 
   data1 key  data2
0    0.0   b    1.0
1    1.0   b    1.0
2    6.0   b    1.0
3    2.0   a    0.0
4    4.0   a    0.0
5    5.0   a    0.0
6    3.0   c    NaN
7    NaN   d    2.0
```

表8-1对这些选项进行了总结。

![表8-1 不同的连接类型](http://upload-images.jianshu.io/upload_images/7178691-e49b3341f4a3c90e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


多对多的合并有些不直观。看下面的例子：
```python
In [45]: df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],
   ....:                     'data1': range(6)})

In [46]: df2 = pd.DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],
   ....:                     'data2': range(5)})

In [47]: df1
Out[47]: 
   data1 key
0      0   b
1      1   b
2      2   a
3      3   c
4      4   a
5      5   b

In [48]: df2
Out[48]: 
   data2 key
0      0   a
1      1   b
2      2   a
3      3   b
4      4   d

In [49]: pd.merge(df1, df2, on='key', how='left')
Out[49]: 
    data1 key  data2
0       0   b    1.0
1       0   b    3.0
2       1   b    1.0
3       1   b    3.0
4       2   a    0.0
5       2   a    2.0
6       3   c    NaN
7       4   a    0.0
8       4   a    2.0
9       5   b    1.0
10      5   b    3.0
```

多对多连接产生的是行的笛卡尔积。由于左边的DataFrame有3个"b"行，右边的有2个，所以最终结果中就有6个"b"行。连接方式只影响出现在结果中的不同的键的值：
```python
In [50]: pd.merge(df1, df2, how='inner')
Out[50]: 
   data1 key  data2
0      0   b      1
1      0   b      3
2      1   b      1
3      1   b      3
4      5   b      1
5      5   b      3
6      2   a      0
7      2   a      2
8      4   a      0
9      4   a      2
```

要根据多个键进行合并，传入一个由列名组成的列表即可：
```python
In [51]: left = pd.DataFrame({'key1': ['foo', 'foo', 'bar'],
   ....:                      'key2': ['one', 'two', 'one'],
   ....:                      'lval': [1, 2, 3]})

In [52]: right = pd.DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],
   ....:                       'key2': ['one', 'one', 'one', 'two'],
   ....:                       'rval': [4, 5, 6, 7]})

In [53]: pd.merge(left, right, on=['key1', 'key2'], how='outer')
Out[53]: 
  key1 key2  lval  rval
0  foo  one   1.0   4.0
1  foo  one   1.0   5.0
2  foo  two   2.0   NaN
3  bar  one   3.0   6.0
4  bar  two   NaN   7.0
```

结果中会出现哪些键组合取决于所选的合并方式，你可以这样来理解：多个键形成一系列元组，并将其当做单个连接键（当然，实际上并不是这么回事）。

>注意：在进行列－列连接时，DataFrame对象中的索引会被丢弃。

对于合并运算需要考虑的最后一个问题是对重复列名的处理。虽然你可以手工处理列名重叠的问题（查看前面介绍的重命名轴标签），但merge有一个更实用的suffixes选项，用于指定附加到左右两个DataFrame对象的重叠列名上的字符串：
```python
In [54]: pd.merge(left, right, on='key1')
Out[54]: 
  key1 key2_x  lval key2_y  rval
0  foo    one     1    one     4
1  foo    one     1    one     5
2  foo    two     2    one     4
3  foo    two     2    one     5
4  bar    one     3    one     6
5  bar    one     3    two     7

In [55]: pd.merge(left, right, on='key1', suffixes=('_left', '_right'))
Out[55]: 
  key1 key2_left  lval key2_right  rval
0  foo       one     1        one     4
1  foo       one     1        one     5
2  foo       two     2        one     4
3  foo       two     2        one     5
4  bar       one     3        one     6
5  bar       one     3        two     7
```

merge的参数请参见表8-2。使用DataFrame的行索引合并是下一节的主题。

表8-2 merge函数的参数

![](http://upload-images.jianshu.io/upload_images/7178691-35ca716a4f1b8475.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/7178691-c86672e733ceccd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

indicator 添加特殊的列_merge，它可以指明每个行的来源，它的值有left_only、right_only或both，根据每行的合并数据的来源。

## 索引上的合并

有时候，DataFrame中的连接键位于其索引中。在这种情况下，你可以传入left_index=True或right_index=True（或两个都传）以说明索引应该被用作连接键：
```python
In [56]: left1 = pd.DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],
   ....:                       'value': range(6)})

In [57]: right1 = pd.DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])

In [58]: left1
Out[58]:

  key  value
0   a      0
1   b      1
2   a      2
3   a      3
4   b      4
5   c      5

In [59]: right1
Out[59]: 
   group_val
a        3.5
b        7.0

In [60]: pd.merge(left1, right1, left_on='key', right_index=True)
Out[60]: 
  key  value  group_val
0   a      0        3.5
2   a      2        3.5
3   a      3        3.5
1   b      1        7.0
4   b      4        7.0
```

由于默认的merge方法是求取连接键的交集，因此你可以通过外连接的方式得到它们的并集：
```python
In [61]: pd.merge(left1, right1, left_on='key', right_index=True, how='outer')
Out[61]: 
  key  value  group_val
0   a      0        3.5
2   a      2        3.5
3   a      3        3.5
1   b      1        7.0
4   b      4        7.0
5   c      5        NaN
```

对于层次化索引的数据，事情就有点复杂了，因为索引的合并默认是多键合并：
```python
In [62]: lefth = pd.DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio',
   ....:                                'Nevada', 'Nevada'],
   ....:                       'key2': [2000, 2001, 2002, 2001, 2002],
   ....:                       'data': np.arange(5.)})

In [63]: righth = pd.DataFrame(np.arange(12).reshape((6, 2)),
   ....:                       index=[['Nevada', 'Nevada', 'Ohio', 'Ohio',
   ....:                               'Ohio', 'Ohio'],
   ....:                              [2001, 2000, 2000, 2000, 2001, 2002]],
   ....:                       columns=['event1', 'event2'])

In [64]: lefth
Out[64]: 
   data    key1  key2
0   0.0    Ohio  2000
1   1.0    Ohio  2001
2   2.0    Ohio  2002
3   3.0  Nevada  2001
4   4.0  Nevada  2002

In [65]: righth
Out[65]: 
             event1  event2
Nevada 2001       0       1
       2000       2       3
Ohio   2000       4       5
       2000       6       7
       2001       8       9
       2002      10      11
```

这种情况下，你必须以列表的形式指明用作合并键的多个列（注意用how='outer'对重复索引值的处理）：
```python
In [66]: pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)
Out[66]: 
   data    key1  key2  event1  event2
0   0.0    Ohio  2000       4       5
0   0.0    Ohio  2000       6       7
1   1.0    Ohio  2001       8       9
2   2.0    Ohio  2002      10      11
3   3.0  Nevada  2001       0       1

In [67]: pd.merge(lefth, righth, left_on=['key1', 'key2'],
   ....:          right_index=True, how='outer')
Out[67]: 
   data    key1  key2  event1  event2
0   0.0    Ohio  2000     4.0     5.0
0   0.0    Ohio  2000     6.0     7.0
1   1.0    Ohio  2001     8.0     9.0
2   2.0    Ohio  2002    10.0    11.0
3   3.0  Nevada  2001     0.0     1.0
4   4.0  Nevada  2002     NaN     NaN
4   NaN  Nevada  2000     2.0     3.0
```

同时使用合并双方的索引也没问题：
```python
In [68]: left2 = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]],
   ....:                      index=['a', 'c', 'e'],
   ....:                      columns=['Ohio', 'Nevada'])

In [69]: right2 = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],
   ....:                       index=['b', 'c', 'd', 'e'],
   ....:                       columns=['Missouri', 'Alabama'])

In [70]: left2
Out[70]: 
   Ohio  Nevada
a   1.0     2.0
c   3.0     4.0
e   5.0     6.0

In [71]: right2
Out[71]: 
   Missouri  Alabama
b       7.0      8.0
c       9.0     10.0
d      11.0     12.0
e      13.0     14.0

In [72]: pd.merge(left2, right2, how='outer', left_index=True, right_index=True)
Out[72]: 
   Ohio  Nevada  Missouri  Alabama
a   1.0     2.0       NaN      NaN
b   NaN     NaN       7.0      8.0
c   3.0     4.0       9.0     10.0
d   NaN     NaN      11.0     12.0
e   5.0     6.0      13.0     14.0
```

DataFrame还有一个便捷的join实例方法，它能更为方便地实现按索引合并。它还可用于合并多个带有相同或相似索引的DataFrame对象，但要求没有重叠的列。在上面那个例子中，我们可以编写：
```python
In [73]: left2.join(right2, how='outer')
Out[73]: 
   Ohio  Nevada  Missouri  Alabama
a   1.0     2.0       NaN      NaN
b   NaN     NaN       7.0      8.0
c   3.0     4.0       9.0     10.0
d   NaN     NaN      11.0     12.0
e   5.0     6.0      13.0     14.0
```

因为一些历史版本的遗留原因，DataFrame的join方法默认使用的是左连接，保留左边表的行索引。它还支持在调用的DataFrame的列上，连接传递的DataFrame索引：
```python
In [74]: left1.join(right1, on='key')
Out[74]: 
  key  value  group_val
0   a      0        3.5
1   b      1        7.0
2   a      2        3.5
3   a      3        3.5
4   b      4        7.0
5   c      5        NaN
```

最后，对于简单的索引合并，你还可以向join传入一组DataFrame，下一节会介绍更为通用的concat函数，也能实现此功能：
```python
In [75]: another = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],
   ....:                        index=['a', 'c', 'e', 'f'],
   ....:                        columns=['New York',
'Oregon'])

In [76]: another
Out[76]: 
   New York  Oregon
a       7.0     8.0
c       9.0    10.0
e      11.0    12.0
f      16.0    17.0

In [77]: left2.join([right2, another])
Out[77]: 
   Ohio  Nevada  Missouri  Alabama  New York  Oregon
a   1.0     2.0       NaN      NaN       7.0     8.0
c   3.0     4.0       9.0     10.0       9.0    10.0
e   5.0     6.0      13.0     14.0      11.0    12.0

In [78]: left2.join([right2, another], how='outer')
Out[78]: 
   Ohio  Nevada  Missouri  Alabama  New York  Oregon
a   1.0     2.0       NaN      NaN       7.0     8.0
b   NaN     NaN       7.0      8.0       NaN     NaN
c   3.0     4.0       9.0     10.0       9.0    10.0
d   NaN     NaN      11.0     12.0       NaN     NaN
e   5.0     6.0      13.0     14.0      11.0    12.0
f   NaN     NaN       NaN      NaN      16.0    17.0
```

## 轴向连接

另一种数据合并运算也被称作连接（concatenation）、绑定（binding）或堆叠（stacking）。NumPy的concatenation函数可以用NumPy数组来做：
```python
In [79]: arr = np.arange(12).reshape((3, 4))

In [80]: arr
Out[80]: 
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])

In [81]: np.concatenate([arr, arr], axis=1)
Out[81]: 
array([[ 0,  1,  2,  3,  0,  1,  2,  3],
       [ 4,  5,  6,  7,  4,  5,  6,  7],
       [ 8,  9, 10, 11,  8,  9, 10, 11]])
```

对于pandas对象（如Series和DataFrame），带有标签的轴使你能够进一步推广数组的连接运算。具体点说，你还需要考虑以下这些东西：

- 如果对象在其它轴上的索引不同，我们应该合并这些轴的不同元素还是只使用交集？
- 连接的数据集是否需要在结果对象中可识别？
- 连接轴中保存的数据是否需要保留？许多情况下，DataFrame默认的整数标签最好在连接时删掉。

pandas的concat函数提供了一种能够解决这些问题的可靠方式。我将给出一些例子来讲解其使用方式。假设有三个没有重叠索引的Series：
```python
In [82]: s1 = pd.Series([0, 1], index=['a', 'b'])

In [83]: s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e'])

In [84]: s3 = pd.Series([5, 6], index=['f', 'g'])
```

对这些对象调用concat可以将值和索引粘合在一起：
```python
In [85]: pd.concat([s1, s2, s3])
Out[85]: 
a    0
b    1
c    2
d    3
e    4
f    5
g    6
dtype: int64
```

默认情况下，concat是在axis=0上工作的，最终产生一个新的Series。如果传入axis=1，则结果就会变成一个DataFrame（axis=1是列）：
```python
In [86]: pd.concat([s1, s2, s3], axis=1)
Out[86]: 
     0    1    2
a  0.0  NaN  NaN
b  1.0  NaN  NaN
c  NaN  2.0  NaN
d  NaN  3.0  NaN
e  NaN  4.0  NaN
f  NaN  NaN  5.0
g  NaN  NaN  6.0
```

这种情况下，另外的轴上没有重叠，从索引的有序并集（外连接）上就可以看出来。传入join='inner'即可得到它们的交集：
```python
In [87]: s4 = pd.concat([s1, s3])

In [88]: s4
Out[88]: 
a    0
b    1
f    5
g    6
dtype: int64

In [89]: pd.concat([s1, s4], axis=1)
Out[89]: 
     0  1
a  0.0  0
b  1.0  1
f  NaN  5
g  NaN  6

In [90]: pd.concat([s1, s4], axis=1, join='inner')
Out[90]: 
   0  1
a  0  0
b  1  1
```

在这个例子中，f和g标签消失了，是因为使用的是join='inner'选项。

你可以通过join_axes指定要在其它轴上使用的索引：
```python
In [91]: pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])
Out[91]: 
     0    1
a  0.0  0.0
c  NaN  NaN
b  1.0  1.0
e  NaN  NaN
```

不过有个问题，参与连接的片段在结果中区分不开。假设你想要在连接轴上创建一个层次化索引。使用keys参数即可达到这个目的：
```python
In [92]: result = pd.concat([s1, s1, s3], keys=['one','two', 'three'])

In [93]: result
Out[93]: 
one    a    0
       b    1
two    a    0
       b    1
three  f    5
       g    6
dtype: int64

In [94]: result.unstack()
Out[94]: 
         a    b    f    g
one    0.0  1.0  NaN  NaN
two    0.0  1.0  NaN  NaN
three  NaN  NaN  5.0  6.0
```

如果沿着axis=1对Series进行合并，则keys就会成为DataFrame的列头：
```python
In [95]: pd.concat([s1, s2, s3], axis=1, keys=['one','two', 'three'])
Out[95]: 
   one  two  three
a  0.0  NaN    NaN
b  1.0  NaN    NaN
c  NaN  2.0    NaN
d  NaN  3.0    NaN
e  NaN  4.0    NaN
f  NaN  NaN    5.0
g  NaN  NaN    6.0
```

同样的逻辑也适用于DataFrame对象：
```python
In [96]: df1 = pd.DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'],
   ....:                    columns=['one', 'two'])

In [97]: df2 = pd.DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'],
   ....:                    columns=['three', 'four'])

In [98]: df1
Out[98]: 
   one  two
a    0    1
b    2    3
c    4    5

In [99]: df2
Out[99]: 
   three  four
a      5     6
c      7     8

In [100]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])
Out[100]: 
  level1     level2     
     one two  three four
a      0   1    5.0  6.0
b      2   3    NaN  NaN
c      4   5    7.0  8.0
```

如果传入的不是列表而是一个字典，则字典的键就会被当做keys选项的值：
```python
In [101]: pd.concat({'level1': df1, 'level2': df2}, axis=1)

Out[101]: 
  level1     level2     
     one two  three four
a      0   1    5.0  6.0
b      2   3    NaN  NaN
c      4   5    7.0  8.0
```

此外还有两个用于管理层次化索引创建方式的参数（参见表8-3）。举个例子，我们可以用names参数命名创建的轴级别：
```python
In [102]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'],
   .....:           names=['upper', 'lower'])
Out[102]: 
upper level1     level2     
lower    one two  three four
a          0   1    5.0  6.0
b          2   3    NaN  NaN
c          4   5    7.0  8.0
```

最后一个关于DataFrame的问题是，DataFrame的行索引不包含任何相关数据：
```python
In [103]: df1 = pd.DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])

In [104]: df2 = pd.DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])

In [105]: df1
Out[105]: 
          a         b         c         d
0  1.246435  1.007189 -1.296221  0.274992
1  0.228913  1.352917  0.886429 -2.001637
2 -0.371843  1.669025 -0.438570 -0.539741

In [106]: df2
Out[106]: 
          b         d         a
0  0.476985  3.248944 -1.021228
1 -0.577087  0.124121  0.302614
```

在这种情况下，传入ignore_index=True即可：
```python
In [107]: pd.concat([df1, df2], ignore_index=True)
Out[107]: 
          a         b         c         d
0  1.246435  1.007189 -1.296221  0.274992
1  0.228913  1.352917  0.886429 -2.001637
2 -0.371843  1.669025 -0.438570 -0.539741
3 -1.021228  0.476985       NaN  3.248944
4  0.302614 -0.577087       NaN  0.124121
```

![表8-3 concat函数的参数](http://upload-images.jianshu.io/upload_images/7178691-339436563b519415.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 合并重叠数据

还有一种数据组合问题不能用简单的合并（merge）或连接（concatenation）运算来处理。比如说，你可能有索引全部或部分重叠的两个数据集。举个有启发性的例子，我们使用NumPy的where函数，它表示一种等价于面向数组的if-else：
```python
In [108]: a = pd.Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],
   .....:               index=['f', 'e', 'd', 'c', 'b', 'a'])

In [109]: b = pd.Series(np.arange(len(a), dtype=np.float64),
   .....:               index=['f', 'e', 'd', 'c', 'b', 'a'])

In [110]: b[-1] = np.nan

In [111]: a
Out[111]: 
f    NaN
e    2.5
d    NaN
c    3.5
b    4.5
a    NaN
dtype: float64

In [112]: b
Out[112]: 
f    0.0
e    1.0
d    2.0
c    3.0
b    4.0
a    NaN
dtype: float64

In [113]: np.where(pd.isnull(a), b, a)
Out[113]: array([ 0. ,  2.5,  2. ,  3.5,  4.5,  nan])
```

Series有一个combine_first方法，实现的也是一样的功能，还带有pandas的数据对齐：
```python
In [114]: b[:-2].combine_first(a[2:])
Out[114]: 
a    NaN
b    4.5
c    3.0
d    2.0
e    1.0
f    0.0
dtype: float64
```

对于DataFrame，combine_first自然也会在列上做同样的事情，因此你可以将其看做：用传递对象中的数据为调用对象的缺失数据“打补丁”：
```python
In [115]: df1 = pd.DataFrame({'a': [1., np.nan, 5., np.nan],
   .....:                     'b': [np.nan, 2., np.nan, 6.],
   .....:                     'c': range(2, 18, 4)})

In [116]: df2 = pd.DataFrame({'a': [5., 4., np.nan, 3., 7.],
   .....:                     'b': [np.nan, 3., 4., 6., 8.]})

In [117]: df1
Out[117]: 
     a    b   c
0  1.0  NaN   2
1  NaN  2.0   6
2  5.0  NaN  10
3  NaN  6.0  14

In [118]: df2
Out[118]: 
     a    b
0  5.0  NaN
1  4.0  3.0
2  NaN  4.0
3  3.0  6.0
4  7.0  8.0

In [119]: df1.combine_first(df2)
Out[119]: 
     a    b     c
0  1.0  NaN   2.0
1  4.0  2.0   6.0
2  5.0  4.0  10.0
3  3.0  6.0  14.0
4  7.0  8.0   NaN
```

# 8.3 重塑和轴向旋转

有许多用于重新排列表格型数据的基础运算。这些函数也称作重塑（reshape）或轴向旋转（pivot）运算。

## 重塑层次化索引

层次化索引为DataFrame数据的重排任务提供了一种具有良好一致性的方式。主要功能有二：

- stack：将数据的列“旋转”为行。
- unstack：将数据的行“旋转”为列。

我将通过一系列的范例来讲解这些操作。接下来看一个简单的DataFrame，其中的行列索引均为字符串数组：
```python
In [120]: data = pd.DataFrame(np.arange(6).reshape((2, 3)),
   .....:                     index=pd.Index(['Ohio','Colorado'], name='state'),
   .....:                     columns=pd.Index(['one', 'two', 'three'],
   .....:                     name='number'))

In [121]: data
Out[121]: 
number    one  two  three
state                    
Ohio        0    1      2
Colorado    3    4      5
```

对该数据使用stack方法即可将列转换为行，得到一个Series：
```python
In [122]: result = data.stack()

In [123]: result
Out[123]: 
state     number
Ohio      one       0
          two       1
          three     2
Colorado  one       3
          two       4
          three     5
dtype: int64
```

对于一个层次化索引的Series，你可以用unstack将其重排为一个DataFrame：
```python
In [124]: result.unstack()
Out[124]: 
number    one  two  three
state                    
Ohio        0    1      2
Colorado    3    4      5
```

默认情况下，unstack操作的是最内层（stack也是如此）。传入分层级别的编号或名称即可对其它级别进行unstack操作：
```python
In [125]: result.unstack(0)
Out[125]: 
state   Ohio  Colorado
number                
one        0         3
two        1         4
three      2         5

In [126]: result.unstack('state')
Out[126]: 
state   Ohio  Colorado
number                
one        0         3
two        1         4
three      2         5
```

如果不是所有的级别值都能在各分组中找到的话，则unstack操作可能会引入缺失数据：
```python
In [127]: s1 = pd.Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])

In [128]: s2 = pd.Series([4, 5, 6], index=['c', 'd', 'e'])

In [129]: data2 = pd.concat([s1, s2], keys=['one', 'two'])

In [130]: data2
Out[130]: 
one  a    0
     b    1
     c    2
     d    3
two  c    4
     d    5
     e    6
dtype: int64

In [131]: data2.unstack()
Out[131]: 
       a    b    c    d    e
one  0.0  1.0  2.0  3.0  NaN
two  NaN  NaN  4.0  5.0  6.0
```

stack默认会滤除缺失数据，因此该运算是可逆的：
```python
In [132]: data2.unstack()
Out[132]: 
       a    b    c    d    e
one  0.0  1.0  2.0  3.0  NaN
two  NaN  NaN  4.0  5.0  6.0

In [133]: data2.unstack().stack()
Out[133]: 
one  a    0.0
     b    1.0
     c    2.0
     d    3.0
two  c    4.0
     d    5.0
     e    6.0
dtype: float64

In [134]: data2.unstack().stack(dropna=False)
Out[134]: 
one  a    0.0
     b    1.0
     c    2.0
     d    3.0
     e    NaN
two  a    NaN
     b    NaN
     c    4.0
     d    5.0
     e    6.0
dtype: float64
```

在对DataFrame进行unstack操作时，作为旋转轴的级别将会成为结果中的最低级别：
```python
In [135]: df = pd.DataFrame({'left': result, 'right': result + 5},
   .....:                   columns=pd.Index(['left', 'right'], name='side'))

In [136]: df
Out[136]: 
side             left  right
state    number             
Ohio     one        0      5
         two        1      6
         three      2      7
Colorado one        3      8
         two        4      9
         three      5     10

In [137]: df.unstack('state')
Out[137]: 
side   left          right
state  Ohio Colorado  Ohio Colorado
number                             
one       0        3     5        8
two       1        4     6        9
three     2        5     7       10
```

当调用stack，我们可以指明轴的名字：
```python
In [138]: df.unstack('state').stack('side')
Out[138]: 
state         Colorado  Ohio
number side                 
one    left          3     0
       right         8     5
two    left          4     1
       right         9     6
three  left          5     2
       right        10     7
```

## 将“长格式”旋转为“宽格式”

多个时间序列数据通常是以所谓的“长格式”（long）或“堆叠格式”（stacked）存储在数据库和CSV中的。我们先加载一些示例数据，做一些时间序列规整和数据清洗：
```python
In [139]: data = pd.read_csv('examples/macrodata.csv')

In [140]: data.head()
Out[140]: 
     year  quarter   realgdp  realcons  realinv  realgovt  realdpi    cpi  \
0  1959.0      1.0  2710.349    1707.4  286.898   470.045   1886.9  28.98   
1  1959.0      2.0  2778.801    1733.7  310.859   481.301   1919.7  29.15   
2  1959.0      3.0  2775.488    1751.8  289.226   491.260   1916.4  29.35   
3  1959.0      4.0  2785.204    1753.7  299.356   484.052   1931.3  29.37   
4  1960.0      1.0  2847.699    1770.5  331.722   462.199   1955.5  29.54   
      m1  tbilrate  unemp      pop  infl  realint  
0  139.7      2.82    5.8  177.146  0.00     0.00
1  141.7      3.08    5.1  177.830  2.34     0.74  
2  140.5      3.82    5.3  178.657  2.74     1.09  
3  140.0      4.33    5.6  179.386  0.27     4.06  
4  139.6      3.50    5.2  180.007  2.31     1.19  

In [141]: periods = pd.PeriodIndex(year=data.year, quarter=data.quarter,
   .....:                          name='date')

In [142]: columns = pd.Index(['realgdp', 'infl', 'unemp'], name='item')

In [143]: data = data.reindex(columns=columns)

In [144]: data.index = periods.to_timestamp('D', 'end')

In [145]: ldata = data.stack().reset_index().rename(columns={0: 'value'})
```

这就是多个时间序列（或者其它带有两个或多个键的可观察数据，这里，我们的键是date和item）的长格式。表中的每行代表一次观察。

关系型数据库（如MySQL）中的数据经常都是这样存储的，因为固定架构（即列名和数据类型）有一个好处：随着表中数据的添加，item列中的值的种类能够增加。在前面的例子中，date和item通常就是主键（用关系型数据库的说法），不仅提供了关系完整性，而且提供了更为简单的查询支持。有的情况下，使用这样的数据会很麻烦，你可能会更喜欢DataFrame，不同的item值分别形成一列，date列中的时间戳则用作索引。DataFrame的pivot方法完全可以实现这个转换：
```python
In [147]: pivoted = ldata.pivot('date', 'item', 'value')

In [148]: pivoted
Out[148]: 
item        infl    realgdp  unemp
date                              
1959-03-31  0.00   2710.349    5.8
1959-06-30  2.34   2778.801    5.1
1959-09-30  2.74   2775.488    5.3
1959-12-31  0.27   2785.204    5.6
1960-03-31  2.31   2847.699    5.2
1960-06-30  0.14   2834.390    5.2
1960-09-30  2.70   2839.022    5.6
1960-12-31  1.21   2802.616    6.3
1961-03-31 -0.40   2819.264    6.8
1961-06-30  1.47   2872.005    7.0
...          ...        ...    ...
2007-06-30  2.75  13203.977    4.5
2007-09-30  3.45  13321.109    4.7
2007-12-31  6.38  13391.249    4.8
2008-03-31  2.82  13366.865    4.9
2008-06-30  8.53  13415.266    5.4
2008-09-30 -3.16  13324.600    6.0
2008-12-31 -8.79  13141.920    6.9
2009-03-31  0.94  12925.410    8.1
2009-06-30  3.37  12901.504    9.2
2009-09-30  3.56  12990.341    9.6
[203 rows x 3 columns]
```

前两个传递的值分别用作行和列索引，最后一个可选值则是用于填充DataFrame的数据列。假设有两个需要同时重塑的数据列：
```python
In [149]: ldata['value2'] = np.random.randn(len(ldata))

In [150]: ldata[:10]
Out[150]: 
        date     item     value    value2
0 1959-03-31  realgdp  2710.349  0.523772
1 1959-03-31     infl     0.000  0.000940
2 1959-03-31    unemp     5.800  1.343810
3 1959-06-30  realgdp  2778.801 -0.713544
4 1959-06-30     infl     2.340 -0.831154
5 1959-06-30    unemp     5.100 -2.370232
6 1959-09-30  realgdp  2775.488 -1.860761
7 1959-09-30     infl     2.740 -0.860757
8 1959-09-30    unemp     5.300  0.560145
9 1959-12-31  realgdp  2785.204 -1.265934
```

如果忽略最后一个参数，得到的DataFrame就会带有层次化的列：
```python
In [151]: pivoted = ldata.pivot('date', 'item')

In [152]: pivoted[:5]
Out[152]: 
           value                    value2                    
item        infl   realgdp unemp      infl   realgdp     unemp
date                                                          
1959-03-31  0.00  2710.349   5.8  0.000940  0.523772  1.343810
1959-06-30  2.34  2778.801   5.1 -0.831154 -0.713544 -2.370232
1959-09-30  2.74  2775.488   5.3 -0.860757 -1.860761  0.560145
1959-12-31  0.27  2785.204   5.6  0.119827 -1.265934 -1.063512
1960-03-31  2.31  2847.699   5.2 -2.359419  0.332883 -0.199543

In [153]: pivoted['value'][:5]
Out[153]: 
item        infl   realgdp  unemp
date                             
1959-03-31  0.00  2710.349    5.8
1959-06-30  2.34  2778.801    5.1
1959-09-30  2.74  2775.488    5.3
1959-12-31  0.27  2785.204    5.6
1960-03-31  2.31  2847.699    5.2
```

注意，pivot其实就是用set_index创建层次化索引，再用unstack重塑：
```python
In [154]: unstacked = ldata.set_index(['date', 'item']).unstack('item')

In [155]: unstacked[:7]
Out[155]: 
           value                    value2                    
item        infl   realgdp unemp      infl   realgdp     unemp
date                                                          
1959-03-31  0.00  2710.349   5.8  0.000940  0.523772  1.343810
1959-06-30  2.34  2778.801   5.1 -0.831154 -0.713544 -2.370232
1959-09-30  2.74  2775.488   5.3 -0.860757 -1.860761  0.560145
1959-12-31  0.27  2785.204   5.6  0.119827 -1.265934 -1.063512
1960-03-31  2.31  2847.699   5.2 -2.359419  0.332883 -0.199543
1960-06-30  0.14  2834.390   5.2 -0.970736 -1.541996 -1.307030
1960-09-30  2.70  2839.022   5.6  0.377984  0.286350 -0.753887
```

## 将“宽格式”旋转为“长格式”

旋转DataFrame的逆运算是pandas.melt。它不是将一列转换到多个新的DataFrame，而是合并多个列成为一个，产生一个比输入长的DataFrame。看一个例子：
```python
In [157]: df = pd.DataFrame({'key': ['foo', 'bar', 'baz'],
   .....:                    'A': [1, 2, 3],
   .....:                    'B': [4, 5, 6],
   .....:                    'C': [7, 8, 9]})

In [158]: df
Out[158]: 
   A  B  C  key
0  1  4  7  foo
1  2  5  8  bar
2  3  6  9  baz
```

key列可能是分组指标，其它的列是数据值。当使用pandas.melt，我们必须指明哪些列是分组指标。下面使用key作为唯一的分组指标：
```python
In [159]: melted = pd.melt(df, ['key'])

In [160]: melted
Out[160]: 
   key variable  value
0  foo        A      1
1  bar        A      2
2  baz        A      3
3  foo        B      4
4  bar        B      5
5  baz        B      6
6  foo        C      7
7  bar        C      8
8  baz        C      9
```

使用pivot，可以重塑回原来的样子：
```python
In [161]: reshaped = melted.pivot('key', 'variable', 'value')

In [162]: reshaped
Out[162]: 
variable  A  B  C
key              
bar       2  5  8
baz       3  6  9
foo       1  4  7
```

因为pivot的结果从列创建了一个索引，用作行标签，我们可以使用reset_index将数据移回列：
```python
In [163]: reshaped.reset_index()
Out[163]: 
variable  key  A  B  C
0         bar  2  5  8
1         baz  3  6  9
2         foo  1  4  7
```

你还可以指定列的子集，作为值的列：
```python
In [164]: pd.melt(df, id_vars=['key'], value_vars=['A', 'B'])
Out[164]: 
   key variable  value
0  foo        A      1
1  bar        A      2
2  baz        A      3
3  foo        B      4
4  bar        B      5
5  baz        B      6
```

pandas.melt也可以不用分组指标：
```python
In [165]: pd.melt(df, value_vars=['A', 'B', 'C'])
Out[165]: 
  variable  value
0        A      1
1        A      2
2        A      3
3        B      4
4        B      5
5        B      6
6        C      7
7        C      8
8        C      9

In [166]: pd.melt(df, value_vars=['key', 'A', 'B'])
Out[166]: 
  variable value
0      key   foo
1      key   bar
2      key   baz
3        A     1
4        A     2
5        A     3
6        B     4
7        B     5
8        B     6
```

#8.4 总结

现在你已经掌握了pandas数据导入、清洗、重塑，我们可以进一步学习matplotlib数据可视化。我们在稍后会回到pandas，学习更高级的分析。
Python是一种广泛使用的编程语言，它的简洁明了的语法使得它成为数据分析的理想工具。在本文中，我们将探讨如何使用Python进行数据分析。

首先，我们需要了解Python的基本数据结构，包括列表、字典和元组。这些数据结构是我们在处理数据时的基础工具。例如，我们可以使用列表来存储一组数据，使用字典来存储键值对，使用元组来存储不可变的数据。

接下来，我们需要学习如何使用Python的数据处理库，如NumPy和Pandas。NumPy是一个强大的数学库，它提供了许多用于处理数组的功能。而Pandas则是一个专门用于数据处理和分析的库，它提供了DataFrame这样的数据结构，可以方便地处理表格数据。

例如，我们可以使用Pandas来读取CSV文件，然后进行各种数据处理操作。以下是一个示例代码：

import pandas as pd

# 读取CSV文件
df = pd.read_csv('data.csv')

# 查看前5行数据
print(df.head())

# 计算每列的平均值
print(df.mean())
此外，我们还需要学习如何使用Python进行数据可视化。Matplotlib是一个常用的数据可视化库，它可以帮助我们创建各种图表，如折线图、柱状图和散点图等。以下是一个示例代码：

import matplotlib.pyplot as plt

# 创建数据
x = [1, 2, 3, 4, 5]
y = [2, 4, 6, 8, 10]

# 创建折线图
plt.plot(x, y)

# 显示图表
plt.show()
最后，我们还可以使用Python进行机器学习。Scikit-learn是一个常用的机器学习库，它提供了大量的机器学习算法，如线性回归、逻辑回归和支持向量机等。以下是一个示例代码：

from sklearn.linear_model import LinearRegression

# 创建数据
X = [[1], [2], [3], [4], [5]]
y = [2, 4, 6, 8, 10]

# 创建线性回归模型
model = LinearRegression()

# 训练模型
model.fit(X, y)

# 预测新数据
print(model.predict([[6]]))
以上就是使用Python进行数据分析的基本步骤。通过学习这些知识，你可以更好地理解和处理数据，从而做出更准确的决策。为了跟随本教程，请确保您的环境中已经安装了以下Python库：
pandas: pip install pandas（用于数据操作）。
numpy: pip install numpy（用于数值计算）。
matplotlib: pip install matplotlib（用于基础图表绘制）。
seaborn: pip install seaborn（用于统计图表）。
scikit-learn: pip install scikit-learn（用于机器学习）。
statsmodels: pip install statsmodels（用于统计建模）。淳朴的民俗民风成为国内独树一帜的旅游胜地。
数据预处理：准备干净的数据集
数据预处理是数据分析的第一步，它包括数据清洗、缺失值处理、重复数据去除等操作，以确保数据的质量和一致性。
加载数据
首先，我们需要加载一个示例数据集来进行分析。
import pandas as pd  

# 加载CSV文件    
data = pd.read_csv('example_data.csv')  

# 查看前几行数据  
print(data.head())  
1
2
3
4
5
6
7
这段代码展示了如何使用`Pandas`库加载CSV文件，并查看前几行数据。
处理缺失值
缺失值可能会对后续分析产生负面影响，因此我们需要对其进行适当的处理。
# 检查缺失值情况  
print(data.isnull().sum())  

# 简单填充缺失值  
data['column_name'].fillna(data['column_name'].mean(), inplace=True)  

# 或者删除含有缺失值的行  
data.dropna(inplace=True)  
1
2
3
4
5
6
7
8
这段代码展示了如何检查数据集中是否存在缺失值，并提供了两种常见的处理方法：填充缺失值或删除含有缺失值的行。
去除重复数据
重复数据可能会导致分析结果失真，因此我们需要对其进行识别和去除。
### 检查是否有重复行   
print(data.duplicated().sum())  

### 删除重复行    
data.drop_duplicates(inplace=True)  
1
2
3
4
5
这段代码展示了如何检查数据集中是否存在重复行，并删除这些重复行。
探索性数据分析（EDA）：揭示数据的秘密
EDA旨在通过对数据进行初步探索，了解其分布、趋势和异常点，为后续分析提供方向。
描述性统计
描述性统计可以帮助我们快速了解数据的基本特征。
### 计算描述性统计量    
print(data.describe())   
 
### 计算分类变量的频数分布    
print(data['category_column'].value_counts())  
1
2
3
4
5
这段代码展示了如何计算数值型变量的描述性统计量以及分类变量的频数分布。
可视化分析
可视化是EDA的重要组成部分，它可以通过图形直观展示数据的特点。
import matplotlib.pyplot as plt    
import seaborn as sns   

### 绘制直方图    
sns.histplot(data['numeric_column'], kde=True)    
plt.title('Histogram of Numeric Column')    
plt.show()  

# 绘制箱形图    
sns.boxplot(x='category_column', y='numeric_column', data=data)    
plt.title('Box Plot by Category')    
plt.show()  
1
2
3
4
5
6
7
8
9
10
11
12
这段代码展示了如何使用`Seaborn`库绘制直方图和箱形图，以展示数值型变量的分布情况及不同类别之间的差异。
特征工程：提升模型性能
特征工程是指通过对原始数据进行转换和组合，创建新的特征来改进模型的表现。
创建新特征
根据业务逻辑或领域知识，我们可以创建一些新的特征来捕捉潜在的信息。
### 创建一个表示年龄区间的特征   
data['age_group'] = pd.cut(data['age'], bins=[0, 18, 35, 60, 100], labels=['child', 'young_adult', 'adult', 'senior'])  
  
### 创建一个表示是否为周末的特征    
data['is_weekend'] = (data['date'].dt.weekday >= 5).astype(int)  
1
2
3
4
5
这段代码展示了如何基于现有数据创建两个新的特征：`age_group`和`is_weekend`。
特征缩放
不同的特征可能具有不同的量纲，这会影响某些算法的效果。因此，我们通常需要对特征进行标准化或归一化处理。
from sklearn.preprocessing 
import StandardScaler    

### 初始化标准化器    
scaler = StandardScaler() 
   
### 对选定的列进行标准化  
data[['feature1', 'feature2']] = scaler.fit_transform(data[['feature1', 'feature2']])    
1
2
3
4
5
6
7
8
这段代码展示了如何使用`StandardScaler`对选定的特征进行标准化处理。
模型构建与评估：验证假设
一旦完成了数据预处理和特征工程，下一步就是构建模型并评估其性能。
分割数据集
为了评估模型的真实表现，我们需要将数据集分为训练集和测试集。
from sklearn.model_selection 
import train_test_split   
  
### 分割数据集   
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  
1
2
3
4
5
这段代码展示了如何使用`train_test_split`函数将数据集随机划分为训练集和测试集。
构建和训练模型
接下来，我们可以选择合适的机器学习算法来构建模型，并使用训练集对其进行训练。
from sklearn.linear_model 
import LogisticRegression  
  
# 初始化模型    
model = LogisticRegression()   
  
### 训练模型    
model.fit(X_train, y_train)  
1
2
3
4
5
6
7
8
这段代码展示了如何使用`LogisticRegression`类构建一个逻辑回归模型，并使用训练集对其进行训练。
评估模型性能
最后，我们需要使用测试集评估模型的性能，以确定其泛化能力。
from sklearn.metrics
import accuracy_score, classification_report    
    
### 预测测试集    
y_pred = model.predict(X_test)  
  
### 计算准确率  
accuracy = accuracy_score(y_test, y_pred)   
print(f"Accuracy: {accuracy:.2f}")    
  
### 打印分类报告    
print(classification_report(y_test, y_pred))  
1
2
3
4
5
6
7
8
9
10
11
12
这段代码展示了如何计算模型的准确率，并打印详细的分类报告，包括精确度、召回率和F1分数等指标。
实战案例：预测客户流失
让我们通过一个实战案例来巩固所学知识。假设我们要构建一个预测客户流失的模型，以帮助企业提前采取措施挽留即将流失的客户。
数据准备与预处理
首先，我们需要加载和预处理客户数据。
### 加载客户数据    
customer_data = pd.read_csv('customer_data.csv')    
# 数据清洗和转换...  
1
2
3
探索性数据分析
接下来，我们将进行初步的探索性数据分析，了解客户数据的基本特征和分布情况。
### 描述性统计和可视化...  
1
特征工程
然后，我们将进行特征工程，创建有助于预测的新特征，并对现有特征进行适当转换。
### 创建新特征和特征缩放...  
1
模型构建与评估
最后，我们将构建多个候选模型，并使用交叉验证等技术评估它们的性能，选择最优模型。
from sklearn.ensemble import RandomForestClassifier    
from sklearn.model_selection import cross_val_score  

### 初始化随机森林分类器    
rf_model = RandomForestClassifier()  

### 使用交叉验证评估模型性能    
scores = cross_val_score(rf_model, X, y, cv=5)    
print(f"Cross-validation scores: {scores}")    
print(f"Average score: {scores.mean():.2f}")  
1
2
3
4
5
6
7
8
9
10
这段代码展示了如何使用`RandomForestClassifier`构建一个随机森林模型，并通过交叉验证评估其性能。
总结与展望
在这篇文章中，我们不仅介绍了如何使用Python进行数据分析的关键步骤，还通过具体的例子让您亲身体验了从数据预处理到模型评估的整个过程。
信息可视化（也叫绘图）是数据分析中最重要的工作之一。它可能是探索过程的一部分，例如，帮助我们找出异常值、必要的数据转换、得出有关模型的idea等。另外，做一个可交互的数据可视化也许是工作的最终目标。Python有许多库进行静态或动态的数据可视化，但我这里重要关注于matplotlib（http://matplotlib.org/）和基于它的库。

matplotlib是一个用于创建出版质量图表的桌面绘图包（主要是2D方面）。该项目是由John Hunter于2002年启动的，其目的是为Python构建一个MATLAB式的绘图接口。matplotlib和IPython社区进行合作，简化了从IPython shell（包括现在的Jupyter notebook）进行交互式绘图。matplotlib支持各种操作系统上许多不同的GUI后端，而且还能将图片导出为各种常见的矢量（vector）和光栅（raster）图：PDF、SVG、JPG、PNG、BMP、GIF等。除了几张，本书中的大部分图都是用它生成的。

随着时间的发展，matplotlib衍生出了多个数据可视化的工具集，它们使用matplotlib作为底层。其中之一是seaborn（http://seaborn.pydata.org/），本章后面会学习它。

学习本章代码案例的最简单方法是在Jupyter notebook进行交互式绘图。在Jupyter notebook中执行下面的语句：
```python
%matplotlib notebook
```

# 9.1 matplotlib API入门

 matplotlib的通常引入约定是：

```python
In [11]: import matplotlib.pyplot as plt
```

在Jupyter中运行%matplotlib notebook（或在IPython中运行%matplotlib），就可以创建一个简单的图形。如果一切设置正确，会看到图9-1：

```python
In [12]: import numpy as np

In [13]: data = np.arange(10)

In [14]: data
Out[14]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

In [15]: plt.plot(data)
```

![图9-1 简单的线图](http://upload-images.jianshu.io/upload_images/7178691-7032e333a6ecdd37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

虽然seaborn这样的库和pandas的内置绘图函数能够处理许多普通的绘图任务，但如果需要自定义一些高级功能的话就必须学习matplotlib API。

>笔记：虽然本书没有详细地讨论matplotlib的各种功能，但足以将你引入门。matplotlib的示例库和文档是学习高级特性的最好资源。

## Figure和Subplot

matplotlib的图像都位于Figure对象中。你可以用plt.figure创建一个新的Figure：

```python
In [16]: fig = plt.figure()
```

如果用的是IPython，这时会弹出一个空窗口，但在Jupyter中，必须再输入更多命令才能看到。plt.figure有一些选项，特别是figsize，它用于确保当图片保存到磁盘时具有一定的大小和纵横比。

不能通过空Figure绘图。必须用add_subplot创建一个或多个subplot才行：

```python
In [17]: ax1 = fig.add_subplot(2, 2, 1)
```

这条代码的意思是：图像应该是2×2的（即最多4张图），且当前选中的是4个subplot中的第一个（编号从1开始）。如果再把后面两个subplot也创建出来，最终得到的图像如图9-2所示：

```python
In [18]: ax2 = fig.add_subplot(2, 2, 2)

In [19]: ax3 = fig.add_subplot(2, 2, 3)
```

![图9-2 带有三个subplot的Figure](http://upload-images.jianshu.io/upload_images/7178691-b8cff158e64eae74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>提示：使用Jupyter notebook有一点不同，即每个小窗重新执行后，图形会被重置。因此，对于复杂的图形，，你必须将所有的绘图命令存在一个小窗里。

这里，我们运行同一个小窗里的所有命令：

```python 
fig = plt.figure()
ax1 = fig.add_subplot(2, 2, 1)
ax2 = fig.add_subplot(2, 2, 2)
ax3 = fig.add_subplot(2, 2, 3)
```

如果这时执行一条绘图命令（如plt.plot([1.5, 3.5, -2, 1.6])），matplotlib就会在最后一个用过的subplot（如果没有则创建一个）上进行绘制，隐藏创建figure和subplot的过程。因此，如果我们执行下列命令，你就会得到如图9-3所示的结果：

```python
In [20]: plt.plot(np.random.randn(50).cumsum(), 'k--')
```

![图9-3 绘制一次之后的图像](http://upload-images.jianshu.io/upload_images/7178691-7bcbd5e56fdbbd92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

"k--"是一个线型选项，用于告诉matplotlib绘制黑色虚线图。上面那些由fig.add_subplot所返回的对象是AxesSubplot对象，直接调用它们的实例方法就可以在其它空着的格子里面画图了，如图9-4所示：

```python
In [21]: ax1.hist(np.random.randn(100), bins=20, color='k', alpha=0.3)

In [22]: ax2.scatter(np.arange(30), np.arange(30) + 3 * np.random.randn(30))
```

![图9-4 继续绘制两次之后的图像](http://upload-images.jianshu.io/upload_images/7178691-2297bcaf355db24c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

你可以在matplotlib的文档中找到各种图表类型。

创建包含subplot网格的figure是一个非常常见的任务，matplotlib有一个更为方便的方法plt.subplots，它可以创建一个新的Figure，并返回一个含有已创建的subplot对象的NumPy数组：

```python
In [24]: fig, axes = plt.subplots(2, 3)

In [25]: axes
Out[25]: 
array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fb626374048>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb62625db00>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb6262f6c88>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fb6261a36a0>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb626181860>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x7fb6260fd4e0>]], dtype
=object)
```

这是非常实用的，因为可以轻松地对axes数组进行索引，就好像是一个二维数组一样，例如axes[0,1]。你还可以通过sharex和sharey指定subplot应该具有相同的X轴或Y轴。在比较相同范围的数据时，这也是非常实用的，否则，matplotlib会自动缩放各图表的界限。有关该方法的更多信息，请参见表9-1。

![表9-1 pyplot.subplots的选项](http://upload-images.jianshu.io/upload_images/7178691-88bb55faca7d01ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 调整subplot周围的间距

默认情况下，matplotlib会在subplot外围留下一定的边距，并在subplot之间留下一定的间距。间距跟图像的高度和宽度有关，因此，如果你调整了图像大小（不管是编程还是手工），间距也会自动调整。利用Figure的subplots_adjust方法可以轻而易举地修改间距，此外，它也是个顶级函数：

```python
subplots_adjust(left=None, bottom=None, right=None, top=None,
                wspace=None, hspace=None)
```

wspace和hspace用于控制宽度和高度的百分比，可以用作subplot之间的间距。下面是一个简单的例子，其中我将间距收缩到了0（如图9-5所示）：

```python
fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
    for j in range(2):
        axes[i, j].hist(np.random.randn(500), bins=50, color='k', alpha=0.5)
plt.subplots_adjust(wspace=0, hspace=0)
```

![图9-5 各subplot之间没有间距](http://upload-images.jianshu.io/upload_images/7178691-80be7ffc3dec88a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

不难看出，其中的轴标签重叠了。matplotlib不会检查标签是否重叠，所以对于这种情况，你只能自己设定刻度位置和刻度标签。后面几节将会详细介绍该内容。

## 颜色、标记和线型

matplotlib的plot函数接受一组X和Y坐标，还可以接受一个表示颜色和线型的字符串缩写。例如，要根据x和y绘制绿色虚线，你可以执行如下代码：

```python
ax.plot(x, y, 'g--')
```

这种在一个字符串中指定颜色和线型的方式非常方便。在实际中，如果你是用代码绘图，你可能不想通过处理字符串来获得想要的格式。通过下面这种更为明确的方式也能得到同样的效果：

```python
ax.plot(x, y, linestyle='--', color='g')
```

常用的颜色可以使用颜色缩写，你也可以指定颜色码（例如，'#CECECE'）。你可以通过查看plot的文档字符串查看所有线型的合集（在IPython和Jupyter中使用plot?）。

线图可以使用标记强调数据点。因为matplotlib可以创建连续线图，在点之间进行插值，因此有时可能不太容易看出真实数据点的位置。标记也可以放到格式字符串中，但标记类型和线型必须放在颜色后面（见图9-6）：

```python
In [30]: from numpy.random import randn

In [31]: plt.plot(randn(30).cumsum(), 'ko--')
```

![图9-6 带有标记的线型图示例](http://upload-images.jianshu.io/upload_images/7178691-404d816f3e1d6621.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

还可以将其写成更为明确的形式：

```python
plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')
```

在线型图中，非实际数据点默认是按线性方式插值的。可以通过drawstyle选项修改（见图9-7）：

```python
In [33]: data = np.random.randn(30).cumsum()

In [34]: plt.plot(data, 'k--', label='Default')
Out[34]: [<matplotlib.lines.Line2D at 0x7fb624d86160>]

In [35]: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
Out[35]: [<matplotlib.lines.Line2D at 0x7fb624d869e8>]

In [36]: plt.legend(loc='best')
```

![图9-7 不同drawstyle选项的线型图](http://upload-images.jianshu.io/upload_images/7178691-3ec7642e1a592f08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

你可能注意到运行上面代码时有输出<matplotlib.lines.Line2D at ...>。matplotlib会返回引用了新添加的子组件的对象。大多数时候，你可以放心地忽略这些输出。这里，因为我们传递了label参数到plot，我们可以创建一个plot图例，指明每条使用plt.legend的线。

>笔记：你必须调用plt.legend（或使用ax.legend，如果引用了轴的话）来创建图例，无论你绘图时是否传递label标签选项。

## 刻度、标签和图例

对于大多数的图表装饰项，其主要实现方式有二：使用过程型的pyplot接口（例如，matplotlib.pyplot）以及更为面向对象的原生matplotlib API。

pyplot接口的设计目的就是交互式使用，含有诸如xlim、xticks和xticklabels之类的方法。它们分别控制图表的范围、刻度位置、刻度标签等。其使用方式有以下两种：

- 调用时不带参数，则返回当前的参数值（例如，plt.xlim()返回当前的X轴绘图范围）。
- 调用时带参数，则设置参数值（例如，plt.xlim([0,10])会将X轴的范围设置为0到10）。

所有这些方法都是对当前或最近创建的AxesSubplot起作用的。它们各自对应subplot对象上的两个方法，以xlim为例，就是ax.get_xlim和ax.set_xlim。我更喜欢使用subplot的实例方法（因为我喜欢明确的事情，而且在处理多个subplot时这样也更清楚一些）。当然你完全可以选择自己觉得方便的那个。

## 设置标题、轴标签、刻度以及刻度标签

为了说明自定义轴，我将创建一个简单的图像并绘制一段随机漫步（如图9-8所示）：

```python
In [37]: fig = plt.figure()

In [38]: ax = fig.add_subplot(1, 1, 1)

In [39]: ax.plot(np.random.randn(1000).cumsum())
```

![图9-8 用于演示xticks的简单线型图（带有标签）](http://upload-images.jianshu.io/upload_images/7178691-caf9300dacb61fa4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

要改变x轴刻度，最简单的办法是使用set_xticks和set_xticklabels。前者告诉matplotlib要将刻度放在数据范围中的哪些位置，默认情况下，这些位置也就是刻度标签。但我们可以通过set_xticklabels将任何其他的值用作标签：

```python
In [40]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])

In [41]: labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'],
   ....:                             rotation=30, fontsize='small')
```

rotation选项设定x刻度标签倾斜30度。最后，再用set_xlabel为X轴设置一个名称，并用set_title设置一个标题（见图9-9的结果）：

```python
In [42]: ax.set_title('My first matplotlib plot')
Out[42]: <matplotlib.text.Text at 0x7fb624d055f8>

In [43]: ax.set_xlabel('Stages')
```

![图9-9 用于演示xticks的简单线型图](http://upload-images.jianshu.io/upload_images/7178691-741f968323bd818f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Y轴的修改方式与此类似，只需将上述代码中的x替换为y即可。轴的类有集合方法，可以批量设定绘图选项。前面的例子，也可以写为：

```python
props = {
    'title': 'My first matplotlib plot',
    'xlabel': 'Stages'
}
ax.set(**props)
```

## 添加图例

图例（legend）是另一种用于标识图表元素的重要工具。添加图例的方式有多种。最简单的是在添加subplot的时候传入label参数：

```python
In [44]: from numpy.random import randn

In [45]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)

In [46]: ax.plot(randn(1000).cumsum(), 'k', label='one')
Out[46]: [<matplotlib.lines.Line2D at 0x7fb624bdf860>]

In [47]: ax.plot(randn(1000).cumsum(), 'k--', label='two')
Out[47]: [<matplotlib.lines.Line2D at 0x7fb624be90f0>]

In [48]: ax.plot(randn(1000).cumsum(), 'k.', label='three')
Out[48]: [<matplotlib.lines.Line2D at 0x7fb624be9160>]
```

在此之后，你可以调用ax.legend()或plt.legend()来自动创建图例（结果见图9-10）：

```python
In [49]: ax.legend(loc='best')
```

![图9-10 带有三条线以及图例的简单线型图](http://upload-images.jianshu.io/upload_images/7178691-651ff89750c0a89b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

legend方法有几个其它的loc位置参数选项。请查看文档字符串（使用ax.legend?）。

loc告诉matplotlib要将图例放在哪。如果你不是吹毛求疵的话，"best"是不错的选择，因为它会选择最不碍事的位置。要从图例中去除一个或多个元素，不传入label或传入label='_nolegend_'即可。（中文第一版这里把best错写成了beat）

## 注解以及在Subplot上绘图

除标准的绘图类型，你可能还希望绘制一些子集的注解，可能是文本、箭头或其他图形等。注解和文字可以通过text、arrow和annotate函数进行添加。text可以将文本绘制在图表的指定坐标(x,y)，还可以加上一些自定义格式：

```python
ax.text(x, y, 'Hello world!',
        family='monospace', fontsize=10)
```

注解中可以既含有文本也含有箭头。例如，我们根据最近的标准普尔500指数价格（来自Yahoo!Finance）绘制一张曲线图，并标出2008年到2009年金融危机期间的一些重要日期。你可以在Jupyter notebook的一个小窗中试验这段代码（图9-11是结果）：

```python
from datetime import datetime

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

data = pd.read_csv('examples/spx.csv', index_col=0, parse_dates=True)
spx = data['SPX']

spx.plot(ax=ax, style='k-')

crisis_data = [
    (datetime(2007, 10, 11), 'Peak of bull market'),
    (datetime(2008, 3, 12), 'Bear Stearns Fails'),
    (datetime(2008, 9, 15), 'Lehman Bankruptcy')
]

for date, label in crisis_data:
    ax.annotate(label, xy=(date, spx.asof(date) + 75),
                xytext=(date, spx.asof(date) + 225),
                arrowprops=dict(facecolor='black', headwidth=4, width=2,
                                headlength=4),
                horizontalalignment='left', verticalalignment='top')

# Zoom in on 2007-2010
ax.set_xlim(['1/1/2007', '1/1/2011'])
ax.set_ylim([600, 1800])

ax.set_title('Important dates in the 2008-2009 financial crisis')
```

![图9-11 2008-2009年金融危机期间的重要日期](http://upload-images.jianshu.io/upload_images/7178691-3127eaa51f5e4c2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这张图中有几个重要的点要强调：ax.annotate方法可以在指定的x和y坐标轴绘制标签。我们使用set_xlim和set_ylim人工设定起始和结束边界，而不使用matplotlib的默认方法。最后，用ax.set_title添加图标标题。

更多有关注解的示例，请访问matplotlib的在线示例库。

图形的绘制要麻烦一些。matplotlib有一些表示常见图形的对象。这些对象被称为块（patch）。其中有些（如Rectangle和Circle），可以在matplotlib.pyplot中找到，但完整集合位于matplotlib.patches。

要在图表中添加一个图形，你需要创建一个块对象shp，然后通过ax.add_patch(shp)将其添加到subplot中（如图9-12所示）：

```python
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)
pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],
                   color='g', alpha=0.5)

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)
```

![图9-12 由三个块图形组成的图](http://upload-images.jianshu.io/upload_images/7178691-1f8a3d7a3a02d7d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


如果查看许多常见图表对象的具体实现代码，你就会发现它们其实就是由块patch组装而成的。

## 将图表保存到文件

利用plt.savefig可以将当前图表保存到文件。该方法相当于Figure对象的实例方法savefig。例如，要将图表保存为SVG文件，你只需输入：

```python
plt.savefig('figpath.svg')
```

文件类型是通过文件扩展名推断出来的。因此，如果你使用的是.pdf，就会得到一个PDF文件。我在发布图片时最常用到两个重要的选项是dpi（控制“每英寸点数”分辨率）和bbox_inches（可以剪除当前图表周围的空白部分）。要得到一张带有最小白边且分辨率为400DPI的PNG图片，你可以：

```python
plt.savefig('figpath.png', dpi=400, bbox_inches='tight')
```

savefig并非一定要写入磁盘，也可以写入任何文件型的对象，比如BytesIO：

```python
from io import BytesIO
buffer = BytesIO()
plt.savefig(buffer)
plot_data = buffer.getvalue()
```

表9-2列出了savefig的其它选项。

![表9-2 Figure.savefig的选项](http://upload-images.jianshu.io/upload_images/7178691-4bee796bf7262423.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## matplotlib配置

matplotlib自带一些配色方案，以及为生成出版质量的图片而设定的默认配置信息。幸运的是，几乎所有默认行为都能通过一组全局参数进行自定义，它们可以管理图像大小、subplot边距、配色方案、字体大小、网格类型等。一种Python编程方式配置系统的方法是使用rc方法。例如，要将全局的图像默认大小设置为10×10，你可以执行：

```python
plt.rc('figure', figsize=(10, 10))
```

rc的第一个参数是希望自定义的对象，如'figure'、'axes'、'xtick'、'ytick'、'grid'、'legend'等。其后可以跟上一系列的关键字参数。一个简单的办法是将这些选项写成一个字典：

```python
font_options = {'family' : 'monospace',
                'weight' : 'bold',
                'size'   : 'small'}
plt.rc('font', **font_options)
```

要了解全部的自定义选项，请查阅matplotlib的配置文件matplotlibrc（位于matplotlib/mpl-data目录中）。如果对该文件进行了自定义，并将其放在你自己的.matplotlibrc目录中，则每次使用matplotlib时就会加载该文件。

下一节，我们会看到，seaborn包有若干内置的绘图主题或类型，它们使用了matplotlib的内部配置。

# 9.2 使用pandas和seaborn绘图

matplotlib实际上是一种比较低级的工具。要绘制一张图表，你组装一些基本组件就行：数据展示（即图表类型：线型图、柱状图、盒形图、散布图、等值线图等）、图例、标题、刻度标签以及其他注解型信息。

在pandas中，我们有多列数据，还有行和列标签。pandas自身就有内置的方法，用于简化从DataFrame和Series绘制图形。另一个库seaborn（https://seaborn.pydata.org/），由Michael Waskom创建的静态图形库。Seaborn简化了许多常见可视类型的创建。

>提示：引入seaborn会修改matplotlib默认的颜色方案和绘图类型，以提高可读性和美观度。即使你不使用seaborn API，你可能也会引入seaborn，作为提高美观度和绘制常见matplotlib图形的简化方法。

## 线型图

Series和DataFrame都有一个用于生成各类图表的plot方法。默认情况下，它们所生成的是线型图（如图9-13所示）：

```python
In [60]: s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))

In [61]: s.plot()
```

![图9-13 简单的Series图表示例](http://upload-images.jianshu.io/upload_images/7178691-f28e5ab2ac94c7a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

该Series对象的索引会被传给matplotlib，并用以绘制X轴。可以通过use_index=False禁用该功能。X轴的刻度和界限可以通过xticks和xlim选项进行调节，Y轴就用yticks和ylim。plot参数的完整列表请参见表9-3。我只会讲解其中几个，剩下的就留给读者自己去研究了。


![](http://upload-images.jianshu.io/upload_images/7178691-6d9fbf863c09370a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![表9-3 Series.plot方法的参数](http://upload-images.jianshu.io/upload_images/7178691-44e50562aeb5eb49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

pandas的大部分绘图方法都有一个可选的ax参数，它可以是一个matplotlib的subplot对象。这使你能够在网格布局中更为灵活地处理subplot的位置。

DataFrame的plot方法会在一个subplot中为各列绘制一条线，并自动创建图例（如图9-14所示）：
```python
In [62]: df = pd.DataFrame(np.random.randn(10, 4).cumsum(0),
   ....:                   columns=['A', 'B', 'C', 'D'],
   ....:                   index=np.arange(0, 100, 10))

In [63]: df.plot()
```

![图9-14 简单的DataFrame绘图](http://upload-images.jianshu.io/upload_images/7178691-a1234d5e5ee41a40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

plot属性包含一批不同绘图类型的方法。例如，df.plot()等价于df.plot.line()。后面会学习这些方法。

>笔记：plot的其他关键字参数会被传给相应的matplotlib绘图函数，所以要更深入地自定义图表，就必须学习更多有关matplotlib API的知识。

DataFrame还有一些用于对列进行灵活处理的选项，例如，是要将所有列都绘制到一个subplot中还是创建各自的subplot。详细信息请参见表9-4。

![表9-4 专用于DataFrame的plot参数](http://upload-images.jianshu.io/upload_images/7178691-96651ecaa90f1c68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>注意： 有关时间序列的绘图，请见第11章。

## 柱状图

plot.bar()和plot.barh()分别绘制水平和垂直的柱状图。这时，Series和DataFrame的索引将会被用作X（bar）或Y（barh）刻度（如图9-15所示）：

```python
In [64]: fig, axes = plt.subplots(2, 1)

In [65]: data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))

In [66]: data.plot.bar(ax=axes[0], color='k', alpha=0.7)
Out[66]: <matplotlib.axes._subplots.AxesSubplot at 0x7fb62493d470>

In [67]: data.plot.barh(ax=axes[1], color='k', alpha=0.7)
```

![图9-15 水平和垂直的柱状图](http://upload-images.jianshu.io/upload_images/7178691-cd54c7ccfa3f0687.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

color='k'和alpha=0.7设定了图形的颜色为黑色，并使用部分的填充透明度。对于DataFrame，柱状图会将每一行的值分为一组，并排显示，如图9-16所示：

```python
In [69]: df = pd.DataFrame(np.random.rand(6, 4),
   ....:                   index=['one', 'two', 'three', 'four', 'five', 'six'],
   ....:                   columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))

In [70]: df
Out[70]: 
Genus         A         B         C         D
one    0.370670  0.602792  0.229159  0.486744
two    0.420082  0.571653  0.049024  0.880592
three  0.814568  0.277160  0.880316  0.431326
four   0.374020  0.899420  0.460304  0.100843
five   0.433270  0.125107  0.494675  0.961825
six    0.601648  0.478576  0.205690  0.560547

In [71]: df.plot.bar()
```

![图9-16 DataFrame的柱状图](http://upload-images.jianshu.io/upload_images/7178691-bfc141acb37d99b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注意，DataFrame各列的名称"Genus"被用作了图例的标题。

设置stacked=True即可为DataFrame生成堆积柱状图，这样每行的值就会被堆积在一起（如图9-17所示）：

```python
In [73]: df.plot.barh(stacked=True, alpha=0.5)
```

![图9-17 DataFrame的堆积柱状图](http://upload-images.jianshu.io/upload_images/7178691-c19e4246eb897978.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>笔记：柱状图有一个非常不错的用法：利用value_counts图形化显示Series中各值的出现频率，比如s.value_counts().plot.bar()。

再以本书前面用过的那个有关小费的数据集为例，假设我们想要做一张堆积柱状图以展示每天各种聚会规模的数据点的百分比。我用read_csv将数据加载进来，然后根据日期和聚会规模创建一张交叉表：

```python
In [75]: tips = pd.read_csv('examples/tips.csv')

In [76]: party_counts = pd.crosstab(tips['day'], tips['size'])

In [77]: party_counts
Out[77]: 
size  1   2   3   4  5  6
day                      
Fri   1  16   1   1  0  0
Sat   2  53  18  13  1  0
Sun   0  39  15  18  3  1
Thur  1  48   4   5  1  3

# Not many 1- and 6-person parties
In [78]: party_counts = party_counts.loc[:, 2:5]
```

然后进行规格化，使得各行的和为1，并生成图表（如图9-18所示）：

```python
# Normalize to sum to 1
In [79]: party_pcts = party_counts.div(party_counts.sum(1), axis=0)

In [80]: party_pcts
Out[80]: 
size         2         3         4         5
day                                         
Fri   0.888889  0.055556  0.055556  0.000000
Sat   0.623529  0.211765  0.152941  0.011765
Sun   0.520000  0.200000  0.240000  0.040000
Thur  0.827586  0.068966  0.086207  0.017241

In [81]: party_pcts.plot.bar()
```

![图9-18 每天各种聚会规模的比例](http://upload-images.jianshu.io/upload_images/7178691-2918f67936823834.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

于是，通过该数据集就可以看出，聚会规模在周末会变大。

对于在绘制一个图形之前，需要进行合计的数据，使用seaborn可以减少工作量。用seaborn来看每天的小费比例（图9-19是结果）：

```python
In [83]: import seaborn as sns

In [84]: tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])

In [85]: tips.head()
Out[85]: 
   total_bill   tip smoker  day    time  size   tip_pct
0       16.99  1.01     No  Sun  Dinner     2  0.063204
1       10.34  1.66     No  Sun  Dinner     3  0.191244
2       21.01  3.50     No  Sun  Dinner     3  0.199886
3       23.68  3.31     No  Sun  Dinner     2  0.162494
4       24.59  3.61     No  Sun  Dinner     4  0.172069

In [86]: sns.barplot(x='tip_pct', y='day', data=tips, orient='h')
```

![图9-19 小费的每日比例，带有误差条](http://upload-images.jianshu.io/upload_images/7178691-c33e8b3add99904b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

seaborn的绘制函数使用data参数，它可能是pandas的DataFrame。其它的参数是关于列的名字。因为一天的每个值有多次观察，柱状图的值是tip_pct的平均值。绘制在柱状图上的黑线代表95%置信区间（可以通过可选参数配置）。

seaborn.barplot有颜色选项，使我们能够通过一个额外的值设置（见图9-20）：

```python
In [88]: sns.barplot(x='tip_pct', y='day', hue='time', data=tips, orient='h')
```

![图9-20 根据天和时间的小费比例](http://upload-images.jianshu.io/upload_images/7178691-06abe2f070222115.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注意，seaborn已经自动修改了图形的美观度：默认调色板，图形背景和网格线的颜色。你可以用seaborn.set在不同的图形外观之间切换：

```python
In [90]: sns.set(style="whitegrid")
```

## 直方图和密度图

直方图（histogram）是一种可以对值频率进行离散化显示的柱状图。数据点被拆分到离散的、间隔均匀的面元中，绘制的是各面元中数据点的数量。再以前面那个小费数据为例，通过在Series使用plot.hist方法，我们可以生成一张“小费占消费总额百分比”的直方图（如图9-21所示）：
```python
In [92]: tips['tip_pct'].plot.hist(bins=50)
```

![图9-21 小费百分比的直方图](http://upload-images.jianshu.io/upload_images/7178691-255279376f7649a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

与此相关的一种图表类型是密度图，它是通过计算“可能会产生观测数据的连续概率分布的估计”而产生的。一般的过程是将该分布近似为一组核（即诸如正态分布之类的较为简单的分布）。因此，密度图也被称作KDE（Kernel Density Estimate，核密度估计）图。使用plot.kde和标准混合正态分布估计即可生成一张密度图（见图9-22）：
```python
In [94]: tips['tip_pct'].plot.density()
```

![图9-22  小费百分比的密度图](http://upload-images.jianshu.io/upload_images/7178691-ee929d033159516a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

seaborn的distplot方法绘制直方图和密度图更加简单，还可以同时画出直方图和连续密度估计图。作为例子，考虑一个双峰分布，由两个不同的标准正态分布组成（见图9-23）：

```python
In [96]: comp1 = np.random.normal(0, 1, size=200)

In [97]: comp2 = np.random.normal(10, 2, size=200)

In [98]: values = pd.Series(np.concatenate([comp1, comp2]))

In [99]: sns.distplot(values, bins=100, color='k')
```

![图9-23 标准混合密度估计的标准直方图](http://upload-images.jianshu.io/upload_images/7178691-975f04d750c4efe2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 散布图或点图

点图或散布图是观察两个一维数据序列之间的关系的有效手段。在下面这个例子中，我加载了来自statsmodels项目的macrodata数据集，选择了几个变量，然后计算对数差：

```python
In [100]: macro = pd.read_csv('examples/macrodata.csv')

In [101]: data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]

In [102]: trans_data = np.log(data).diff().dropna()

In [103]: trans_data[-5:]
Out[103]: 
          cpi        m1  tbilrate     unemp
198 -0.007904  0.045361 -0.396881  0.105361
199 -0.021979  0.066753 -2.277267  0.139762
200  0.002340  0.010286  0.606136  0.160343
201  0.008419  0.037461 -0.200671  0.127339
202  0.008894  0.012202 -0.405465  0.042560
```

然后可以使用seaborn的regplot方法，它可以做一个散布图，并加上一条线性回归的线（见图9-24）：

```python
In [105]: sns.regplot('m1', 'unemp', data=trans_data)
Out[105]: <matplotlib.axes._subplots.AxesSubplot at 0x7fb613720be0>

In [106]: plt.title('Changes in log %s versus log %s' % ('m1', 'unemp'))
```

![图9-24 seaborn的回归/散布图](http://upload-images.jianshu.io/upload_images/7178691-2133d20739478a80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在探索式数据分析工作中，同时观察一组变量的散布图是很有意义的，这也被称为散布图矩阵（scatter plot matrix）。纯手工创建这样的图表很费工夫，所以seaborn提供了一个便捷的pairplot函数，它支持在对角线上放置每个变量的直方图或密度估计（见图9-25）：

```python
In [107]: sns.pairplot(trans_data, diag_kind='kde', plot_kws={'alpha': 0.2})
```

![图9-25 statsmodels macro data的散布图矩阵](http://upload-images.jianshu.io/upload_images/7178691-20aa530a44e06f61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

你可能注意到了plot_kws参数。它可以让我们传递配置选项到非对角线元素上的图形使用。对于更详细的配置选项，可以查阅seaborn.pairplot文档字符串。


##分面网格（facet grid）和类型数据
要是数据集有额外的分组维度呢？有多个分类变量的数据可视化的一种方法是使用小面网格。seaborn有一个有用的内置函数factorplot，可以简化制作多种分面图（见图9-26）：
```python
 In [108]: sns.factorplot(x='day', y='tip_pct', hue='time', col='smoker',
   .....:                kind='bar', data=tips[tips.tip_pct < 1])
```

![图9-26 按照天/时间/吸烟者的小费百分比](http://upload-images.jianshu.io/upload_images/7178691-737ba19a0cbdd46f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

除了在分面中用不同的颜色按时间分组，我们还可以通过给每个时间值添加一行来扩展分面网格：

```python
In [109]: sns.factorplot(x='day', y='tip_pct', row='time',
   .....:                col='smoker',
   .....:                kind='bar', data=tips[tips.tip_pct < 1])
```

![图9-27 按天的tip_pct，通过time/smoker分面](http://upload-images.jianshu.io/upload_images/7178691-4e52192441c609f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

factorplot支持其它的绘图类型，你可能会用到。例如，盒图（它可以显示中位数，四分位数，和异常值）就是一个有用的可视化类型（见图9-28）：

```python
In [110]: sns.factorplot(x='tip_pct', y='day', kind='box',
   .....:                data=tips[tips.tip_pct < 0.5])
```

![图9-28 按天的tip_pct的盒图](http://upload-images.jianshu.io/upload_images/7178691-356fb27a7c658920.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

使用更通用的seaborn.FacetGrid类，你可以创建自己的分面网格。请查阅seaborn的文档（https://seaborn.pydata.org/）。

# 9.3 其它的Python可视化工具
与其它开源库类似，Python创建图形的方式非常多（根本罗列不完）。自从2010年，许多开发工作都集中在创建交互式图形以便在Web上发布。利用工具如Boken（https://bokeh.pydata.org/en/latest/）和Plotly（https://github.com/plotly/plotly.py），现在可以创建动态交互图形，用于网页浏览器。

对于创建用于打印或网页的静态图形，我建议默认使用matplotlib和附加的库，比如pandas和seaborn。对于其它数据可视化要求，学习其它的可用工具可能是有用的。我鼓励你探索绘图的生态系统，因为它将持续发展。

# 9.4 总结

本章的目的是熟悉一些基本的数据可视化操作，使用pandas，matplotlib，和seaborn。如果视觉显示数据分析的结果对你的工作很重要，我鼓励你寻求更多的资源来了解更高效的数据可视化。这是一个活跃的研究领域，你可以通过在线和纸质的形式学习许多优秀的资源。

下一章，我们将重点放在pandas的数据聚合和分组操作上。
